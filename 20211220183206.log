log 2021-12-20 18:32:06.869530
SELECT keyvalue FROM mw_WDOsettings WHERE keyname == INOUTIDFLDNAME AND section == DEMANDPREDICTION... done
SELECT keyvalue FROM mw_WDOsettings WHERE keyname == INOUTDATETIMEFLDNAME AND section == DEMANDPREDICTION... done
SELECT keyvalue FROM mw_WDOsettings WHERE keyname == INOUTVALUEFLDNAME AND section == DEMANDPREDICTION... done
SELECT keyvalue FROM mw_WDOsettings WHERE keyname == INOUTQUALITYFLDNAME AND section == DEMANDPREDICTION... done
SELECT keyvalue FROM mw_WDOsettings WHERE keyname == AUTOKEYID AND section == DEMANDPREDICTION... done
SELECT * FROM mw_WDOdemandpredictions ORDER BY ID ASC... done
*** df_wd ***
           muid  altid  active  id comment                      sensorid    sensortable predicttable  ...       param2  param3    param4  param5  param6 param7 param8 param9
0  Prediction_1      0       1   3    None         MALMEDY_CM_FT009.F.PV  cpt.inputdata       cpt.ao  ...          add       7      None    None    None   None   None   None
1  Prediction_2      0       1   4    None  CroixChodes_SP_CM_FT002.F.PV  cpt.inputdata       cpt.ao  ...  (64,128,64)    adam  adaptive    0.01    1000   1000   0.01   None
2  Prediction_3      0       1   5    None  CroixChodes_SP_CM_FT003.F.PV  cpt.inputdata       cpt.ao  ...  (64,128,64)    adam  adaptive    0.01    1000   1000   0.01   None
3  Prediction_4      0       1   6    None         MALMEDY_CM_FT005.F.PV  cpt.inputdata       cpt.ao  ...  (64,128,64)    adam  adaptive    0.01    1000   1000   0.01   None
4  Prediction_5      0       1   7    None         MALMEDY_CM_FT003.F.PV  cpt.inputdata       cpt.ao  ...  (64,128,64)    adam  adaptive    0.01    1000   1000   0.01   None
5  Prediction_6      0       1   8    None         MALMEDY_CM_FT007.F.PV  cpt.inputdata       cpt.ao  ...  (64,128,64)    adam  adaptive    0.01    1000   1000   0.01   None
6  Prediction_7      0       1   9    None         MALMEDY_CM_FT008.F.PV  cpt.inputdata       cpt.ao  ...  (64,128,64)    adam  adaptive    0.01    1000   1000   0.01   None
7  Prediction_8      0       1  10    None         MALMEDY_CM_FT052.F.PV  cpt.inputdata       cpt.ao  ...  (64,128,64)    adam  adaptive    0.01    1000   1000   0.01   None

[8 rows x 22 columns]
3 cpt.inputdata MALMEDY_CM_FT009.F.PV cpt.ao 60.0 1440.0 30240.0 HOLTWINTERS
DELETE FROM cpt.ao WHERE scadaid='MALMEDY_CM_FT009.F.PV'... done
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT009.F.PV' ORDER BY scadadatetime DESC... done
*** ldate, fdate ***
2021-12-17 14:15:00 2021-11-18 19:30:00
>>> Water demand prediction : TOF 2021-12-20 18:00:00
*** start_train, stop_train, start_pred, stop_pred, toftime ***
2021-11-26 15:00:00 2021-12-17 15:00:00 2021-12-17 15:00:00 2021-12-21 19:00:00 2021-12-20 18:00:00
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT009.F.PV' ORDER BY scadadatetime ASC, rnum ASC... done
*** df_in ***
        rnum       scadadatetime                scadaid  scadavalue scadaquality
0     333868 2021-11-18 19:30:00  MALMEDY_CM_FT009.F.PV       1.849         None
1     333891 2021-11-18 20:00:00  MALMEDY_CM_FT009.F.PV       1.849         None
2     333920 2021-11-18 20:00:00  MALMEDY_CM_FT009.F.PV       1.849         None
3     333931 2021-11-18 20:15:00  MALMEDY_CM_FT009.F.PV       1.463         None
4     333960 2021-11-18 20:15:00  MALMEDY_CM_FT009.F.PV       1.463         None
...      ...                 ...                    ...         ...          ...
6358  498787 2021-12-17 14:00:00  MALMEDY_CM_FT009.F.PV       1.583         None
6359  498811 2021-12-17 14:00:00  MALMEDY_CM_FT009.F.PV       1.583         None
6360  498828 2021-12-17 14:00:00  MALMEDY_CM_FT009.F.PV       1.583         None
6361  498847 2021-12-17 14:15:00  MALMEDY_CM_FT009.F.PV       1.583         None
6362  498869 2021-12-17 14:15:00  MALMEDY_CM_FT009.F.PV       1.583         None

[6363 rows x 5 columns]
*** X_train, y_train, df_X, df_y, X_pred ***
[[330. 330. 330. ... 351. 351. 351.]
 [  5.   5.   5. ...   5.   5.   5.]
 [ 15.  16.  17. ...  12.  13.  14.]] [1.708      1.75666667 1.84283333 2.152      1.708      1.6125
 1.57583333 1.87       1.195      1.195      1.2135     1.232
 1.1175     1.01083333 1.05       1.06933333 1.243      1.407
 2.243      2.313      2.21783333 1.567      1.93       2.2385
 2.42983333 1.844      1.85244444 1.86088889 1.86933333 1.87777778
 1.88622222 1.89466667 1.90311111 1.91155556 1.92       1.92844444
 1.93688889 1.94533333 1.95377778 1.96222222 1.97066667 1.97911111
 1.98755556 1.996      1.897      1.819      1.924      1.84533333
 1.8605     1.915      1.598      2.01533333 2.25466667 2.408
 1.579      1.579      1.579      1.59516667 1.676      1.256
 1.043      1.04466667 1.068      1.159      1.64       2.04966667
 1.693      1.857      1.84       1.741      1.63       1.66733333
 1.63683333 1.391      1.868      1.7275     1.6755     2.118
 1.76       1.392      1.203      1.203      1.215      1.1335
 1.041      1.03933333 1.086      1.457      1.7575     2.0465
 1.989      1.84966667 1.78433333 1.806      2.06466667 2.0905
 1.573      1.41833333 1.38183333 1.586      1.301      1.658
 2.015      1.668      1.5665     1.446      1.351      1.20033333
 1.1235     1.122      1.024      1.338      1.70883333 1.993
 1.749      1.68716667 1.988      1.86533333 1.8305     1.963
 1.60966667 1.427      1.397      2.03833333 2.313      2.083
 1.745      1.6445     1.544      1.544      1.15       1.1735
 1.1225     1.05516667 1.091      1.235      1.6945     2.128
 1.998      1.918      2.0025     2.03733333 1.789      1.71366667
 1.618      1.56       1.618      1.85266667 1.7235     1.477
 2.311      1.593      1.234      1.217      1.132      1.05266667
 1.02233333 1.03166667 1.055      1.386      1.5725     1.7775
 1.87       1.88866667 2.144      2.39       1.789      1.70433333
 1.68333333 1.79       2.169      1.8595     1.58116667 1.737
 1.375      1.409      1.443      1.443      1.301      1.241
 1.089      1.04816667 1.149      1.223      1.21833333 1.3145
 1.807      2.277      2.23433333 2.219      2.249      2.285
 2.223      1.9975     1.803      2.851      2.073      1.554
 1.4235     1.421      1.55366667 1.366      1.1365     1.034
 1.042      1.04533333 1.042      1.068      1.24       1.422
 1.518      2.075      2.42566667 2.3335     2.066      1.686
 1.64466667 1.736      1.8665     1.959      1.82833333 1.6575
 1.5145     1.327      1.57566667 1.7        1.416      1.132
 1.114      1.10416667 1.1        1.277      1.90166667 2.17733333
 1.994      1.861      1.949      2.01533333 2.127      1.882
 1.7395     1.6255     1.768      1.718      2.006      2.24916667
 2.025      1.411      1.455      1.428      1.32183333 1.146
 1.11666667 1.10166667 1.1        1.362      1.7365     2.09816667
 2.034      1.822      1.801      1.886      1.969      1.873
 1.73183333 1.506      1.49733333 1.6175     2.24       1.772
 1.915      2.058      1.485      1.485      1.3085     1.132
 1.121      1.11       1.209      1.28433333 1.46533333 2.182
 2.23933333 2.1805     2.093      1.823      1.649      1.475
 1.769      1.8835     1.998      1.739      1.8235     1.908
 1.36       1.3105     1.261      1.261      1.185      1.147
 1.1235     1.1        1.101      1.20433333 1.37783333 1.987
 1.97033333 1.938      1.818      1.7475     1.677      1.593
 1.52033333 1.44       1.22       1.22       1.22773611 1.23547222
 1.24320833 1.25094444 1.25868056 1.26641667 1.27415278 1.28188889
 1.289625   1.29736111 1.30509722 1.31283333 1.32056944 1.32830556
 1.33604167 1.34377778 1.35151389 1.35925    1.36698611 1.37472222
 1.38245833 1.39019444 1.39793056 1.40566667 1.41340278 1.42113889
 1.428875   1.43661111 1.44434722 1.45208333 1.45981944 1.46755556
 1.47529167 1.48302778 1.49076389 1.4985     1.50623611 1.51397222
 1.52170833 1.52944444 1.53718056 1.54491667 1.55265278 1.56038889
 1.568125   1.57586111 1.58359722 1.59133333 1.59906944 1.60680556
 1.61454167 1.62227778 1.63001389 1.63775    1.64548611 1.65322222
 1.66095833 1.66869444 1.67643056 1.68416667 1.69190278 1.69963889
 1.707375   1.71511111 1.72284722 1.73058333 1.73831944 1.74605556
 1.75379167 1.76152778 1.76926389 1.777      1.929      1.805
 1.5905     1.438      1.403      1.124      1.204      1.244
 1.148      1.1015     1.109      1.508      1.51268519 1.51737037
 1.52205556 1.52674074 1.53142593 1.53611111 1.5407963  1.54548148
 1.55016667 1.55485185 1.55953704 1.56422222 1.56890741 1.57359259
 1.57827778 1.58296296 1.58764815 1.59233333 1.59701852 1.6017037
 1.60638889 1.61107407 1.61575926 1.62044444 1.62512963 1.62981481
 1.6345     1.63918519 1.64387037 1.64855556 1.65324074 1.65792593
 1.66261111 1.6672963  1.67198148 1.67666667 1.783      2.398
 1.412      1.448      1.484      1.484      1.146      1.11933333
 1.12733333 1.13783333 1.137      1.143      1.70566667 1.9895
 1.992      1.955      1.955      1.98311111 2.01122222 2.03933333
 2.06744444 2.09555556 2.12366667 2.15177778 2.17988889 2.208
 2.208      2.15858824 2.10917647 2.05976471 2.01035294 1.96094118
 1.91152941 1.86211765 1.81270588 1.76329412 1.71388235 1.66447059
 1.61505882 1.56564706 1.51623529 1.46682353 1.41741176 1.368
 1.32       1.272      1.663      1.655      1.647      1.647
 1.539      1.46733333 1.379      1.379      1.239      1.169
 1.145      1.121      1.101      1.14566667 1.31816667 2.069
 2.27833333 2.2635     1.666      1.67333333 1.66133333 1.583     ]                        DOY   WD    HR
2021-11-26 15:00:00  330.0  5.0  15.0
2021-11-26 16:00:00  330.0  5.0  16.0
2021-11-26 17:00:00  330.0  5.0  17.0
2021-11-26 18:00:00  330.0  5.0  18.0
2021-11-26 19:00:00  330.0  5.0  19.0
...                    ...  ...   ...
2021-12-21 15:00:00  355.0  2.0  15.0
2021-12-21 16:00:00  355.0  2.0  16.0
2021-12-21 17:00:00  355.0  2.0  17.0
2021-12-21 18:00:00  355.0  2.0  18.0
2021-12-21 19:00:00  355.0  2.0  19.0

[605 rows x 3 columns] 2021-11-26 15:00:00    1.708000
2021-11-26 16:00:00    1.756667
2021-11-26 17:00:00    1.842833
2021-11-26 18:00:00    2.152000
2021-11-26 19:00:00    1.708000
                         ...   
2021-12-21 15:00:00    0.000000
2021-12-21 16:00:00    0.000000
2021-12-21 17:00:00    0.000000
2021-12-21 18:00:00    0.000000
2021-12-21 19:00:00    0.000000
Freq: 60T, Name: scadavalue, Length: 605, dtype: float64 [[351. 351. 351. 351. 351. 351. 351. 351. 351. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354.
  354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 355. 355. 355.
  355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355.
  355. 355. 355.]
 [  5.   5.   5.   5.   5.   5.   5.   5.   5.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.]
 [ 15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.   3.   4.
    5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.
   19.  20.  21.  22.  23.   0.   1.   2.   3.   4.   5.   6.   7.   8.
    9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.
   23.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
   13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.
    3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.
   17.  18.  19.]]
Run 0
                       ExponentialSmoothing Model Results                       
================================================================================
Dep. Variable:                    endog   No. Observations:                  504
Model:             ExponentialSmoothing   SSE                             23.116
Optimized:                         True   AIC                          -1531.360
Trend:                         Additive   BIC                          -1484.912
Seasonal:                      Additive   AICC                         -1530.617
Seasonal Periods:                     7   Date:                 Mon, 20 Dec 2021
Box-Cox:                          False   Time:                         18:32:08
Box-Cox Coeff.:                    None                                         
=================================================================================
                          coeff                 code              optimized      
---------------------------------------------------------------------------------
smoothing_level               1.0000000                alpha                 True
smoothing_trend              1.3665e-12                 beta                 True
smoothing_seasonal           1.0668e-09                gamma                 True
initial_level                 1.5653577                  l.0                 True
initial_trend                -0.0002161                  b.0                 True
initial_seasons.0             0.1425989                  s.0                 True
initial_seasons.1             0.1533647                  s.1                 True
initial_seasons.2             0.1604642                  s.2                 True
initial_seasons.3             0.1718936                  s.3                 True
initial_seasons.4             0.1788802                  s.4                 True
initial_seasons.5             0.1381525                  s.5                 True
initial_seasons.6             0.1262481                  s.6                 True
---------------------------------------------------------------------------------
*** fcst ***
[[1.59913476]
 [1.60968456]
 [1.616568  ]
 [1.62778135]
 [1.63455184]
 [1.59360813]
 [1.5814876 ]
 [1.59762236]
 [1.60817215]
 [1.61505559]
 [1.62626895]
 [1.63303944]
 [1.59209573]
 [1.5799752 ]
 [1.59610995]
 [1.60665975]
 [1.61354319]
 [1.62475654]
 [1.63152703]
 [1.59058332]
 [1.57846279]
 [1.59459755]
 [1.60514735]
 [1.61203079]
 [1.62324414]
 [1.63001463]
 [1.58907092]
 [1.57695039]
 [1.59308515]
 [1.60363495]
 [1.61051838]
 [1.62173174]
 [1.62850223]
 [1.58755852]
 [1.57543799]
 [1.59157275]
 [1.60212254]
 [1.60900598]
 [1.62021933]
 [1.62698982]
 [1.58604612]
 [1.57392558]
 [1.59006034]
 [1.60061014]
 [1.60749358]
 [1.61870693]
 [1.62547742]
 [1.58453371]
 [1.57241318]
 [1.58854794]
 [1.59909774]
 [1.60598118]
 [1.61719453]
 [1.62396502]
 [1.58302131]
 [1.57090078]
 [1.58703554]
 [1.59758534]
 [1.60446877]
 [1.61568213]
 [1.62245262]
 [1.58150891]
 [1.56938838]
 [1.58552313]
 [1.59607293]
 [1.60295637]
 [1.61416972]
 [1.62094021]
 [1.57999651]
 [1.56787597]
 [1.58401073]
 [1.59456053]
 [1.60144397]
 [1.61265732]
 [1.61942781]
 [1.5784841 ]
 [1.56636357]
 [1.58249833]
 [1.59304813]
 [1.59993157]
 [1.61114492]
 [1.61791541]
 [1.5769717 ]
 [1.56485117]
 [1.58098593]
 [1.59153572]
 [1.59841916]
 [1.60963252]
 [1.61640301]
 [1.5754593 ]
 [1.56333877]
 [1.57947352]
 [1.59002332]
 [1.59690676]
 [1.60812011]
 [1.6148906 ]
 [1.57394689]
 [1.56182636]
 [1.57796112]
 [1.58851092]
 [1.59539436]]
                   scadaid            scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT009.F.PV  [1.5991347585907842] 2021-12-17 15:00:00          100
1    MALMEDY_CM_FT009.F.PV  [1.6096845568108733] 2021-12-17 16:00:00          100
2    MALMEDY_CM_FT009.F.PV  [1.6165679957325985] 2021-12-17 17:00:00          100
3    MALMEDY_CM_FT009.F.PV  [1.6277813482057915] 2021-12-17 18:00:00          100
4    MALMEDY_CM_FT009.F.PV  [1.6345518378590218] 2021-12-17 19:00:00          100
..                     ...                   ...                 ...          ...
96   MALMEDY_CM_FT009.F.PV   [1.573946894873699] 2021-12-21 15:00:00          100
97   MALMEDY_CM_FT009.F.PV   [1.561826363136866] 2021-12-21 16:00:00          100
98   MALMEDY_CM_FT009.F.PV  [1.5779611207409823] 2021-12-21 17:00:00          100
99   MALMEDY_CM_FT009.F.PV  [1.5885109189610713] 2021-12-21 18:00:00          100
100  MALMEDY_CM_FT009.F.PV  [1.5953943578827965] 2021-12-21 19:00:00          100

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT009.F.PV*** df_out ***
                   scadaid            scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT009.F.PV  [1.5991347585907842] 2021-12-17 15:00:00          100
1    MALMEDY_CM_FT009.F.PV  [1.6096845568108733] 2021-12-17 16:00:00          100
2    MALMEDY_CM_FT009.F.PV  [1.6165679957325985] 2021-12-17 17:00:00          100
3    MALMEDY_CM_FT009.F.PV  [1.6277813482057915] 2021-12-17 18:00:00          100
4    MALMEDY_CM_FT009.F.PV  [1.6345518378590218] 2021-12-17 19:00:00          100
..                     ...                   ...                 ...          ...
96   MALMEDY_CM_FT009.F.PV   [1.573946894873699] 2021-12-21 15:00:00          100
97   MALMEDY_CM_FT009.F.PV   [1.561826363136866] 2021-12-21 16:00:00          100
98   MALMEDY_CM_FT009.F.PV  [1.5779611207409823] 2021-12-21 17:00:00          100
99   MALMEDY_CM_FT009.F.PV  [1.5885109189610713] 2021-12-21 18:00:00          100
100  MALMEDY_CM_FT009.F.PV  [1.5953943578827965] 2021-12-21 19:00:00          100

[101 rows x 4 columns]
                   scadaid            scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT009.F.PV  [1.5991347585907842] 2021-12-17 15:00:00          101
1    MALMEDY_CM_FT009.F.PV  [1.6096845568108733] 2021-12-17 16:00:00          101
2    MALMEDY_CM_FT009.F.PV  [1.6165679957325985] 2021-12-17 17:00:00          101
3    MALMEDY_CM_FT009.F.PV  [1.6277813482057915] 2021-12-17 18:00:00          101
4    MALMEDY_CM_FT009.F.PV  [1.6345518378590218] 2021-12-17 19:00:00          101
..                     ...                   ...                 ...          ...
96   MALMEDY_CM_FT009.F.PV   [1.573946894873699] 2021-12-21 15:00:00          101
97   MALMEDY_CM_FT009.F.PV   [1.561826363136866] 2021-12-21 16:00:00          101
98   MALMEDY_CM_FT009.F.PV  [1.5779611207409823] 2021-12-21 17:00:00          101
99   MALMEDY_CM_FT009.F.PV  [1.5885109189610713] 2021-12-21 18:00:00          101
100  MALMEDY_CM_FT009.F.PV  [1.5953943578827965] 2021-12-21 19:00:00          101

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT009.F.PV*** df_out ***
                   scadaid            scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT009.F.PV  [1.5991347585907842] 2021-12-17 15:00:00          101
1    MALMEDY_CM_FT009.F.PV  [1.6096845568108733] 2021-12-17 16:00:00          101
2    MALMEDY_CM_FT009.F.PV  [1.6165679957325985] 2021-12-17 17:00:00          101
3    MALMEDY_CM_FT009.F.PV  [1.6277813482057915] 2021-12-17 18:00:00          101
4    MALMEDY_CM_FT009.F.PV  [1.6345518378590218] 2021-12-17 19:00:00          101
..                     ...                   ...                 ...          ...
96   MALMEDY_CM_FT009.F.PV   [1.573946894873699] 2021-12-21 15:00:00          101
97   MALMEDY_CM_FT009.F.PV   [1.561826363136866] 2021-12-21 16:00:00          101
98   MALMEDY_CM_FT009.F.PV  [1.5779611207409823] 2021-12-21 17:00:00          101
99   MALMEDY_CM_FT009.F.PV  [1.5885109189610713] 2021-12-21 18:00:00          101
100  MALMEDY_CM_FT009.F.PV  [1.5953943578827965] 2021-12-21 19:00:00          101

[101 rows x 4 columns]


4 cpt.inputdata CroixChodes_SP_CM_FT002.F.PV cpt.ao 60.0 1440.0 30240.0 MLPDYNAMIC
DELETE FROM cpt.ao WHERE scadaid='CroixChodes_SP_CM_FT002.F.PV'... done
SELECT * FROM cpt.inputdata WHERE scadaid='CroixChodes_SP_CM_FT002.F.PV' ORDER BY scadadatetime DESC... done
*** ldate, fdate ***
2021-12-17 14:15:00 2021-11-18 19:30:00
>>> Water demand prediction : TOF 2021-12-20 18:00:00
*** start_train, stop_train, start_pred, stop_pred, toftime ***
2021-11-26 15:00:00 2021-12-17 15:00:00 2021-12-17 15:00:00 2021-12-21 19:00:00 2021-12-20 18:00:00
SELECT * FROM cpt.inputdata WHERE scadaid='CroixChodes_SP_CM_FT002.F.PV' ORDER BY scadadatetime ASC, rnum ASC... done
*** df_in ***
        rnum       scadadatetime                       scadaid  scadavalue scadaquality
0     333862 2021-11-18 19:30:00  CroixChodes_SP_CM_FT002.F.PV      35.083         None
1     333902 2021-11-18 20:00:00  CroixChodes_SP_CM_FT002.F.PV      37.646         None
2     333917 2021-11-18 20:00:00  CroixChodes_SP_CM_FT002.F.PV      37.646         None
3     333942 2021-11-18 20:15:00  CroixChodes_SP_CM_FT002.F.PV      36.702         None
4     333957 2021-11-18 20:15:00  CroixChodes_SP_CM_FT002.F.PV      36.702         None
...      ...                 ...                           ...         ...          ...
6358  498782 2021-12-17 14:00:00  CroixChodes_SP_CM_FT002.F.PV      47.010         None
6359  498802 2021-12-17 14:00:00  CroixChodes_SP_CM_FT002.F.PV      47.010         None
6360  498822 2021-12-17 14:00:00  CroixChodes_SP_CM_FT002.F.PV      47.010         None
6361  498838 2021-12-17 14:15:00  CroixChodes_SP_CM_FT002.F.PV      47.449         None
6362  498863 2021-12-17 14:15:00  CroixChodes_SP_CM_FT002.F.PV      47.449         None

[6363 rows x 5 columns]
*** X_train, y_train, df_X, df_y, X_pred ***
[[330. 330. 330. ... 351. 351. 351.]
 [  5.   5.   5. ...   5.   5.   5.]
 [ 15.  16.  17. ...  12.  13.  14.]] [39.1214     36.894      36.79883333 36.70066667 30.42816667 34.09816667
 27.90083333 25.9115     21.11316667 19.17216667 12.63716667 12.5855
 11.24233333 12.2055     13.1638     19.36766667 26.72116667 42.84316667
 44.17766667 49.67966667 51.30716667 44.665      44.8675     46.1405
 37.8775     47.54333333 47.5224537  47.50157407 47.48069444 47.45981481
 47.43893519 47.41805556 47.39717593 47.3762963  47.35541667 47.33453704
 47.31365741 47.29277778 47.27189815 47.25101852 47.23013889 47.20925926
 47.18837963 47.1675     48.6685     47.06583333 40.14083333 36.31233333
 36.83716667 34.23916667 37.0465     36.401      34.96933333 37.35366667
 26.103      27.2645     18.87116667 15.789      12.04283333 12.70266667
 12.00783333 15.24166667 16.75766667 29.3225     49.57883333 49.96316667
 55.3805     55.924      48.05766667 50.2405     47.95116667 41.33466667
 35.922      35.37266667 35.75533333 35.256      36.14183333 34.09216667
 27.8975     24.1885     18.61166667 13.77866667 11.02366667 10.52833333
  9.3486     13.94783333 13.71316667 23.92116667 35.9745     40.31983333
 43.2315     46.7335     47.86816667 36.50633333 39.745      40.94466667
 38.12183333 40.7145     35.6165     37.9445     37.77033333 37.92333333
 32.03916667 27.568      21.98       18.364      15.798      13.61733333
 13.90166667 17.2805     26.08816667 31.31116667 47.833      52.0955
 53.31516667 54.47666667 46.72866667 51.267      47.1685     44.95266667
 41.38366667 37.56216667 37.874      43.749      39.038      42.92316667
 37.326      29.31383333 24.036      19.66483333 16.59433333 14.29683333
 13.6845     14.729      18.45133333 27.37333333 43.44833333 42.7765
 45.75083333 52.53516667 43.64983333 41.81866667 44.11766667 42.0055
 44.36333333 44.12333333 42.32716667 45.492      40.43833333 41.01883333
 35.856      33.59216667 29.356      21.365      16.98716667 18.52716667
 17.79233333 15.9315     21.41       36.95983333 43.5035     54.31083333
 51.157      56.53666667 48.135      42.45133333 41.23916667 39.85283333
 31.63633333 35.6575     37.9035     41.135      36.841      31.77416667
 29.69083333 25.89633333 25.47233333 17.78683333 12.869      11.2205
 10.01033333 16.74333333 11.9775     15.56866667 28.0465     37.57016667
 50.4705     54.06516667 50.7775     47.56666667 40.60716667 41.33683333
 37.681      37.34666667 39.52183333 38.9775     35.73116667 34.98033333
 29.9945     24.0635     23.68816667 16.792      16.373      12.2805
  9.803      10.9135     11.3995     14.28266667 19.965      29.5295
 38.8115     45.43066667 43.10533333 42.70583333 41.1375     37.86333333
 34.24216667 36.6045     36.33283333 34.12716667 34.80883333 31.46266667
 32.52583333 22.46316667 20.97866667 15.7495     12.35966667 10.4545
 11.30566667 11.497      19.52516667 35.3725     52.38733333 52.73916667
 55.57       63.5052     55.91083333 53.011      52.88983333 50.31383333
 56.9735     43.3995     43.75366667 41.29433333 39.70116667 36.9215
 33.22       30.50516667 24.15666667 19.60183333 15.89016667 15.92033333
 15.80216667 17.25416667 20.9405     27.17166667 40.897      44.52783333
 47.44083333 47.623      49.941      41.4846     43.57266667 45.76516667
 43.33466667 39.28633333 37.78466667 38.10816667 41.51966667 35.18916667
 32.14583333 27.4245     20.143      15.95383333 13.96716667 12.805
 12.767      14.12183333 20.85383333 29.70916667 48.07466667 52.77633333
 51.5885     53.04133333 58.12816667 51.9325     47.219      44.34216667
 39.59983333 43.20666667 41.60216667 41.79483333 41.889      39.61283333
 30.38233333 24.2935     21.71716667 16.21316667 12.23333333 13.47583333
 13.37366667 14.228      15.00783333 22.6905     43.268      41.66966667
 44.9795     46.164      43.023      40.9        39.973      38.25516667
 39.50916667 43.50483333 41.49733333 42.997      42.86865278 42.74030556
 42.61195833 42.48361111 42.35526389 42.22691667 42.09856944 41.97022222
 41.841875   41.71352778 41.58518056 41.45683333 41.32848611 41.20013889
 41.07179167 40.94344444 40.81509722 40.68675    40.55840278 40.43005556
 40.30170833 40.17336111 40.04501389 39.91666667 39.78831944 39.65997222
 39.531625   39.40327778 39.27493056 39.14658333 39.01823611 38.88988889
 38.76154167 38.63319444 38.50484722 38.3765     38.24815278 38.11980556
 37.99145833 37.86311111 37.73476389 37.60641667 37.47806944 37.34972222
 37.221375   37.09302778 36.96468056 36.83633333 36.70798611 36.57963889
 36.45129167 36.32294444 36.19459722 36.06625    35.93790278 35.80955556
 35.68120833 35.55286111 35.42451389 35.29616667 35.16781944 35.03947222
 34.911125   34.78277778 34.65443056 34.52608333 34.39773611 34.26938889
 34.14104167 34.01269444 33.88434722 33.756      32.5555     33.30533333
 29.72566667 21.44916667 22.07883333 15.76133333 14.2455     11.63
 13.32266667 12.553      18.67966667 35.4022     35.48899074 35.57578148
 35.66257222 35.74936296 35.8361537  35.92294444 36.00973519 36.09652593
 36.18331667 36.27010741 36.35689815 36.44368889 36.53047963 36.61727037
 36.70406111 36.79085185 36.87764259 36.96443333 37.05122407 37.13801481
 37.22480556 37.3115963  37.39838704 37.48517778 37.57196852 37.65875926
 37.74555    37.83234074 37.91913148 38.00592222 38.09271296 38.1795037
 38.26629444 38.35308519 38.43987593 38.52666667 39.97716667 35.9565
 30.59133333 28.1665     20.65566667 17.42666667 13.0975     12.27433333
 12.34866667 14.5275     21.78766667 31.7425     47.64566667 51.1865
 51.41433333 53.2866     60.454      57.89303704 55.33207407 52.77111111
 50.21014815 47.64918519 45.08822222 42.52725926 39.9662963  37.40533333
 37.1918     38.18663529 39.18147059 40.17630588 41.17114118 42.16597647
 43.16081176 44.15564706 45.15048235 46.14531765 47.14015294 48.13498824
 49.12982353 50.12465882 51.11949412 52.11432941 53.10916471 54.104
 47.16783333 43.42316667 43.818      42.4905     39.537      37.1355
 32.97166667 32.3615     25.322      17.3025     12.99983333 11.025
 11.45633333 12.13583333 18.4225     34.2905     56.32433333 50.34233333
 52.4615     54.4725     55.552      49.87983333 50.75233333 47.15633333]                        DOY   WD    HR
2021-11-26 15:00:00  330.0  5.0  15.0
2021-11-26 16:00:00  330.0  5.0  16.0
2021-11-26 17:00:00  330.0  5.0  17.0
2021-11-26 18:00:00  330.0  5.0  18.0
2021-11-26 19:00:00  330.0  5.0  19.0
...                    ...  ...   ...
2021-12-21 15:00:00  355.0  2.0  15.0
2021-12-21 16:00:00  355.0  2.0  16.0
2021-12-21 17:00:00  355.0  2.0  17.0
2021-12-21 18:00:00  355.0  2.0  18.0
2021-12-21 19:00:00  355.0  2.0  19.0

[605 rows x 3 columns] 2021-11-26 15:00:00    39.121400
2021-11-26 16:00:00    36.894000
2021-11-26 17:00:00    36.798833
2021-11-26 18:00:00    36.700667
2021-11-26 19:00:00    30.428167
                         ...    
2021-12-21 15:00:00     0.000000
2021-12-21 16:00:00     0.000000
2021-12-21 17:00:00     0.000000
2021-12-21 18:00:00     0.000000
2021-12-21 19:00:00     0.000000
Freq: 60T, Name: scadavalue, Length: 605, dtype: float64 [[351. 351. 351. 351. 351. 351. 351. 351. 351. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354.
  354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 355. 355. 355.
  355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355.
  355. 355. 355.]
 [  5.   5.   5.   5.   5.   5.   5.   5.   5.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.]
 [ 15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.   3.   4.
    5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.
   19.  20.  21.  22.  23.   0.   1.   2.   3.   4.   5.   6.   7.   8.
    9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.
   23.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
   13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.
    3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.
   17.  18.  19.]]
Run 0
Iteration 1, loss = 3829.17222517
Iteration 2, loss = 732.81222866
Iteration 3, loss = 672.04076992
Iteration 4, loss = 110.75474535
Iteration 5, loss = 190.58960860
Iteration 6, loss = 93.36634424
Iteration 7, loss = 110.75211904
Iteration 8, loss = 91.55323082
Iteration 9, loss = 78.91977746
Iteration 10, loss = 85.59883836
Iteration 11, loss = 76.83490163
Iteration 12, loss = 74.84186562
Iteration 13, loss = 76.86251068
Iteration 14, loss = 83.45652041
Iteration 15, loss = 76.48570861
Iteration 16, loss = 75.11881815
Iteration 17, loss = 76.34883387
Iteration 18, loss = 74.47509785
Iteration 19, loss = 74.57109049
Iteration 20, loss = 74.60833439
Iteration 21, loss = 74.06966779
Iteration 22, loss = 74.01339564
Iteration 23, loss = 73.40425069
Iteration 24, loss = 73.62474148
Iteration 25, loss = 72.87818092
Iteration 26, loss = 72.25421688
Iteration 27, loss = 72.25863228
Iteration 28, loss = 72.00966097
Iteration 29, loss = 69.89743419
Iteration 30, loss = 69.17850705
Iteration 31, loss = 66.79461827
Iteration 32, loss = 66.79379318
Iteration 33, loss = 63.92460313
Iteration 34, loss = 64.97376291
Iteration 35, loss = 61.51698925
Iteration 36, loss = 61.69967975
Iteration 37, loss = 57.79266839
Iteration 38, loss = 56.96098955
Iteration 39, loss = 62.53296166
Iteration 40, loss = 58.92390362
Iteration 41, loss = 54.84453356
Iteration 42, loss = 52.33005476
Iteration 43, loss = 54.12449336
Iteration 44, loss = 52.59483706
Iteration 45, loss = 51.70379784
Iteration 46, loss = 50.04638400
Iteration 47, loss = 50.23390699
Iteration 48, loss = 50.02734552
Iteration 49, loss = 49.38482530
Iteration 50, loss = 48.26064978
Iteration 51, loss = 48.15446738
Iteration 52, loss = 47.52818204
Iteration 53, loss = 46.88654472
Iteration 54, loss = 46.78606828
Iteration 55, loss = 47.54731183
Iteration 56, loss = 46.65595567
Iteration 57, loss = 46.21572574
Iteration 58, loss = 46.37042130
Iteration 59, loss = 46.08741865
Iteration 60, loss = 45.73812169
Iteration 61, loss = 45.81271223
Iteration 62, loss = 45.56290602
Iteration 63, loss = 45.21867655
Iteration 64, loss = 45.15970450
Iteration 65, loss = 45.61298155
Iteration 66, loss = 45.04354409
Iteration 67, loss = 45.33270706
Iteration 68, loss = 45.64489826
Iteration 69, loss = 45.56180043
Iteration 70, loss = 44.64981878
Iteration 71, loss = 45.22946546
Iteration 72, loss = 45.13255984
Iteration 73, loss = 44.09378502
Iteration 74, loss = 44.11536399
Iteration 75, loss = 44.05653052
Iteration 76, loss = 45.40659611
Iteration 77, loss = 48.39654042
Iteration 78, loss = 47.53188951
Iteration 79, loss = 45.80862161
Iteration 80, loss = 44.08181106
Iteration 81, loss = 43.63172941
Iteration 82, loss = 45.40908723
Iteration 83, loss = 44.09118561
Iteration 84, loss = 42.46821651
Iteration 85, loss = 43.75343979
Iteration 86, loss = 41.93388958
Iteration 87, loss = 42.61187612
Iteration 88, loss = 42.03396696
Iteration 89, loss = 41.99220213
Iteration 90, loss = 41.34734906
Iteration 91, loss = 42.00302091
Iteration 92, loss = 40.38858776
Iteration 93, loss = 40.84326424
Iteration 94, loss = 40.03789898
Iteration 95, loss = 40.42158597
Iteration 96, loss = 39.37441260
Iteration 97, loss = 39.44667219
Iteration 98, loss = 39.36142221
Iteration 99, loss = 38.99974538
Iteration 100, loss = 38.01519724
Iteration 101, loss = 37.60806915
Iteration 102, loss = 37.53309819
Iteration 103, loss = 39.06441083
Iteration 104, loss = 36.63279049
Iteration 105, loss = 37.04724722
Iteration 106, loss = 36.27332339
Iteration 107, loss = 38.98140409
Iteration 108, loss = 38.06856602
Iteration 109, loss = 42.19308601
Iteration 110, loss = 42.05397237
Iteration 111, loss = 36.61595133
Iteration 112, loss = 38.96668770
Iteration 113, loss = 35.99934532
Iteration 114, loss = 37.75091090
Iteration 115, loss = 37.21814083
Iteration 116, loss = 36.26842358
Iteration 117, loss = 36.73055844
Iteration 118, loss = 34.95244012
Iteration 119, loss = 35.26959293
Iteration 120, loss = 35.74043184
Iteration 121, loss = 35.08615685
Iteration 122, loss = 34.55451364
Iteration 123, loss = 34.33596076
Iteration 124, loss = 34.27536871
Iteration 125, loss = 35.50035128
Iteration 126, loss = 33.85019356
Iteration 127, loss = 35.26789026
Iteration 128, loss = 35.54518621
Iteration 129, loss = 34.79621627
Iteration 130, loss = 35.90759553
Iteration 131, loss = 35.47166008
Iteration 132, loss = 34.26713102
Iteration 133, loss = 36.71247029
Iteration 134, loss = 36.80800764
Iteration 135, loss = 36.33530917
Iteration 136, loss = 38.06934073
Iteration 137, loss = 35.42231111
Iteration 138, loss = 38.93966166
Iteration 139, loss = 35.54951588
Iteration 140, loss = 36.64074118
Iteration 141, loss = 33.96236435
Iteration 142, loss = 36.21607987
Iteration 143, loss = 34.64052318
Iteration 144, loss = 34.21151237
Iteration 145, loss = 34.24030103
Iteration 146, loss = 36.63534274
Iteration 147, loss = 34.03078073
Iteration 148, loss = 33.51910986
Iteration 149, loss = 33.40212707
Iteration 150, loss = 34.26581113
Iteration 151, loss = 33.76572349
Iteration 152, loss = 34.37255744
Iteration 153, loss = 35.36349833
Iteration 154, loss = 36.17738975
Iteration 155, loss = 39.21573459
Iteration 156, loss = 35.01483865
Iteration 157, loss = 35.89433648
Iteration 158, loss = 34.42543597
Iteration 159, loss = 34.18698916
Iteration 160, loss = 33.64045093
Iteration 161, loss = 34.53934722
Iteration 162, loss = 33.12941052
Iteration 163, loss = 33.11078928
Iteration 164, loss = 33.48035797
Iteration 165, loss = 33.82040626
Iteration 166, loss = 34.61423560
Iteration 167, loss = 33.66878185
Iteration 168, loss = 34.41188524
Iteration 169, loss = 33.78539159
Iteration 170, loss = 33.27936169
Iteration 171, loss = 32.93859065
Iteration 172, loss = 33.19824940
Iteration 173, loss = 34.64673376
Iteration 174, loss = 33.74233231
Iteration 175, loss = 33.93327069
Iteration 176, loss = 34.89846960
Iteration 177, loss = 34.03310766
Iteration 178, loss = 33.97611348
Iteration 179, loss = 33.64562605
Iteration 180, loss = 32.71204480
Iteration 181, loss = 33.68172477
Iteration 182, loss = 33.02743659
Iteration 183, loss = 35.76257483
Iteration 184, loss = 36.56758889
Iteration 185, loss = 34.75656011
Iteration 186, loss = 34.45923609
Iteration 187, loss = 37.71638246
Iteration 188, loss = 36.68079373
Iteration 189, loss = 35.33723882
Iteration 190, loss = 32.97666005
Iteration 191, loss = 33.55523065
Iteration 192, loss = 33.85272853
Iteration 193, loss = 33.64138324
Iteration 194, loss = 33.50046064
Iteration 195, loss = 32.76204320
Iteration 196, loss = 32.89277464
Iteration 197, loss = 33.50080243
Iteration 198, loss = 32.37904577
Iteration 199, loss = 33.70003617
Iteration 200, loss = 37.92519898
Iteration 201, loss = 34.43440245
Iteration 202, loss = 33.59721788
Iteration 203, loss = 35.38745896
Iteration 204, loss = 36.11785884
Iteration 205, loss = 34.06630165
Iteration 206, loss = 34.32980476
Iteration 207, loss = 33.09041728
Iteration 208, loss = 32.14032279
Iteration 209, loss = 31.71975063
Iteration 210, loss = 32.02861713
Iteration 211, loss = 33.03732321
Iteration 212, loss = 32.86488096
Iteration 213, loss = 31.82245683
Iteration 214, loss = 32.26572170
Iteration 215, loss = 32.26341840
Iteration 216, loss = 31.82993288
Iteration 217, loss = 31.95101146
Iteration 218, loss = 33.79102264
Iteration 219, loss = 33.44789822
Iteration 220, loss = 31.98937713
Iteration 221, loss = 32.27868152
Iteration 222, loss = 32.18419852
Iteration 223, loss = 31.72023639
Iteration 224, loss = 31.71016395
Iteration 225, loss = 31.85512831
Iteration 226, loss = 32.38562326
Iteration 227, loss = 31.44270960
Iteration 228, loss = 31.39614972
Iteration 229, loss = 31.36210500
Iteration 230, loss = 31.22319784
Iteration 231, loss = 31.45375692
Iteration 232, loss = 31.30384694
Iteration 233, loss = 31.34078036
Iteration 234, loss = 31.92457763
Iteration 235, loss = 32.89490212
Iteration 236, loss = 32.57182941
Iteration 237, loss = 32.54466899
Iteration 238, loss = 32.87586523
Iteration 239, loss = 31.96097237
Iteration 240, loss = 32.03813366
Iteration 241, loss = 34.28172362
Iteration 242, loss = 31.10347565
Iteration 243, loss = 33.90909281
Iteration 244, loss = 32.95834999
Iteration 245, loss = 33.54838424
Iteration 246, loss = 32.58550053
Iteration 247, loss = 33.13271331
Iteration 248, loss = 35.62749608
Iteration 249, loss = 35.27125145
Iteration 250, loss = 34.27653188
Iteration 251, loss = 32.64150733
Iteration 252, loss = 33.70643670
Iteration 253, loss = 32.63610835
Iteration 254, loss = 31.80082571
Iteration 255, loss = 32.86115019
Iteration 256, loss = 33.22257277
Iteration 257, loss = 32.12643230
Iteration 258, loss = 32.66805551
Iteration 259, loss = 33.02501401
Iteration 260, loss = 31.09602399
Iteration 261, loss = 31.78159383
Iteration 262, loss = 31.87814457
Iteration 263, loss = 31.67586925
Iteration 264, loss = 31.74315409
Iteration 265, loss = 31.00003325
Iteration 266, loss = 30.86532618
Iteration 267, loss = 31.08657868
Iteration 268, loss = 31.00031890
Iteration 269, loss = 31.48978435
Iteration 270, loss = 31.25982480
Iteration 271, loss = 32.38442234
Iteration 272, loss = 32.43182957
Iteration 273, loss = 32.08764786
Iteration 274, loss = 32.07911102
Iteration 275, loss = 31.18065256
Iteration 276, loss = 31.73606023
Iteration 277, loss = 31.87498361
Iteration 278, loss = 32.68221260
Iteration 279, loss = 33.07237551
Iteration 280, loss = 33.04437770
Iteration 281, loss = 31.91771064
Iteration 282, loss = 31.41803278
Iteration 283, loss = 31.38387635
Iteration 284, loss = 31.26548810
Iteration 285, loss = 31.57186306
Iteration 286, loss = 30.90093570
Iteration 287, loss = 31.31173069
Iteration 288, loss = 30.68622275
Iteration 289, loss = 31.78210298
Iteration 290, loss = 31.18864443
Iteration 291, loss = 31.85014732
Iteration 292, loss = 32.78239884
Iteration 293, loss = 34.82310943
Iteration 294, loss = 32.75185265
Iteration 295, loss = 33.19258140
Iteration 296, loss = 32.16511730
Iteration 297, loss = 34.67881869
Iteration 298, loss = 33.67427327
Iteration 299, loss = 36.01665726
Iteration 300, loss = 33.92879130
Iteration 301, loss = 34.53548279
Iteration 302, loss = 33.45966578
Iteration 303, loss = 33.79778042
Iteration 304, loss = 32.77506526
Iteration 305, loss = 33.48056872
Iteration 306, loss = 34.69865041
Iteration 307, loss = 33.11695469
Iteration 308, loss = 32.36322071
Iteration 309, loss = 32.07370368
Iteration 310, loss = 31.83479788
Iteration 311, loss = 31.11350481
Iteration 312, loss = 31.47971066
Iteration 313, loss = 32.29456622
Iteration 314, loss = 34.03087208
Iteration 315, loss = 33.36831340
Iteration 316, loss = 33.99046097
Iteration 317, loss = 32.64994057
Iteration 318, loss = 31.14055855
Iteration 319, loss = 31.78703694
Iteration 320, loss = 31.40016162
Iteration 321, loss = 31.30114198
Iteration 322, loss = 32.40561265
Iteration 323, loss = 32.65179334
Iteration 324, loss = 31.78148695
Iteration 325, loss = 32.88544977
Iteration 326, loss = 31.64179568
Iteration 327, loss = 33.18082548
Iteration 328, loss = 33.63115417
Iteration 329, loss = 32.19136931
Iteration 330, loss = 32.00402584
Iteration 331, loss = 32.39596945
Iteration 332, loss = 30.99239385
Iteration 333, loss = 31.69697851
Iteration 334, loss = 31.69531629
Iteration 335, loss = 31.84144128
Iteration 336, loss = 31.31404595
Iteration 337, loss = 31.05877245
Iteration 338, loss = 30.89683349
Iteration 339, loss = 30.88850620
Iteration 340, loss = 31.30396665
Iteration 341, loss = 30.86254413
Iteration 342, loss = 31.24968018
Iteration 343, loss = 30.96682190
Iteration 344, loss = 31.08445265
Iteration 345, loss = 31.37083178
Iteration 346, loss = 31.92188742
Iteration 347, loss = 33.11690116
Iteration 348, loss = 33.64688793
Iteration 349, loss = 35.76350373
Iteration 350, loss = 35.89647897
Iteration 351, loss = 32.65416725
Iteration 352, loss = 33.36256525
Iteration 353, loss = 32.22413141
Iteration 354, loss = 32.37357296
Iteration 355, loss = 31.14660717
Iteration 356, loss = 31.01314478
Iteration 357, loss = 33.02563810
Iteration 358, loss = 32.39958496
Iteration 359, loss = 31.80972513
Iteration 360, loss = 31.84113506
Iteration 361, loss = 31.37457498
Iteration 362, loss = 31.36041498
Iteration 363, loss = 31.48096142
Iteration 364, loss = 30.95387998
Iteration 365, loss = 31.76179939
Iteration 366, loss = 31.41507907
Iteration 367, loss = 31.37790554
Iteration 368, loss = 32.58881345
Iteration 369, loss = 31.61446232
Iteration 370, loss = 33.06202111
Iteration 371, loss = 32.58070153
Iteration 372, loss = 35.47597934
Iteration 373, loss = 33.35243158
Iteration 374, loss = 34.48030235
Iteration 375, loss = 32.23296641
Iteration 376, loss = 33.73332300
Iteration 377, loss = 31.48781174
Iteration 378, loss = 31.74852741
Iteration 379, loss = 31.76210689
Iteration 380, loss = 32.02247855
Iteration 381, loss = 32.59320317
Iteration 382, loss = 32.99302816
Iteration 383, loss = 31.44074019
Iteration 384, loss = 30.83715597
Iteration 385, loss = 30.90777039
Iteration 386, loss = 31.02800061
Iteration 387, loss = 31.35311940
Iteration 388, loss = 31.02554072
Iteration 389, loss = 31.71651520
Iteration 390, loss = 31.24789363
Iteration 391, loss = 30.80710844
Iteration 392, loss = 31.86653843
Iteration 393, loss = 31.44026435
Iteration 394, loss = 31.30607675
Iteration 395, loss = 31.32524381
Iteration 396, loss = 31.28027011
Iteration 397, loss = 32.67588848
Iteration 398, loss = 31.75896525
Iteration 399, loss = 31.68533837
Iteration 400, loss = 30.93790374
Iteration 401, loss = 31.34277168
Iteration 402, loss = 31.08296424
Iteration 403, loss = 31.66834297
Iteration 404, loss = 30.94220173
Iteration 405, loss = 32.09632243
Iteration 406, loss = 31.69712390
Iteration 407, loss = 35.16321893
Iteration 408, loss = 35.10407524
Iteration 409, loss = 33.30431158
Iteration 410, loss = 32.02559211
Iteration 411, loss = 32.12095093
Iteration 412, loss = 33.11919212
Iteration 413, loss = 32.71128874
Iteration 414, loss = 32.33178500
Iteration 415, loss = 31.95760528
Iteration 416, loss = 31.55893520
Iteration 417, loss = 31.50184539
Iteration 418, loss = 30.98843678
Iteration 419, loss = 31.04810274
Iteration 420, loss = 31.07444568
Iteration 421, loss = 30.93495778
Iteration 422, loss = 31.28917597
Iteration 423, loss = 31.76465587
Iteration 424, loss = 30.81799529
Iteration 425, loss = 31.31414923
Iteration 426, loss = 30.83298893
Iteration 427, loss = 31.18816203
Iteration 428, loss = 32.13312874
Iteration 429, loss = 33.38565574
Iteration 430, loss = 33.41107484
Iteration 431, loss = 33.53113349
Iteration 432, loss = 34.38309711
Iteration 433, loss = 34.55835303
Iteration 434, loss = 33.72182758
Iteration 435, loss = 32.38245398
Iteration 436, loss = 30.95314835
Iteration 437, loss = 32.30732975
Iteration 438, loss = 30.60631416
Iteration 439, loss = 32.37176570
Iteration 440, loss = 33.06084646
Iteration 441, loss = 32.20508748
Iteration 442, loss = 33.27205098
Iteration 443, loss = 31.74873506
Iteration 444, loss = 31.99507092
Iteration 445, loss = 32.63602656
Iteration 446, loss = 32.00899401
Iteration 447, loss = 31.79614185
Iteration 448, loss = 31.03814180
Iteration 449, loss = 31.73739994
Iteration 450, loss = 31.19641608
Iteration 451, loss = 31.76368275
Iteration 452, loss = 31.69768828
Iteration 453, loss = 31.10414460
Iteration 454, loss = 31.31192690
Iteration 455, loss = 30.77137843
Iteration 456, loss = 31.26874763
Iteration 457, loss = 30.49089657
Iteration 458, loss = 31.07192096
Iteration 459, loss = 31.33096086
Iteration 460, loss = 30.86293986
Iteration 461, loss = 30.68871033
Iteration 462, loss = 30.56793749
Iteration 463, loss = 31.11833211
Iteration 464, loss = 30.48575327
Iteration 465, loss = 30.91717862
Iteration 466, loss = 30.70703555
Iteration 467, loss = 31.31472354
Iteration 468, loss = 31.37553863
Iteration 469, loss = 32.05975902
Iteration 470, loss = 32.02929838
Iteration 471, loss = 30.80601161
Iteration 472, loss = 30.64377688
Iteration 473, loss = 30.95436656
Iteration 474, loss = 30.72947525
Iteration 475, loss = 30.82266333
Iteration 476, loss = 30.83970367
Iteration 477, loss = 31.60866812
Iteration 478, loss = 30.75246460
Iteration 479, loss = 31.17448787
Iteration 480, loss = 30.88256181
Iteration 481, loss = 30.65925141
Iteration 482, loss = 31.54537362
Iteration 483, loss = 31.02571788
Iteration 484, loss = 30.99312855
Iteration 485, loss = 30.53352335
Iteration 486, loss = 31.22350334
Iteration 487, loss = 30.84019512
Iteration 488, loss = 30.78781191
Iteration 489, loss = 31.55503438
Iteration 490, loss = 30.47088319
Iteration 491, loss = 31.25692419
Iteration 492, loss = 32.16665957
Iteration 493, loss = 31.07188614
Iteration 494, loss = 31.05228823
Iteration 495, loss = 31.43968102
Iteration 496, loss = 31.49626729
Iteration 497, loss = 32.31249225
Iteration 498, loss = 32.16513803
Iteration 499, loss = 31.55765269
Iteration 500, loss = 30.96434938
Iteration 501, loss = 31.16039045
Iteration 502, loss = 30.55924351
Iteration 503, loss = 30.86144170
Iteration 504, loss = 30.91360783
Iteration 505, loss = 30.69774080
Iteration 506, loss = 30.86135786
Iteration 507, loss = 31.00946871
Iteration 508, loss = 33.02430999
Iteration 509, loss = 34.72551713
Iteration 510, loss = 36.62119298
Iteration 511, loss = 33.68920544
Iteration 512, loss = 34.27009802
Iteration 513, loss = 32.97289688
Iteration 514, loss = 31.65681231
Iteration 515, loss = 31.43913914
Iteration 516, loss = 30.55625131
Iteration 517, loss = 31.23966874
Iteration 518, loss = 30.41985522
Iteration 519, loss = 30.84441659
Iteration 520, loss = 31.50840760
Iteration 521, loss = 30.94364397
Iteration 522, loss = 32.32095178
Iteration 523, loss = 31.18886858
Iteration 524, loss = 30.24950070
Iteration 525, loss = 31.15298655
Iteration 526, loss = 31.25770949
Iteration 527, loss = 31.49523381
Iteration 528, loss = 31.27679882
Iteration 529, loss = 30.76277501
Iteration 530, loss = 30.74734188
Iteration 531, loss = 31.84851657
Iteration 532, loss = 31.77585460
Iteration 533, loss = 31.23123273
Iteration 534, loss = 31.14151216
Iteration 535, loss = 32.77157136
Iteration 536, loss = 32.41715418
Iteration 537, loss = 31.53863017
Iteration 538, loss = 31.82745434
Iteration 539, loss = 30.71209313
Iteration 540, loss = 31.04102154
Iteration 541, loss = 30.69802078
Iteration 542, loss = 31.08729793
Iteration 543, loss = 31.07723308
Iteration 544, loss = 30.50911868
Iteration 545, loss = 30.68945930
Iteration 546, loss = 30.69116334
Iteration 547, loss = 30.51135338
Iteration 548, loss = 31.12308848
Iteration 549, loss = 31.39301148
Iteration 550, loss = 31.43066245
Iteration 551, loss = 31.42161376
Iteration 552, loss = 30.99344119
Iteration 553, loss = 30.67073246
Iteration 554, loss = 30.67891410
Iteration 555, loss = 30.49687763
Iteration 556, loss = 30.87343299
Iteration 557, loss = 31.12476051
Iteration 558, loss = 30.48488978
Iteration 559, loss = 31.37357316
Iteration 560, loss = 31.07405732
Iteration 561, loss = 30.69923031
Iteration 562, loss = 30.35704612
Iteration 563, loss = 30.64181097
Iteration 564, loss = 30.70460003
Iteration 565, loss = 30.55572355
Iteration 566, loss = 30.52810481
Iteration 567, loss = 30.48461925
Iteration 568, loss = 30.61811470
Iteration 569, loss = 30.47912600
Iteration 570, loss = 30.36409106
Iteration 571, loss = 30.53496293
Iteration 572, loss = 30.89280496
Iteration 573, loss = 30.79967699
Iteration 574, loss = 30.42469864
Iteration 575, loss = 30.53545861
Iteration 576, loss = 30.77067375
Iteration 577, loss = 31.46376531
Iteration 578, loss = 31.08815928
Iteration 579, loss = 31.20027457
Iteration 580, loss = 30.87113342
Iteration 581, loss = 31.97157562
Iteration 582, loss = 30.89131665
Iteration 583, loss = 31.11095765
Iteration 584, loss = 31.29435661
Iteration 585, loss = 31.04003840
Iteration 586, loss = 30.78914996
Iteration 587, loss = 31.03787886
Iteration 588, loss = 31.46266629
Iteration 589, loss = 32.22273708
Iteration 590, loss = 31.61393169
Iteration 591, loss = 31.49947009
Iteration 592, loss = 31.15947947
Iteration 593, loss = 31.50020725
Iteration 594, loss = 31.53201073
Iteration 595, loss = 31.54538047
Iteration 596, loss = 31.50720228
Iteration 597, loss = 31.65457257
Iteration 598, loss = 34.30277667
Iteration 599, loss = 32.35183369
Iteration 600, loss = 31.63088287
Iteration 601, loss = 32.53022138
Iteration 602, loss = 31.16827251
Iteration 603, loss = 31.71671906
Iteration 604, loss = 30.76565230
Iteration 605, loss = 31.05821645
Iteration 606, loss = 31.91386991
Iteration 607, loss = 31.57163382
Iteration 608, loss = 32.14387528
Iteration 609, loss = 31.01476203
Iteration 610, loss = 30.99312652
Iteration 611, loss = 31.17478655
Iteration 612, loss = 32.10985634
Iteration 613, loss = 31.50518411
Iteration 614, loss = 31.62276695
Iteration 615, loss = 31.33627902
Iteration 616, loss = 31.02792298
Iteration 617, loss = 31.39264088
Iteration 618, loss = 30.74860080
Iteration 619, loss = 31.18520099
Iteration 620, loss = 30.76494842
Iteration 621, loss = 30.25793189
Iteration 622, loss = 30.96954106
Iteration 623, loss = 30.29705313
Iteration 624, loss = 30.54589024
Iteration 625, loss = 30.66118271
Iteration 626, loss = 30.78358789
Iteration 627, loss = 31.33508393
Iteration 628, loss = 31.52882049
Iteration 629, loss = 31.96667823
Iteration 630, loss = 31.96216580
Iteration 631, loss = 31.87536492
Iteration 632, loss = 32.97346112
Iteration 633, loss = 32.95618965
Iteration 634, loss = 31.13104796
Iteration 635, loss = 30.89926186
Iteration 636, loss = 31.08930746
Iteration 637, loss = 32.02800810
Iteration 638, loss = 31.67008354
Iteration 639, loss = 32.47874447
Iteration 640, loss = 34.23782286
Iteration 641, loss = 32.96784877
Iteration 642, loss = 32.06170706
Iteration 643, loss = 31.96783528
Iteration 644, loss = 32.04050654
Iteration 645, loss = 32.08968979
Iteration 646, loss = 30.49510373
Iteration 647, loss = 31.11777149
Iteration 648, loss = 31.13067508
Iteration 649, loss = 31.22232290
Iteration 650, loss = 31.44218949
Iteration 651, loss = 30.89288712
Iteration 652, loss = 30.63754895
Iteration 653, loss = 30.78146522
Iteration 654, loss = 31.63231297
Iteration 655, loss = 31.38394208
Iteration 656, loss = 31.19557732
Iteration 657, loss = 30.70435704
Iteration 658, loss = 30.79669255
Iteration 659, loss = 31.01776094
Iteration 660, loss = 30.70385238
Iteration 661, loss = 30.81405056
Iteration 662, loss = 30.45091456
Iteration 663, loss = 30.45927387
Iteration 664, loss = 30.54418610
Iteration 665, loss = 30.64523250
Iteration 666, loss = 30.39183486
Iteration 667, loss = 31.02698687
Iteration 668, loss = 31.02053379
Iteration 669, loss = 30.42466313
Iteration 670, loss = 30.65946809
Iteration 671, loss = 30.80331992
Iteration 672, loss = 30.88495074
Iteration 673, loss = 31.21523366
Iteration 674, loss = 30.72721761
Iteration 675, loss = 31.23076188
Iteration 676, loss = 30.96224691
Iteration 677, loss = 32.31463345
Iteration 678, loss = 32.53872196
Iteration 679, loss = 31.66320689
Iteration 680, loss = 31.83394396
Iteration 681, loss = 31.91031364
Iteration 682, loss = 30.84730374
Iteration 683, loss = 31.73875999
Iteration 684, loss = 31.56841855
Iteration 685, loss = 30.80352061
Iteration 686, loss = 31.01019342
Iteration 687, loss = 30.76064006
Iteration 688, loss = 30.59143734
Iteration 689, loss = 31.13515531
Iteration 690, loss = 31.72669542
Iteration 691, loss = 31.70430807
Iteration 692, loss = 31.38059078
Iteration 693, loss = 31.20498084
Iteration 694, loss = 30.96917332
Iteration 695, loss = 30.78263921
Iteration 696, loss = 30.43075631
Iteration 697, loss = 30.60289650
Iteration 698, loss = 30.69273990
Iteration 699, loss = 31.33588863
Iteration 700, loss = 30.84493066
Iteration 701, loss = 31.09495744
Iteration 702, loss = 30.97191501
Iteration 703, loss = 31.04649721
Iteration 704, loss = 31.33985188
Iteration 705, loss = 31.26206116
Iteration 706, loss = 31.37886565
Iteration 707, loss = 30.98375044
Iteration 708, loss = 30.71800174
Iteration 709, loss = 30.81392554
Iteration 710, loss = 31.39672601
Iteration 711, loss = 30.81175151
Iteration 712, loss = 30.94486930
Iteration 713, loss = 30.44986989
Iteration 714, loss = 31.57092778
Iteration 715, loss = 30.40515774
Iteration 716, loss = 30.69279333
Iteration 717, loss = 30.42790260
Iteration 718, loss = 30.72109984
Iteration 719, loss = 30.21157926
Iteration 720, loss = 30.79028072
Iteration 721, loss = 31.09666891
Iteration 722, loss = 31.16381110
Iteration 723, loss = 31.22263175
Iteration 724, loss = 31.89240215
Iteration 725, loss = 30.88705825
Iteration 726, loss = 31.40604801
Iteration 727, loss = 32.03536039
Iteration 728, loss = 31.94042446
Iteration 729, loss = 31.32399912
Iteration 730, loss = 30.86340645
Iteration 731, loss = 31.86079524
Iteration 732, loss = 31.42283522
Iteration 733, loss = 30.68336300
Iteration 734, loss = 31.36642397
Iteration 735, loss = 30.43005463
Iteration 736, loss = 31.24747564
Iteration 737, loss = 30.82778087
Iteration 738, loss = 30.91870584
Iteration 739, loss = 31.88636617
Iteration 740, loss = 31.42778074
Iteration 741, loss = 30.69116444
Iteration 742, loss = 30.37301768
Iteration 743, loss = 30.14640933
Iteration 744, loss = 30.46006011
Iteration 745, loss = 30.81123132
Iteration 746, loss = 30.97934110
Iteration 747, loss = 30.30906391
Iteration 748, loss = 30.69861112
Iteration 749, loss = 30.23015865
Iteration 750, loss = 30.87297836
Iteration 751, loss = 31.46020557
Iteration 752, loss = 31.18707807
Iteration 753, loss = 30.58081334
Iteration 754, loss = 30.29212890
Iteration 755, loss = 30.43946608
Iteration 756, loss = 30.40762747
Iteration 757, loss = 30.43772659
Iteration 758, loss = 31.16056144
Iteration 759, loss = 30.75337270
Iteration 760, loss = 32.04796708
Iteration 761, loss = 31.69928112
Iteration 762, loss = 31.09306268
Iteration 763, loss = 30.77498535
Iteration 764, loss = 30.05746906
Iteration 765, loss = 30.59751763
Iteration 766, loss = 30.63030310
Iteration 767, loss = 30.56931042
Iteration 768, loss = 30.50015642
Iteration 769, loss = 30.56320813
Iteration 770, loss = 31.29935427
Iteration 771, loss = 31.40986436
Iteration 772, loss = 30.38448977
Iteration 773, loss = 30.34897063
Iteration 774, loss = 31.15555678
Iteration 775, loss = 30.56743214
Iteration 776, loss = 30.25201264
Iteration 777, loss = 30.48223539
Iteration 778, loss = 30.32912487
Iteration 779, loss = 30.16383075
Iteration 780, loss = 30.35984204
Iteration 781, loss = 30.31877345
Iteration 782, loss = 30.16642657
Iteration 783, loss = 30.37892056
Iteration 784, loss = 30.32937057
Iteration 785, loss = 30.64230517
Iteration 786, loss = 29.97810517
Iteration 787, loss = 30.47259485
Iteration 788, loss = 30.15757697
Iteration 789, loss = 29.98466501
Iteration 790, loss = 30.43944785
Iteration 791, loss = 29.90402873
Iteration 792, loss = 31.15966432
Iteration 793, loss = 31.60627743
Iteration 794, loss = 30.29680638
Iteration 795, loss = 30.63466804
Iteration 796, loss = 31.71172887
Iteration 797, loss = 30.99182158
Iteration 798, loss = 30.67333623
Iteration 799, loss = 30.45035757
Iteration 800, loss = 32.84982157
Iteration 801, loss = 31.13616568
Iteration 802, loss = 31.94277680
Iteration 803, loss = 30.63672991
Iteration 804, loss = 31.81149801
Iteration 805, loss = 31.52328158
Iteration 806, loss = 30.37259025
Iteration 807, loss = 30.31773049
Iteration 808, loss = 31.01281479
Iteration 809, loss = 30.31694444
Iteration 810, loss = 30.67818802
Iteration 811, loss = 30.44012907
Iteration 812, loss = 31.11061359
Iteration 813, loss = 31.87370391
Iteration 814, loss = 32.64814287
Iteration 815, loss = 32.16068796
Iteration 816, loss = 32.95885092
Iteration 817, loss = 34.13525043
Iteration 818, loss = 32.64293631
Iteration 819, loss = 33.93901050
Iteration 820, loss = 31.66132884
Iteration 821, loss = 33.18322435
Iteration 822, loss = 30.70276999
Iteration 823, loss = 31.27343919
Iteration 824, loss = 30.89192943
Iteration 825, loss = 31.46731362
Iteration 826, loss = 30.59012338
Iteration 827, loss = 30.91744204
Iteration 828, loss = 30.62733576
Iteration 829, loss = 30.60031828
Iteration 830, loss = 30.75142795
Iteration 831, loss = 31.40982490
Iteration 832, loss = 30.53167895
Iteration 833, loss = 30.57978188
Iteration 834, loss = 30.34831706
Iteration 835, loss = 30.59454740
Iteration 836, loss = 30.16940300
Iteration 837, loss = 30.47368456
Iteration 838, loss = 29.93965821
Iteration 839, loss = 29.96759044
Iteration 840, loss = 30.07491038
Iteration 841, loss = 30.33135432
Iteration 842, loss = 30.75824429
Iteration 843, loss = 30.41196592
Iteration 844, loss = 32.17160072
Iteration 845, loss = 34.14712664
Iteration 846, loss = 34.73750162
Iteration 847, loss = 34.77170393
Iteration 848, loss = 32.34928197
Iteration 849, loss = 31.02442892
Iteration 850, loss = 31.11334310
Iteration 851, loss = 30.82082478
Iteration 852, loss = 30.52527989
Iteration 853, loss = 30.31037518
Iteration 854, loss = 30.73677409
Iteration 855, loss = 31.10696525
Iteration 856, loss = 30.92050787
Iteration 857, loss = 30.72420426
Iteration 858, loss = 31.06491539
Iteration 859, loss = 31.13091486
Iteration 860, loss = 30.71155410
Iteration 861, loss = 31.10670703
Iteration 862, loss = 31.60804098
Iteration 863, loss = 30.49872067
Iteration 864, loss = 30.88389710
Iteration 865, loss = 31.16286080
Iteration 866, loss = 31.22310598
Iteration 867, loss = 31.25459441
Iteration 868, loss = 31.18739510
Iteration 869, loss = 30.94610060
Iteration 870, loss = 31.34973316
Iteration 871, loss = 30.92904383
Iteration 872, loss = 30.57088928
Iteration 873, loss = 30.60492876
Iteration 874, loss = 30.60192782
Iteration 875, loss = 31.15672364
Iteration 876, loss = 31.13596494
Iteration 877, loss = 30.97280586
Iteration 878, loss = 31.16287232
Iteration 879, loss = 30.24914464
Iteration 880, loss = 31.11995121
Iteration 881, loss = 31.68354932
Iteration 882, loss = 31.22262466
Iteration 883, loss = 30.67066204
Iteration 884, loss = 31.76731910
Iteration 885, loss = 31.29698743
Iteration 886, loss = 31.85230949
Iteration 887, loss = 32.89723675
Iteration 888, loss = 32.33155499
Iteration 889, loss = 31.55409421
Iteration 890, loss = 31.30256804
Iteration 891, loss = 31.43711768
Iteration 892, loss = 30.32495815
Iteration 893, loss = 30.47828065
Iteration 894, loss = 30.56488045
Iteration 895, loss = 30.97025365
Iteration 896, loss = 30.41809224
Iteration 897, loss = 30.98824048
Iteration 898, loss = 31.07463473
Iteration 899, loss = 31.60294214
Iteration 900, loss = 30.80497464
Iteration 901, loss = 31.05910922
Iteration 902, loss = 31.63549995
Iteration 903, loss = 31.48796695
Iteration 904, loss = 31.37346206
Iteration 905, loss = 30.66148384
Iteration 906, loss = 30.41519239
Iteration 907, loss = 31.57850345
Iteration 908, loss = 31.75879736
Iteration 909, loss = 32.00951431
Iteration 910, loss = 33.21369398
Iteration 911, loss = 32.61575246
Iteration 912, loss = 34.27084121
Iteration 913, loss = 31.06567738
Iteration 914, loss = 31.54310841
Iteration 915, loss = 31.29905690
Iteration 916, loss = 32.35601403
Iteration 917, loss = 30.31468934
Iteration 918, loss = 30.72511162
Iteration 919, loss = 30.56520467
Iteration 920, loss = 30.55310929
Iteration 921, loss = 30.34669741
Iteration 922, loss = 30.43988828
Iteration 923, loss = 30.98260939
Iteration 924, loss = 32.81740779
Iteration 925, loss = 32.72844267
Iteration 926, loss = 30.59702106
Iteration 927, loss = 31.54577578
Iteration 928, loss = 30.90569298
Iteration 929, loss = 32.59548771
Iteration 930, loss = 30.75883035
Iteration 931, loss = 31.96143298
Iteration 932, loss = 30.43637076
Iteration 933, loss = 31.96313032
Iteration 934, loss = 30.56373724
Iteration 935, loss = 31.16453679
Iteration 936, loss = 30.54421881
Iteration 937, loss = 30.42676770
Iteration 938, loss = 30.74836710
Iteration 939, loss = 31.38037994
Iteration 940, loss = 30.49203981
Iteration 941, loss = 30.56189196
Iteration 942, loss = 30.62813360
Iteration 943, loss = 31.11796068
Iteration 944, loss = 30.87781046
Iteration 945, loss = 30.44138048
Iteration 946, loss = 31.36891736
Iteration 947, loss = 30.23308670
Iteration 948, loss = 31.66854813
Iteration 949, loss = 31.47302414
Iteration 950, loss = 31.01711589
Iteration 951, loss = 30.95499191
Iteration 952, loss = 29.87809482
Iteration 953, loss = 30.32435836
Iteration 954, loss = 30.01529408
Iteration 955, loss = 29.99106728
Iteration 956, loss = 29.87914769
Iteration 957, loss = 29.94506666
Iteration 958, loss = 29.98421631
Iteration 959, loss = 30.36432559
Iteration 960, loss = 30.10738963
Iteration 961, loss = 30.06080332
Iteration 962, loss = 30.95501395
Iteration 963, loss = 30.12586797
Iteration 964, loss = 30.21163817
Iteration 965, loss = 31.16291827
Iteration 966, loss = 30.29050050
Iteration 967, loss = 31.04911569
Iteration 968, loss = 31.38307830
Iteration 969, loss = 31.50521775
Iteration 970, loss = 31.31443321
Iteration 971, loss = 32.07077005
Iteration 972, loss = 30.48249720
Iteration 973, loss = 30.96367350
Iteration 974, loss = 31.86388730
Iteration 975, loss = 31.51597991
Iteration 976, loss = 30.64306355
Iteration 977, loss = 31.73647408
Iteration 978, loss = 32.00329466
Iteration 979, loss = 31.23849622
Iteration 980, loss = 31.29991280
Iteration 981, loss = 32.42438944
Iteration 982, loss = 30.72712277
Iteration 983, loss = 31.20333068
Iteration 984, loss = 31.16947384
Iteration 985, loss = 32.61960575
Iteration 986, loss = 31.34816391
Iteration 987, loss = 31.51976614
Iteration 988, loss = 31.61382251
Iteration 989, loss = 32.29266275
Iteration 990, loss = 31.45926790
Iteration 991, loss = 31.47677909
Iteration 992, loss = 31.07866263
Iteration 993, loss = 30.52087013
Iteration 994, loss = 30.56198649
Iteration 995, loss = 30.49094519
Iteration 996, loss = 30.59690853
Iteration 997, loss = 30.99445383
Iteration 998, loss = 31.00345997
Iteration 999, loss = 31.39521979
Iteration 1000, loss = 31.98375268
Run 1
Iteration 1, loss = 2577.37098303
Iteration 2, loss = 389.22519135
Iteration 3, loss = 200.12318128
Iteration 4, loss = 133.34377575
Iteration 5, loss = 150.02607653
Iteration 6, loss = 142.21684449
Iteration 7, loss = 92.61040933
Iteration 8, loss = 88.70105508
Iteration 9, loss = 97.44138613
Iteration 10, loss = 79.84250460
Iteration 11, loss = 80.00915016
Iteration 12, loss = 76.56699893
Iteration 13, loss = 72.61724740
Iteration 14, loss = 75.54032160
Iteration 15, loss = 74.46560607
Iteration 16, loss = 74.31454909
Iteration 17, loss = 73.15147381
Iteration 18, loss = 71.55329332
Iteration 19, loss = 71.07742668
Iteration 20, loss = 71.96671428
Iteration 21, loss = 70.72611314
Iteration 22, loss = 71.11783083
Iteration 23, loss = 69.17650337
Iteration 24, loss = 69.48255712
Iteration 25, loss = 65.08259131
Iteration 26, loss = 63.79907008
Iteration 27, loss = 61.56424919
Iteration 28, loss = 59.72036300
Iteration 29, loss = 58.66982610
Iteration 30, loss = 56.56691509
Iteration 31, loss = 59.54169589
Iteration 32, loss = 55.64101280
Iteration 33, loss = 55.61685638
Iteration 34, loss = 54.19337571
Iteration 35, loss = 52.63375027
Iteration 36, loss = 52.95506285
Iteration 37, loss = 50.90196379
Iteration 38, loss = 49.73376495
Iteration 39, loss = 50.64122185
Iteration 40, loss = 49.19843645
Iteration 41, loss = 48.15734879
Iteration 42, loss = 47.33254394
Iteration 43, loss = 46.80555841
Iteration 44, loss = 48.96164844
Iteration 45, loss = 54.83221603
Iteration 46, loss = 50.47069799
Iteration 47, loss = 48.79757619
Iteration 48, loss = 48.61138834
Iteration 49, loss = 47.07110815
Iteration 50, loss = 47.81043824
Iteration 51, loss = 47.88430083
Iteration 52, loss = 50.98343880
Iteration 53, loss = 46.82534849
Iteration 54, loss = 45.97425327
Iteration 55, loss = 45.38833476
Iteration 56, loss = 42.64968101
Iteration 57, loss = 43.22825176
Iteration 58, loss = 41.92183052
Iteration 59, loss = 42.74805156
Iteration 60, loss = 41.04475992
Iteration 61, loss = 40.70337050
Iteration 62, loss = 41.51540398
Iteration 63, loss = 39.61712625
Iteration 64, loss = 40.18979059
Iteration 65, loss = 39.52346442
Iteration 66, loss = 38.63610702
Iteration 67, loss = 39.29369405
Iteration 68, loss = 45.84627941
Iteration 69, loss = 37.96437858
Iteration 70, loss = 37.63302620
Iteration 71, loss = 38.45455763
Iteration 72, loss = 36.66351089
Iteration 73, loss = 39.09231911
Iteration 74, loss = 40.47423757
Iteration 75, loss = 42.37478017
Iteration 76, loss = 41.91276953
Iteration 77, loss = 37.76021615
Iteration 78, loss = 36.85277461
Iteration 79, loss = 37.72206904
Iteration 80, loss = 37.28671381
Iteration 81, loss = 38.71089511
Iteration 82, loss = 36.19864468
Iteration 83, loss = 38.48763202
Iteration 84, loss = 39.64646965
Iteration 85, loss = 40.02878065
Iteration 86, loss = 41.27180002
Iteration 87, loss = 38.18991340
Iteration 88, loss = 38.29053970
Iteration 89, loss = 40.60825382
Iteration 90, loss = 38.90499378
Iteration 91, loss = 39.38194627
Iteration 92, loss = 38.55268522
Iteration 93, loss = 41.14380319
Iteration 94, loss = 39.03236462
Iteration 95, loss = 37.42065689
Iteration 96, loss = 38.02205612
Iteration 97, loss = 36.22835643
Iteration 98, loss = 37.67312520
Iteration 99, loss = 36.17188467
Iteration 100, loss = 38.15899035
Iteration 101, loss = 39.92556059
Iteration 102, loss = 38.01652143
Iteration 103, loss = 39.09872898
Iteration 104, loss = 43.57878744
Iteration 105, loss = 36.75060761
Iteration 106, loss = 37.92703588
Iteration 107, loss = 38.18755694
Iteration 108, loss = 36.66742600
Iteration 109, loss = 36.98159697
Iteration 110, loss = 38.03519398
Iteration 111, loss = 36.21168766
Iteration 112, loss = 36.28282189
Iteration 113, loss = 37.89641838
Iteration 114, loss = 36.09385216
Iteration 115, loss = 35.93696195
Iteration 116, loss = 35.38859867
Iteration 117, loss = 36.87671945
Iteration 118, loss = 40.62294080
Iteration 119, loss = 35.50885824
Iteration 120, loss = 36.31512284
Iteration 121, loss = 35.85925121
Iteration 122, loss = 37.86229776
Iteration 123, loss = 38.24368432
Iteration 124, loss = 39.42262345
Iteration 125, loss = 38.06224455
Iteration 126, loss = 36.91633181
Iteration 127, loss = 36.27321618
Iteration 128, loss = 35.56232884
Iteration 129, loss = 36.91095121
Iteration 130, loss = 36.25742779
Iteration 131, loss = 36.17499894
Iteration 132, loss = 35.86287200
Iteration 133, loss = 37.00390552
Iteration 134, loss = 35.21146222
Iteration 135, loss = 37.42718590
Iteration 136, loss = 35.98901237
Iteration 137, loss = 35.09148319
Iteration 138, loss = 34.86148926
Iteration 139, loss = 34.59214815
Iteration 140, loss = 35.23256409
Iteration 141, loss = 35.56926660
Iteration 142, loss = 34.48324279
Iteration 143, loss = 34.88343285
Iteration 144, loss = 37.08690955
Iteration 145, loss = 42.72058454
Iteration 146, loss = 38.22057993
Iteration 147, loss = 38.23361399
Iteration 148, loss = 41.29808548
Iteration 149, loss = 41.99492653
Iteration 150, loss = 43.92911267
Iteration 151, loss = 41.52065756
Iteration 152, loss = 42.27366353
Iteration 153, loss = 37.37324178
Iteration 154, loss = 40.48580093
Iteration 155, loss = 37.74591053
Iteration 156, loss = 35.92648936
Iteration 157, loss = 36.90491307
Iteration 158, loss = 36.65741408
Iteration 159, loss = 38.96487775
Iteration 160, loss = 36.36084954
Iteration 161, loss = 36.12269905
Iteration 162, loss = 37.88699694
Iteration 163, loss = 37.11438320
Iteration 164, loss = 35.14054518
Iteration 165, loss = 37.15586264
Iteration 166, loss = 38.16499749
Iteration 167, loss = 36.68769140
Iteration 168, loss = 37.01147767
Iteration 169, loss = 36.54645519
Iteration 170, loss = 35.91144709
Iteration 171, loss = 35.66058116
Iteration 172, loss = 34.30095368
Iteration 173, loss = 34.13661934
Iteration 174, loss = 35.20036075
Iteration 175, loss = 34.53744002
Iteration 176, loss = 34.72360899
Iteration 177, loss = 34.15118263
Iteration 178, loss = 35.74933995
Iteration 179, loss = 35.04600315
Iteration 180, loss = 34.32685078
Iteration 181, loss = 34.74574863
Iteration 182, loss = 36.96508313
Iteration 183, loss = 34.03962979
Iteration 184, loss = 35.05723667
Iteration 185, loss = 35.48553114
Iteration 186, loss = 35.82205640
Iteration 187, loss = 36.55038248
Iteration 188, loss = 35.66843711
Iteration 189, loss = 35.06569754
Iteration 190, loss = 37.09129567
Iteration 191, loss = 40.64158585
Iteration 192, loss = 35.55648012
Iteration 193, loss = 40.03424237
Iteration 194, loss = 38.40482980
Iteration 195, loss = 37.47113041
Iteration 196, loss = 36.01317901
Iteration 197, loss = 36.94364792
Iteration 198, loss = 37.35564362
Iteration 199, loss = 35.76337814
Iteration 200, loss = 34.17293272
Iteration 201, loss = 34.14966431
Iteration 202, loss = 34.87780820
Iteration 203, loss = 35.53219092
Iteration 204, loss = 35.15039668
Iteration 205, loss = 34.62590726
Iteration 206, loss = 34.01297363
Iteration 207, loss = 34.37765141
Iteration 208, loss = 34.22985375
Iteration 209, loss = 34.71050033
Iteration 210, loss = 35.52845103
Iteration 211, loss = 33.97295961
Iteration 212, loss = 34.52738206
Iteration 213, loss = 33.84963726
Iteration 214, loss = 33.85219546
Iteration 215, loss = 33.53806136
Iteration 216, loss = 33.70497879
Iteration 217, loss = 33.61600403
Iteration 218, loss = 33.88186015
Iteration 219, loss = 33.73252622
Iteration 220, loss = 33.71840064
Iteration 221, loss = 33.54424446
Iteration 222, loss = 34.02146152
Iteration 223, loss = 33.88506253
Iteration 224, loss = 34.49256766
Iteration 225, loss = 34.97851450
Iteration 226, loss = 33.66698215
Iteration 227, loss = 35.62550764
Iteration 228, loss = 33.63456919
Iteration 229, loss = 35.86796186
Iteration 230, loss = 34.16433527
Iteration 231, loss = 34.84508078
Iteration 232, loss = 36.21852650
Iteration 233, loss = 40.44290190
Iteration 234, loss = 37.04243875
Iteration 235, loss = 34.40387673
Iteration 236, loss = 33.45767524
Iteration 237, loss = 34.24304372
Iteration 238, loss = 33.54018078
Iteration 239, loss = 34.04814543
Iteration 240, loss = 33.48499414
Iteration 241, loss = 33.50589144
Iteration 242, loss = 35.36083553
Iteration 243, loss = 33.44245550
Iteration 244, loss = 35.59549572
Iteration 245, loss = 34.76659048
Iteration 246, loss = 35.50159945
Iteration 247, loss = 36.08711776
Iteration 248, loss = 36.68999006
Iteration 249, loss = 37.87162998
Iteration 250, loss = 38.88334902
Iteration 251, loss = 36.13645889
Iteration 252, loss = 36.31014431
Iteration 253, loss = 35.55600238
Iteration 254, loss = 35.59389282
Iteration 255, loss = 34.25329478
Iteration 256, loss = 33.52764679
Iteration 257, loss = 33.15154376
Iteration 258, loss = 33.37533058
Iteration 259, loss = 34.85375008
Iteration 260, loss = 33.48255198
Iteration 261, loss = 34.13596274
Iteration 262, loss = 33.72612219
Iteration 263, loss = 33.89823958
Iteration 264, loss = 35.94226744
Iteration 265, loss = 37.22007361
Iteration 266, loss = 34.01358540
Iteration 267, loss = 34.23323241
Iteration 268, loss = 33.50661821
Iteration 269, loss = 33.45998872
Iteration 270, loss = 33.25788602
Iteration 271, loss = 33.56611695
Iteration 272, loss = 33.82308441
Iteration 273, loss = 33.08054591
Iteration 274, loss = 33.76016306
Iteration 275, loss = 33.07974732
Iteration 276, loss = 33.13467525
Iteration 277, loss = 34.57625907
Iteration 278, loss = 34.53635383
Iteration 279, loss = 35.13049861
Iteration 280, loss = 34.14723592
Iteration 281, loss = 34.49386571
Iteration 282, loss = 34.95302892
Iteration 283, loss = 33.88625710
Iteration 284, loss = 34.92259463
Iteration 285, loss = 34.92669008
Iteration 286, loss = 32.68189650
Iteration 287, loss = 32.58003374
Iteration 288, loss = 33.91384357
Iteration 289, loss = 32.63715872
Iteration 290, loss = 34.45904144
Iteration 291, loss = 35.85882342
Iteration 292, loss = 34.69313781
Iteration 293, loss = 34.67439407
Iteration 294, loss = 34.67973037
Iteration 295, loss = 33.11865777
Iteration 296, loss = 32.22928974
Iteration 297, loss = 33.57476504
Iteration 298, loss = 33.06294244
Iteration 299, loss = 33.69641358
Iteration 300, loss = 32.27208965
Iteration 301, loss = 31.99729241
Iteration 302, loss = 32.12009045
Iteration 303, loss = 32.49009062
Iteration 304, loss = 31.59408472
Iteration 305, loss = 32.03538989
Iteration 306, loss = 31.99809341
Iteration 307, loss = 32.39856634
Iteration 308, loss = 32.28459527
Iteration 309, loss = 33.92307753
Iteration 310, loss = 34.17904777
Iteration 311, loss = 32.66891845
Iteration 312, loss = 33.86554169
Iteration 313, loss = 32.64704632
Iteration 314, loss = 32.08794063
Iteration 315, loss = 31.37713913
Iteration 316, loss = 31.97545350
Iteration 317, loss = 36.43321975
Iteration 318, loss = 35.96896371
Iteration 319, loss = 34.58185183
Iteration 320, loss = 34.91503429
Iteration 321, loss = 33.95300363
Iteration 322, loss = 34.62676255
Iteration 323, loss = 34.47738917
Iteration 324, loss = 36.64694846
Iteration 325, loss = 34.07487054
Iteration 326, loss = 35.66352441
Iteration 327, loss = 34.47197744
Iteration 328, loss = 31.46403457
Iteration 329, loss = 34.78044354
Iteration 330, loss = 35.76700409
Iteration 331, loss = 37.44469727
Iteration 332, loss = 34.00344179
Iteration 333, loss = 31.83391456
Iteration 334, loss = 32.85618821
Iteration 335, loss = 31.37810544
Iteration 336, loss = 32.82977257
Iteration 337, loss = 34.76580238
Iteration 338, loss = 36.39943420
Iteration 339, loss = 35.83000319
Iteration 340, loss = 34.78952496
Iteration 341, loss = 36.83607345
Iteration 342, loss = 35.94593671
Iteration 343, loss = 34.00275359
Iteration 344, loss = 32.15268518
Iteration 345, loss = 31.66200457
Iteration 346, loss = 31.77402191
Iteration 347, loss = 31.73668480
Iteration 348, loss = 31.48468106
Iteration 349, loss = 31.23568245
Iteration 350, loss = 31.37243116
Iteration 351, loss = 31.71175654
Iteration 352, loss = 31.14101004
Iteration 353, loss = 30.73179717
Iteration 354, loss = 32.63902209
Iteration 355, loss = 32.10992434
Iteration 356, loss = 31.58591649
Iteration 357, loss = 32.53270498
Iteration 358, loss = 31.70265379
Iteration 359, loss = 31.98681726
Iteration 360, loss = 33.04501314
Iteration 361, loss = 32.66158804
Iteration 362, loss = 35.07979588
Iteration 363, loss = 34.48271490
Iteration 364, loss = 31.77033020
Iteration 365, loss = 33.87215119
Iteration 366, loss = 32.88854629
Iteration 367, loss = 33.43863818
Iteration 368, loss = 32.80021847
Iteration 369, loss = 33.17243954
Iteration 370, loss = 32.25038773
Iteration 371, loss = 32.07039964
Iteration 372, loss = 31.07108668
Iteration 373, loss = 31.34056510
Iteration 374, loss = 31.88127113
Iteration 375, loss = 31.91344579
Iteration 376, loss = 30.92554999
Iteration 377, loss = 31.96794935
Iteration 378, loss = 31.13151220
Iteration 379, loss = 31.21902703
Iteration 380, loss = 31.65157237
Iteration 381, loss = 30.69398559
Iteration 382, loss = 31.62101134
Iteration 383, loss = 31.14369676
Iteration 384, loss = 30.57754108
Iteration 385, loss = 32.60831931
Iteration 386, loss = 31.65234165
Iteration 387, loss = 31.02704090
Iteration 388, loss = 32.00934466
Iteration 389, loss = 30.79689049
Iteration 390, loss = 31.06756864
Iteration 391, loss = 31.62820676
Iteration 392, loss = 31.21673804
Iteration 393, loss = 31.77717870
Iteration 394, loss = 31.71158505
Iteration 395, loss = 33.09830886
Iteration 396, loss = 31.29138642
Iteration 397, loss = 31.80662681
Iteration 398, loss = 30.86935693
Iteration 399, loss = 31.52138954
Iteration 400, loss = 32.00104074
Iteration 401, loss = 31.75531647
Iteration 402, loss = 31.77692859
Iteration 403, loss = 30.93509467
Iteration 404, loss = 31.19313251
Iteration 405, loss = 33.39839425
Iteration 406, loss = 33.54272127
Iteration 407, loss = 35.14859197
Iteration 408, loss = 32.24026656
Iteration 409, loss = 33.41108476
Iteration 410, loss = 32.94974170
Iteration 411, loss = 31.89723973
Iteration 412, loss = 31.83342950
Iteration 413, loss = 31.70702984
Iteration 414, loss = 32.17606969
Iteration 415, loss = 36.01056949
Iteration 416, loss = 35.48185602
Iteration 417, loss = 33.45159798
Iteration 418, loss = 32.58757859
Iteration 419, loss = 32.89064582
Iteration 420, loss = 32.45859489
Iteration 421, loss = 31.76040250
Iteration 422, loss = 30.82766943
Iteration 423, loss = 31.22651763
Iteration 424, loss = 31.66391835
Iteration 425, loss = 31.44349814
Iteration 426, loss = 30.78880434
Iteration 427, loss = 30.41334691
Iteration 428, loss = 31.00360352
Iteration 429, loss = 31.15592800
Iteration 430, loss = 31.73818291
Iteration 431, loss = 32.26686786
Iteration 432, loss = 30.58290933
Iteration 433, loss = 31.70975873
Iteration 434, loss = 31.14117622
Iteration 435, loss = 30.75989493
Iteration 436, loss = 30.69413997
Iteration 437, loss = 31.00249076
Iteration 438, loss = 31.02783525
Iteration 439, loss = 32.31718526
Iteration 440, loss = 33.23292599
Iteration 441, loss = 32.49986268
Iteration 442, loss = 34.49236177
Iteration 443, loss = 31.75223505
Iteration 444, loss = 32.89505765
Iteration 445, loss = 33.76096100
Iteration 446, loss = 31.79433625
Iteration 447, loss = 31.36443274
Iteration 448, loss = 30.82605673
Iteration 449, loss = 32.71470910
Iteration 450, loss = 33.29720335
Iteration 451, loss = 33.56781483
Iteration 452, loss = 33.05301961
Iteration 453, loss = 35.12185872
Iteration 454, loss = 33.23907493
Iteration 455, loss = 31.95890500
Iteration 456, loss = 32.91285500
Iteration 457, loss = 31.83582672
Iteration 458, loss = 32.11674004
Iteration 459, loss = 31.34774464
Iteration 460, loss = 31.18700707
Iteration 461, loss = 31.37436909
Iteration 462, loss = 31.33211523
Iteration 463, loss = 30.88332858
Iteration 464, loss = 31.08566643
Iteration 465, loss = 30.47044408
Iteration 466, loss = 30.74833671
Iteration 467, loss = 30.97405257
Iteration 468, loss = 31.09412446
Iteration 469, loss = 30.54884285
Iteration 470, loss = 31.29368245
Iteration 471, loss = 30.85511311
Iteration 472, loss = 31.79241854
Iteration 473, loss = 31.18819930
Iteration 474, loss = 31.32787026
Iteration 475, loss = 31.21815383
Iteration 476, loss = 30.97516529
Iteration 477, loss = 32.89799449
Iteration 478, loss = 33.14200522
Iteration 479, loss = 32.87245377
Iteration 480, loss = 33.17276865
Iteration 481, loss = 31.89773462
Iteration 482, loss = 30.87808546
Iteration 483, loss = 31.16769262
Iteration 484, loss = 31.00375281
Iteration 485, loss = 31.99034670
Iteration 486, loss = 31.19569278
Iteration 487, loss = 32.64016994
Iteration 488, loss = 31.65499073
Iteration 489, loss = 30.48148157
Iteration 490, loss = 31.11812797
Iteration 491, loss = 30.69069600
Iteration 492, loss = 31.29964554
Iteration 493, loss = 31.05390849
Iteration 494, loss = 30.86288207
Iteration 495, loss = 31.26970251
Iteration 496, loss = 32.00095136
Iteration 497, loss = 30.51659235
Iteration 498, loss = 31.30182080
Iteration 499, loss = 30.91422843
Iteration 500, loss = 30.83159516
Iteration 501, loss = 31.08626537
Iteration 502, loss = 30.81827799
Iteration 503, loss = 30.70190429
Iteration 504, loss = 31.79810976
Iteration 505, loss = 32.20726213
Iteration 506, loss = 30.72572255
Iteration 507, loss = 31.57454180
Iteration 508, loss = 30.73716670
Iteration 509, loss = 31.03605336
Iteration 510, loss = 31.65078087
Iteration 511, loss = 31.22559118
Iteration 512, loss = 31.40380646
Iteration 513, loss = 31.27539144
Iteration 514, loss = 32.00660701
Iteration 515, loss = 30.78621783
Iteration 516, loss = 33.98652112
Iteration 517, loss = 35.42216812
Iteration 518, loss = 35.06436211
Iteration 519, loss = 32.53477148
Iteration 520, loss = 32.82273606
Iteration 521, loss = 32.79459121
Iteration 522, loss = 33.03634857
Iteration 523, loss = 33.06641364
Iteration 524, loss = 34.47277860
Iteration 525, loss = 34.70101093
Iteration 526, loss = 34.44001448
Iteration 527, loss = 35.81494520
Iteration 528, loss = 33.91883299
Iteration 529, loss = 34.69120471
Iteration 530, loss = 32.15306867
Iteration 531, loss = 30.78101061
Iteration 532, loss = 30.94344536
Iteration 533, loss = 30.84299395
Iteration 534, loss = 31.21709740
Iteration 535, loss = 30.37392637
Iteration 536, loss = 31.31740436
Iteration 537, loss = 30.56200960
Iteration 538, loss = 31.20208587
Iteration 539, loss = 31.08430332
Iteration 540, loss = 31.15020826
Iteration 541, loss = 31.83725141
Iteration 542, loss = 32.78567194
Iteration 543, loss = 31.76263900
Iteration 544, loss = 30.83054935
Iteration 545, loss = 31.06827915
Iteration 546, loss = 30.84673802
Iteration 547, loss = 31.19558152
Iteration 548, loss = 30.95769926
Iteration 549, loss = 31.18701667
Iteration 550, loss = 31.11686869
Iteration 551, loss = 30.97703970
Iteration 552, loss = 30.95328488
Iteration 553, loss = 31.19142072
Iteration 554, loss = 31.04694317
Iteration 555, loss = 30.68081857
Iteration 556, loss = 32.26096713
Iteration 557, loss = 30.73161444
Iteration 558, loss = 31.94499027
Iteration 559, loss = 31.75466373
Iteration 560, loss = 31.69498974
Iteration 561, loss = 33.77850905
Iteration 562, loss = 37.14793569
Iteration 563, loss = 36.39468290
Iteration 564, loss = 36.45496799
Iteration 565, loss = 34.20188799
Iteration 566, loss = 32.66395072
Iteration 567, loss = 31.63246062
Iteration 568, loss = 31.34906813
Iteration 569, loss = 32.44617575
Iteration 570, loss = 32.05839517
Iteration 571, loss = 31.91192478
Iteration 572, loss = 31.17905865
Iteration 573, loss = 31.16596342
Iteration 574, loss = 32.20856229
Iteration 575, loss = 30.51644684
Iteration 576, loss = 30.90164690
Iteration 577, loss = 30.71042090
Iteration 578, loss = 30.46370161
Iteration 579, loss = 30.46940957
Iteration 580, loss = 30.70702041
Iteration 581, loss = 30.89730139
Iteration 582, loss = 30.58640668
Iteration 583, loss = 30.61899681
Iteration 584, loss = 31.30951217
Iteration 585, loss = 30.55540470
Iteration 586, loss = 30.72591244
Iteration 587, loss = 30.69792799
Iteration 588, loss = 30.67128999
Iteration 589, loss = 30.47982044
Iteration 590, loss = 30.88581758
Iteration 591, loss = 31.55525907
Iteration 592, loss = 31.33465982
Iteration 593, loss = 31.95256857
Iteration 594, loss = 33.08925916
Iteration 595, loss = 31.82202933
Iteration 596, loss = 31.97184933
Iteration 597, loss = 30.60341777
Iteration 598, loss = 30.85283019
Iteration 599, loss = 31.20993189
Iteration 600, loss = 31.06829981
Iteration 601, loss = 30.98769976
Iteration 602, loss = 31.03732330
Iteration 603, loss = 31.08794301
Iteration 604, loss = 31.61082079
Iteration 605, loss = 32.12290167
Iteration 606, loss = 31.05374914
Iteration 607, loss = 31.57899154
Iteration 608, loss = 30.27030686
Iteration 609, loss = 31.02984392
Iteration 610, loss = 30.78175898
Iteration 611, loss = 30.59585628
Iteration 612, loss = 30.74241148
Iteration 613, loss = 30.87348711
Iteration 614, loss = 30.38082561
Iteration 615, loss = 31.49650836
Iteration 616, loss = 31.09158123
Iteration 617, loss = 30.62252837
Iteration 618, loss = 30.67978936
Iteration 619, loss = 30.61643948
Iteration 620, loss = 30.12893194
Iteration 621, loss = 30.66327405
Iteration 622, loss = 30.51829879
Iteration 623, loss = 30.34455833
Iteration 624, loss = 30.47593222
Iteration 625, loss = 30.13860657
Iteration 626, loss = 30.69592488
Iteration 627, loss = 30.15205569
Iteration 628, loss = 30.17208080
Iteration 629, loss = 30.40609579
Iteration 630, loss = 30.63179704
Iteration 631, loss = 31.40762212
Iteration 632, loss = 31.05886163
Iteration 633, loss = 31.28859809
Iteration 634, loss = 31.25660850
Iteration 635, loss = 30.64122218
Iteration 636, loss = 30.83747674
Iteration 637, loss = 31.71276942
Iteration 638, loss = 30.82380934
Iteration 639, loss = 30.39080162
Iteration 640, loss = 31.44062779
Iteration 641, loss = 31.18692206
Iteration 642, loss = 31.41512409
Iteration 643, loss = 31.37598856
Iteration 644, loss = 31.81881959
Iteration 645, loss = 29.92542075
Iteration 646, loss = 30.45307724
Iteration 647, loss = 31.72559380
Iteration 648, loss = 32.15973537
Iteration 649, loss = 32.51158608
Iteration 650, loss = 31.95635136
Iteration 651, loss = 31.75289489
Iteration 652, loss = 30.58654084
Iteration 653, loss = 31.96933846
Iteration 654, loss = 33.14705846
Iteration 655, loss = 31.37794934
Iteration 656, loss = 30.92790781
Iteration 657, loss = 31.32527669
Iteration 658, loss = 30.45129948
Iteration 659, loss = 31.08150002
Iteration 660, loss = 30.92227441
Iteration 661, loss = 30.57589655
Iteration 662, loss = 30.29124839
Iteration 663, loss = 30.26824315
Iteration 664, loss = 30.38444549
Iteration 665, loss = 30.38747327
Iteration 666, loss = 30.22037828
Iteration 667, loss = 30.26953177
Iteration 668, loss = 30.24836676
Iteration 669, loss = 30.95391904
Iteration 670, loss = 30.94929117
Iteration 671, loss = 31.19915082
Iteration 672, loss = 31.68965542
Iteration 673, loss = 32.53851115
Iteration 674, loss = 34.80141423
Iteration 675, loss = 34.94959740
Iteration 676, loss = 32.10104738
Iteration 677, loss = 32.96470344
Iteration 678, loss = 31.16069568
Iteration 679, loss = 31.19403815
Iteration 680, loss = 30.86530894
Iteration 681, loss = 30.80437957
Iteration 682, loss = 30.86355503
Iteration 683, loss = 31.43886206
Iteration 684, loss = 31.13498089
Iteration 685, loss = 31.95363896
Iteration 686, loss = 31.28221469
Iteration 687, loss = 31.23847031
Iteration 688, loss = 30.70450664
Iteration 689, loss = 31.47985573
Iteration 690, loss = 32.86430083
Iteration 691, loss = 31.47251003
Iteration 692, loss = 33.60859414
Iteration 693, loss = 32.52939904
Iteration 694, loss = 33.25984191
Iteration 695, loss = 32.04824066
Iteration 696, loss = 30.76971833
Iteration 697, loss = 32.56355107
Iteration 698, loss = 30.04238778
Iteration 699, loss = 30.75420873
Iteration 700, loss = 30.51321873
Iteration 701, loss = 30.48886155
Iteration 702, loss = 30.48596216
Iteration 703, loss = 29.86508172
Iteration 704, loss = 30.32561791
Iteration 705, loss = 31.61891883
Iteration 706, loss = 31.27774291
Iteration 707, loss = 31.03445082
Iteration 708, loss = 30.78413832
Iteration 709, loss = 30.98581417
Iteration 710, loss = 30.27508850
Iteration 711, loss = 30.54069113
Iteration 712, loss = 30.06910726
Iteration 713, loss = 30.16122243
Iteration 714, loss = 30.92617682
Iteration 715, loss = 31.08366455
Iteration 716, loss = 32.38861057
Iteration 717, loss = 32.15478047
Iteration 718, loss = 31.74026112
Iteration 719, loss = 31.36186845
Iteration 720, loss = 30.93669816
Iteration 721, loss = 30.16058419
Iteration 722, loss = 30.84028145
Iteration 723, loss = 30.96266210
Iteration 724, loss = 33.43882146
Iteration 725, loss = 33.66892643
Iteration 726, loss = 32.56904757
Iteration 727, loss = 31.14619377
Iteration 728, loss = 30.77105545
Iteration 729, loss = 31.63058030
Iteration 730, loss = 31.25242402
Iteration 731, loss = 30.12329525
Iteration 732, loss = 30.40140386
Iteration 733, loss = 30.27040493
Iteration 734, loss = 30.34413193
Iteration 735, loss = 31.65382989
Iteration 736, loss = 31.53363078
Iteration 737, loss = 30.94954911
Iteration 738, loss = 32.07474085
Iteration 739, loss = 31.02289157
Iteration 740, loss = 31.02309842
Iteration 741, loss = 30.34356940
Iteration 742, loss = 31.54035323
Iteration 743, loss = 32.87035941
Iteration 744, loss = 32.90786198
Iteration 745, loss = 32.18892597
Iteration 746, loss = 33.82839601
Iteration 747, loss = 30.32609842
Iteration 748, loss = 30.45903121
Iteration 749, loss = 30.32130072
Iteration 750, loss = 30.24532392
Iteration 751, loss = 30.47857227
Iteration 752, loss = 31.69928674
Iteration 753, loss = 32.41346307
Iteration 754, loss = 30.67951255
Iteration 755, loss = 30.80255149
Iteration 756, loss = 30.41096977
Iteration 757, loss = 31.43973280
Iteration 758, loss = 31.38097900
Iteration 759, loss = 31.64759394
Iteration 760, loss = 30.76053626
Iteration 761, loss = 31.30451199
Iteration 762, loss = 31.94327057
Iteration 763, loss = 31.10401581
Iteration 764, loss = 31.73520842
Iteration 765, loss = 31.73747139
Iteration 766, loss = 33.13447274
Iteration 767, loss = 32.17838385
Iteration 768, loss = 31.00553132
Iteration 769, loss = 31.38204712
Iteration 770, loss = 31.08089381
Iteration 771, loss = 31.11477737
Iteration 772, loss = 32.15780112
Iteration 773, loss = 30.99345399
Iteration 774, loss = 31.10724896
Iteration 775, loss = 29.73082565
Iteration 776, loss = 30.00336611
Iteration 777, loss = 29.70489071
Iteration 778, loss = 29.83224191
Iteration 779, loss = 29.77108313
Iteration 780, loss = 29.71763732
Iteration 781, loss = 30.51302478
Iteration 782, loss = 30.86608495
Iteration 783, loss = 30.46961356
Iteration 784, loss = 29.96770110
Iteration 785, loss = 30.35790721
Iteration 786, loss = 29.84917784
Iteration 787, loss = 30.64316586
Iteration 788, loss = 31.80859037
Iteration 789, loss = 30.10167138
Iteration 790, loss = 31.14348262
Iteration 791, loss = 31.74531488
Iteration 792, loss = 31.09701732
Iteration 793, loss = 32.13343896
Iteration 794, loss = 30.62577119
Iteration 795, loss = 29.40956398
Iteration 796, loss = 30.05768001
Iteration 797, loss = 30.38926869
Iteration 798, loss = 29.89736372
Iteration 799, loss = 30.02889113
Iteration 800, loss = 29.47775992
Iteration 801, loss = 30.15236684
Iteration 802, loss = 30.45002952
Iteration 803, loss = 31.49547455
Iteration 804, loss = 31.78332453
Iteration 805, loss = 33.35266365
Iteration 806, loss = 31.63895050
Iteration 807, loss = 31.28564839
Iteration 808, loss = 30.40872889
Iteration 809, loss = 30.30674209
Iteration 810, loss = 31.03577032
Iteration 811, loss = 33.17644235
Iteration 812, loss = 31.72815300
Iteration 813, loss = 31.13177445
Iteration 814, loss = 30.51715958
Iteration 815, loss = 29.56788661
Iteration 816, loss = 29.67558797
Iteration 817, loss = 31.12006313
Iteration 818, loss = 30.13132179
Iteration 819, loss = 29.23999411
Iteration 820, loss = 29.83222506
Iteration 821, loss = 30.58754880
Iteration 822, loss = 31.81697402
Iteration 823, loss = 31.00193288
Iteration 824, loss = 31.61676142
Iteration 825, loss = 30.56711390
Iteration 826, loss = 29.49808512
Iteration 827, loss = 30.47440914
Iteration 828, loss = 30.94835679
Iteration 829, loss = 30.09472765
Iteration 830, loss = 32.60864048
Iteration 831, loss = 31.44116833
Iteration 832, loss = 30.93204351
Iteration 833, loss = 32.55222369
Iteration 834, loss = 34.37648689
Iteration 835, loss = 33.08955885
Iteration 836, loss = 32.08824934
Iteration 837, loss = 30.34067458
Iteration 838, loss = 29.80569354
Iteration 839, loss = 30.19120642
Iteration 840, loss = 29.44572358
Iteration 841, loss = 31.05644032
Iteration 842, loss = 32.63070957
Iteration 843, loss = 30.64750083
Iteration 844, loss = 30.38122288
Iteration 845, loss = 29.74245622
Iteration 846, loss = 32.16271829
Iteration 847, loss = 31.41957903
Iteration 848, loss = 33.04009591
Iteration 849, loss = 31.23086654
Iteration 850, loss = 30.22380009
Iteration 851, loss = 29.67153286
Iteration 852, loss = 29.28803671
Iteration 853, loss = 29.74195155
Iteration 854, loss = 30.17418190
Iteration 855, loss = 29.29564490
Iteration 856, loss = 30.67962306
Iteration 857, loss = 30.77120945
Iteration 858, loss = 35.26411634
Iteration 859, loss = 31.23724634
Iteration 860, loss = 31.41497307
Iteration 861, loss = 29.92089922
Iteration 862, loss = 31.10458136
Iteration 863, loss = 31.59944662
Iteration 864, loss = 30.20513249
Iteration 865, loss = 30.92658431
Iteration 866, loss = 29.58166006
Iteration 867, loss = 29.48410614
Iteration 868, loss = 29.54591699
Iteration 869, loss = 29.89031385
Iteration 870, loss = 30.12922709
Iteration 871, loss = 29.38267917
Iteration 872, loss = 31.22211218
Iteration 873, loss = 30.50603641
Iteration 874, loss = 31.02374791
Iteration 875, loss = 31.18224584
Iteration 876, loss = 30.85865631
Iteration 877, loss = 31.83623580
Iteration 878, loss = 30.04188646
Iteration 879, loss = 30.59318570
Iteration 880, loss = 29.89076491
Iteration 881, loss = 30.14874442
Iteration 882, loss = 31.08287261
Iteration 883, loss = 31.50622124
Iteration 884, loss = 32.20014554
Iteration 885, loss = 30.39461561
Iteration 886, loss = 31.31742395
Iteration 887, loss = 31.90601321
Iteration 888, loss = 31.14688158
Iteration 889, loss = 32.18280066
Iteration 890, loss = 32.15062110
Iteration 891, loss = 31.22898379
Iteration 892, loss = 30.77966488
Iteration 893, loss = 30.52614553
Iteration 894, loss = 28.84710599
Iteration 895, loss = 29.38998550
Iteration 896, loss = 30.48077102
Iteration 897, loss = 30.78829823
Iteration 898, loss = 29.41372596
Iteration 899, loss = 29.64308809
Iteration 900, loss = 29.66189783
Iteration 901, loss = 29.46138984
Iteration 902, loss = 29.57044543
Iteration 903, loss = 30.26295543
Iteration 904, loss = 29.53882935
Iteration 905, loss = 29.71473295
Iteration 906, loss = 30.49046271
Iteration 907, loss = 29.05322114
Iteration 908, loss = 29.25256100
Iteration 909, loss = 29.03398031
Iteration 910, loss = 28.96134357
Iteration 911, loss = 28.98183982
Iteration 912, loss = 29.14827715
Iteration 913, loss = 29.36035448
Iteration 914, loss = 29.35801989
Iteration 915, loss = 28.97491510
Iteration 916, loss = 29.19567271
Iteration 917, loss = 30.20861288
Iteration 918, loss = 29.89873869
Iteration 919, loss = 29.76615196
Iteration 920, loss = 29.20570995
Iteration 921, loss = 29.03738770
Iteration 922, loss = 29.32165679
Iteration 923, loss = 29.26946398
Iteration 924, loss = 29.53395049
Iteration 925, loss = 29.31255010
Iteration 926, loss = 29.11652412
Iteration 927, loss = 29.15609204
Iteration 928, loss = 29.46484459
Iteration 929, loss = 29.14080373
Iteration 930, loss = 29.29487789
Iteration 931, loss = 28.75573485
Iteration 932, loss = 29.70275512
Iteration 933, loss = 28.53757623
Iteration 934, loss = 29.66684302
Iteration 935, loss = 29.83147543
Iteration 936, loss = 29.16076022
Iteration 937, loss = 29.17705933
Iteration 938, loss = 29.46731781
Iteration 939, loss = 29.26560357
Iteration 940, loss = 29.38955564
Iteration 941, loss = 30.28361143
Iteration 942, loss = 34.21087436
Iteration 943, loss = 33.93572622
Iteration 944, loss = 30.69322757
Iteration 945, loss = 31.35446670
Iteration 946, loss = 30.26772885
Iteration 947, loss = 30.65703618
Iteration 948, loss = 30.02550826
Iteration 949, loss = 30.44381069
Iteration 950, loss = 31.74936400
Iteration 951, loss = 30.54278509
Iteration 952, loss = 30.03000234
Iteration 953, loss = 29.63991210
Iteration 954, loss = 29.46095934
Iteration 955, loss = 30.64876778
Iteration 956, loss = 33.20698971
Iteration 957, loss = 30.91189507
Iteration 958, loss = 31.42919279
Iteration 959, loss = 30.73702349
Iteration 960, loss = 28.93414464
Iteration 961, loss = 29.18213724
Iteration 962, loss = 29.26556709
Iteration 963, loss = 28.85765979
Iteration 964, loss = 29.70160449
Iteration 965, loss = 29.47046464
Iteration 966, loss = 28.99204097
Iteration 967, loss = 28.65131203
Iteration 968, loss = 28.71786772
Iteration 969, loss = 29.24707058
Iteration 970, loss = 29.02945790
Iteration 971, loss = 30.65626733
Iteration 972, loss = 31.69489046
Iteration 973, loss = 30.25258106
Iteration 974, loss = 30.79656249
Iteration 975, loss = 30.43387599
Iteration 976, loss = 30.29416977
Iteration 977, loss = 32.01656720
Iteration 978, loss = 30.25245990
Iteration 979, loss = 30.24227787
Iteration 980, loss = 30.83967098
Iteration 981, loss = 29.64755931
Iteration 982, loss = 30.12159236
Iteration 983, loss = 29.55048710
Iteration 984, loss = 29.76201663
Iteration 985, loss = 29.44884854
Iteration 986, loss = 29.21952069
Iteration 987, loss = 30.24025126
Iteration 988, loss = 31.17436564
Iteration 989, loss = 31.61416016
Iteration 990, loss = 33.48477454
Iteration 991, loss = 31.89432630
Iteration 992, loss = 30.25486527
Iteration 993, loss = 29.95452334
Iteration 994, loss = 29.38838398
Iteration 995, loss = 29.26398868
Iteration 996, loss = 28.80133475
Iteration 997, loss = 29.98620986
Iteration 998, loss = 28.90193386
Iteration 999, loss = 29.51084265
Iteration 1000, loss = 29.04944770
Run 2
Iteration 1, loss = 4533.75859709
Iteration 2, loss = 1057.47904764
Iteration 3, loss = 365.91073980
Iteration 4, loss = 338.05372644
Iteration 5, loss = 92.03134455
Iteration 6, loss = 149.01034924
Iteration 7, loss = 81.86482394
Iteration 8, loss = 103.38065550
Iteration 9, loss = 79.47082493
Iteration 10, loss = 79.55471363
Iteration 11, loss = 74.86781426
Iteration 12, loss = 74.39978506
Iteration 13, loss = 73.65505992
Iteration 14, loss = 75.24272959
Iteration 15, loss = 76.03810420
Iteration 16, loss = 72.74798734
Iteration 17, loss = 72.59777836
Iteration 18, loss = 73.57453805
Iteration 19, loss = 75.60047269
Iteration 20, loss = 74.59620554
Iteration 21, loss = 72.52513034
Iteration 22, loss = 71.55554794
Iteration 23, loss = 70.24682290
Iteration 24, loss = 69.96443650
Iteration 25, loss = 69.45599713
Iteration 26, loss = 68.69587384
Iteration 27, loss = 67.65765452
Iteration 28, loss = 65.38639041
Iteration 29, loss = 64.99575823
Iteration 30, loss = 64.97819880
Iteration 31, loss = 65.06463211
Iteration 32, loss = 65.84406669
Iteration 33, loss = 61.95446458
Iteration 34, loss = 62.20580440
Iteration 35, loss = 59.46113821
Iteration 36, loss = 58.83340901
Iteration 37, loss = 59.39970993
Iteration 38, loss = 56.31951824
Iteration 39, loss = 54.57858823
Iteration 40, loss = 53.30685373
Iteration 41, loss = 52.61370831
Iteration 42, loss = 51.53084281
Iteration 43, loss = 50.80050131
Iteration 44, loss = 49.88334365
Iteration 45, loss = 49.54138859
Iteration 46, loss = 48.16658218
Iteration 47, loss = 46.65366216
Iteration 48, loss = 45.91759733
Iteration 49, loss = 44.93929477
Iteration 50, loss = 44.42903679
Iteration 51, loss = 44.12027911
Iteration 52, loss = 44.24976715
Iteration 53, loss = 42.54016297
Iteration 54, loss = 41.93381191
Iteration 55, loss = 40.65699709
Iteration 56, loss = 39.75911224
Iteration 57, loss = 38.96560706
Iteration 58, loss = 38.19966907
Iteration 59, loss = 38.34891365
Iteration 60, loss = 38.45417635
Iteration 61, loss = 37.17781994
Iteration 62, loss = 36.59360009
Iteration 63, loss = 36.40685487
Iteration 64, loss = 37.34728904
Iteration 65, loss = 36.36160508
Iteration 66, loss = 36.30757807
Iteration 67, loss = 36.73706799
Iteration 68, loss = 37.31555885
Iteration 69, loss = 38.11747417
Iteration 70, loss = 39.84818097
Iteration 71, loss = 37.81722841
Iteration 72, loss = 36.63629084
Iteration 73, loss = 36.13237092
Iteration 74, loss = 35.71722035
Iteration 75, loss = 35.88957808
Iteration 76, loss = 36.09183367
Iteration 77, loss = 35.74316120
Iteration 78, loss = 36.15677269
Iteration 79, loss = 35.86557981
Iteration 80, loss = 35.44340061
Iteration 81, loss = 38.05281974
Iteration 82, loss = 35.43166039
Iteration 83, loss = 35.73022578
Iteration 84, loss = 35.25751474
Iteration 85, loss = 36.06802853
Iteration 86, loss = 36.13244153
Iteration 87, loss = 38.85182774
Iteration 88, loss = 36.69473271
Iteration 89, loss = 35.79823868
Iteration 90, loss = 36.41521374
Iteration 91, loss = 36.46633028
Iteration 92, loss = 35.99406580
Iteration 93, loss = 35.87151632
Iteration 94, loss = 36.56536414
Iteration 95, loss = 36.08878505
Iteration 96, loss = 35.61370764
Iteration 97, loss = 35.86280579
Iteration 98, loss = 36.41716161
Iteration 99, loss = 36.72456758
Iteration 100, loss = 35.99223551
Iteration 101, loss = 35.92583880
Iteration 102, loss = 37.61762046
Iteration 103, loss = 35.49158404
Iteration 104, loss = 36.53607687
Iteration 105, loss = 35.87720543
Iteration 106, loss = 34.98883247
Iteration 107, loss = 34.52125839
Iteration 108, loss = 34.93755944
Iteration 109, loss = 35.99989562
Iteration 110, loss = 39.35175849
Iteration 111, loss = 37.10577749
Iteration 112, loss = 38.31218998
Iteration 113, loss = 35.87696838
Iteration 114, loss = 36.40087469
Iteration 115, loss = 36.29200027
Iteration 116, loss = 37.72018939
Iteration 117, loss = 34.74496946
Iteration 118, loss = 35.02419689
Iteration 119, loss = 35.32733796
Iteration 120, loss = 34.46887205
Iteration 121, loss = 34.86546655
Iteration 122, loss = 34.79881719
Iteration 123, loss = 36.96206990
Iteration 124, loss = 37.04463559
Iteration 125, loss = 35.51304988
Iteration 126, loss = 33.79084687
Iteration 127, loss = 36.82951599
Iteration 128, loss = 35.13407650
Iteration 129, loss = 34.72023539
Iteration 130, loss = 35.78115342
Iteration 131, loss = 34.46420451
Iteration 132, loss = 35.64267974
Iteration 133, loss = 35.45326447
Iteration 134, loss = 34.49997118
Iteration 135, loss = 33.89824975
Iteration 136, loss = 34.07004867
Iteration 137, loss = 35.13550122
Iteration 138, loss = 35.58330679
Iteration 139, loss = 33.67588899
Iteration 140, loss = 35.37193452
Iteration 141, loss = 35.35508834
Iteration 142, loss = 34.96905262
Iteration 143, loss = 34.83259252
Iteration 144, loss = 35.59149985
Iteration 145, loss = 34.04359384
Iteration 146, loss = 33.70041901
Iteration 147, loss = 34.08830879
Iteration 148, loss = 34.36215240
Iteration 149, loss = 34.26106081
Iteration 150, loss = 39.10387088
Iteration 151, loss = 38.82521700
Iteration 152, loss = 40.14073391
Iteration 153, loss = 39.09079267
Iteration 154, loss = 37.12948320
Iteration 155, loss = 36.66826386
Iteration 156, loss = 38.39367139
Iteration 157, loss = 36.13662327
Iteration 158, loss = 36.06697185
Iteration 159, loss = 35.00711427
Iteration 160, loss = 35.23171275
Iteration 161, loss = 35.40154299
Iteration 162, loss = 35.86610920
Iteration 163, loss = 33.97779688
Iteration 164, loss = 33.50163492
Iteration 165, loss = 33.74685115
Iteration 166, loss = 36.27791400
Iteration 167, loss = 37.07719259
Iteration 168, loss = 36.16264563
Iteration 169, loss = 34.13433516
Iteration 170, loss = 34.81685068
Iteration 171, loss = 34.56757012
Iteration 172, loss = 33.98163212
Iteration 173, loss = 34.25391380
Iteration 174, loss = 34.18135073
Iteration 175, loss = 34.34240819
Iteration 176, loss = 34.25241830
Iteration 177, loss = 33.46048554
Iteration 178, loss = 35.34978020
Iteration 179, loss = 32.89905328
Iteration 180, loss = 33.91344554
Iteration 181, loss = 34.51795760
Iteration 182, loss = 33.76147100
Iteration 183, loss = 34.90904059
Iteration 184, loss = 35.10860906
Iteration 185, loss = 34.49022803
Iteration 186, loss = 33.66186883
Iteration 187, loss = 34.34538280
Iteration 188, loss = 33.66888817
Iteration 189, loss = 35.30591212
Iteration 190, loss = 33.96481105
Iteration 191, loss = 33.72801243
Iteration 192, loss = 33.19254751
Iteration 193, loss = 34.13062811
Iteration 194, loss = 34.77522020
Iteration 195, loss = 35.30637391
Iteration 196, loss = 33.66430142
Iteration 197, loss = 33.52036056
Iteration 198, loss = 33.25121263
Iteration 199, loss = 36.77157677
Iteration 200, loss = 34.82917440
Iteration 201, loss = 35.48749793
Iteration 202, loss = 34.37284821
Iteration 203, loss = 33.66430399
Iteration 204, loss = 33.93193769
Iteration 205, loss = 34.11212364
Iteration 206, loss = 34.71931769
Iteration 207, loss = 33.64881044
Iteration 208, loss = 36.33197351
Iteration 209, loss = 38.19388588
Iteration 210, loss = 36.76115087
Iteration 211, loss = 33.37746374
Iteration 212, loss = 34.20787034
Iteration 213, loss = 33.29724184
Iteration 214, loss = 33.55579354
Iteration 215, loss = 33.10688358
Iteration 216, loss = 33.30459247
Iteration 217, loss = 33.07876769
Iteration 218, loss = 33.31343275
Iteration 219, loss = 32.95064372
Iteration 220, loss = 34.93163386
Iteration 221, loss = 35.63001469
Iteration 222, loss = 35.41054104
Iteration 223, loss = 35.28869885
Iteration 224, loss = 35.17517556
Iteration 225, loss = 33.53069462
Iteration 226, loss = 33.18552833
Iteration 227, loss = 33.45170500
Iteration 228, loss = 33.24032352
Iteration 229, loss = 32.38378924
Iteration 230, loss = 34.58503181
Iteration 231, loss = 35.26947639
Iteration 232, loss = 35.17032791
Iteration 233, loss = 33.82083911
Iteration 234, loss = 34.54922015
Iteration 235, loss = 34.77125978
Iteration 236, loss = 32.50411500
Iteration 237, loss = 36.33181442
Iteration 238, loss = 32.10966237
Iteration 239, loss = 31.85484379
Iteration 240, loss = 32.90825742
Iteration 241, loss = 31.31976356
Iteration 242, loss = 34.61652732
Iteration 243, loss = 32.13341521
Iteration 244, loss = 32.32123756
Iteration 245, loss = 32.51164046
Iteration 246, loss = 31.97633014
Iteration 247, loss = 32.21534185
Iteration 248, loss = 33.45661926
Iteration 249, loss = 33.83574037
Iteration 250, loss = 31.95749831
Iteration 251, loss = 33.08893509
Iteration 252, loss = 34.06051950
Iteration 253, loss = 33.30445364
Iteration 254, loss = 33.21021305
Iteration 255, loss = 34.82380211
Iteration 256, loss = 33.80923183
Iteration 257, loss = 33.99725981
Iteration 258, loss = 34.05557331
Iteration 259, loss = 33.57619500
Iteration 260, loss = 35.46992160
Iteration 261, loss = 34.75567812
Iteration 262, loss = 33.44329637
Iteration 263, loss = 33.71225316
Iteration 264, loss = 33.59802029
Iteration 265, loss = 33.10544707
Iteration 266, loss = 32.76533620
Iteration 267, loss = 32.13730666
Iteration 268, loss = 31.83759151
Iteration 269, loss = 32.65431015
Iteration 270, loss = 31.38245573
Iteration 271, loss = 34.36410056
Iteration 272, loss = 33.86980069
Iteration 273, loss = 33.93785642
Iteration 274, loss = 32.65798800
Iteration 275, loss = 32.17713497
Iteration 276, loss = 31.85767161
Iteration 277, loss = 31.75844522
Iteration 278, loss = 31.28361208
Iteration 279, loss = 31.52693503
Iteration 280, loss = 31.75156739
Iteration 281, loss = 30.80297672
Iteration 282, loss = 30.90898780
Iteration 283, loss = 31.18171077
Iteration 284, loss = 30.29688920
Iteration 285, loss = 31.08799583
Iteration 286, loss = 31.61310378
Iteration 287, loss = 32.78869890
Iteration 288, loss = 32.60903611
Iteration 289, loss = 31.25746160
Iteration 290, loss = 31.52187095
Iteration 291, loss = 32.15267821
Iteration 292, loss = 31.12653457
Iteration 293, loss = 31.53333469
Iteration 294, loss = 31.87683592
Iteration 295, loss = 31.03122048
Iteration 296, loss = 30.95592184
Iteration 297, loss = 30.37775514
Iteration 298, loss = 30.64327020
Iteration 299, loss = 30.88646815
Iteration 300, loss = 30.61853773
Iteration 301, loss = 31.80091504
Iteration 302, loss = 31.64788889
Iteration 303, loss = 31.03153977
Iteration 304, loss = 30.95695541
Iteration 305, loss = 31.87468669
Iteration 306, loss = 31.94135765
Iteration 307, loss = 33.71968661
Iteration 308, loss = 33.46619361
Iteration 309, loss = 32.49327702
Iteration 310, loss = 32.07386827
Iteration 311, loss = 31.39644627
Iteration 312, loss = 31.70182321
Iteration 313, loss = 31.19210863
Iteration 314, loss = 30.83515789
Iteration 315, loss = 31.31595583
Iteration 316, loss = 31.18299629
Iteration 317, loss = 31.56173762
Iteration 318, loss = 31.12317810
Iteration 319, loss = 31.94194168
Iteration 320, loss = 31.85331230
Iteration 321, loss = 32.42930583
Iteration 322, loss = 31.99564768
Iteration 323, loss = 30.67399459
Iteration 324, loss = 30.99094069
Iteration 325, loss = 30.98560044
Iteration 326, loss = 31.47407276
Iteration 327, loss = 31.69248947
Iteration 328, loss = 31.03948674
Iteration 329, loss = 32.79758660
Iteration 330, loss = 30.96329593
Iteration 331, loss = 31.74039974
Iteration 332, loss = 31.05967274
Iteration 333, loss = 30.51816174
Iteration 334, loss = 31.45054672
Iteration 335, loss = 31.42227489
Iteration 336, loss = 30.98142199
Iteration 337, loss = 31.03218521
Iteration 338, loss = 30.89726844
Iteration 339, loss = 30.72181857
Iteration 340, loss = 30.61846866
Iteration 341, loss = 30.73449533
Iteration 342, loss = 30.75132017
Iteration 343, loss = 30.78042512
Iteration 344, loss = 30.91624963
Iteration 345, loss = 31.43852598
Iteration 346, loss = 31.10920966
Iteration 347, loss = 30.52993264
Iteration 348, loss = 30.94613101
Iteration 349, loss = 31.10193613
Iteration 350, loss = 31.04525195
Iteration 351, loss = 31.26041176
Iteration 352, loss = 32.80385981
Iteration 353, loss = 33.14793020
Iteration 354, loss = 31.87881750
Iteration 355, loss = 31.36040841
Iteration 356, loss = 30.60420205
Iteration 357, loss = 30.57750328
Iteration 358, loss = 31.35819067
Iteration 359, loss = 33.05872998
Iteration 360, loss = 33.06409866
Iteration 361, loss = 33.60604500
Iteration 362, loss = 32.55523516
Iteration 363, loss = 31.97982613
Iteration 364, loss = 31.09605388
Iteration 365, loss = 31.22860333
Iteration 366, loss = 32.06794840
Iteration 367, loss = 32.78569983
Iteration 368, loss = 33.23150595
Iteration 369, loss = 33.64073885
Iteration 370, loss = 31.86418703
Iteration 371, loss = 32.60923586
Iteration 372, loss = 31.17926403
Iteration 373, loss = 30.47963430
Iteration 374, loss = 31.40951935
Iteration 375, loss = 30.21470955
Iteration 376, loss = 30.95938042
Iteration 377, loss = 31.86552544
Iteration 378, loss = 31.28699549
Iteration 379, loss = 31.93069641
Iteration 380, loss = 30.51656405
Iteration 381, loss = 30.40853604
Iteration 382, loss = 31.98619399
Iteration 383, loss = 30.27376357
Iteration 384, loss = 30.51443565
Iteration 385, loss = 31.04109189
Iteration 386, loss = 30.86597034
Iteration 387, loss = 31.71212481
Iteration 388, loss = 33.46227931
Iteration 389, loss = 32.35340606
Iteration 390, loss = 32.87215094
Iteration 391, loss = 31.40749324
Iteration 392, loss = 33.25527150
Iteration 393, loss = 32.72295319
Iteration 394, loss = 31.72232057
Iteration 395, loss = 32.02083515
Iteration 396, loss = 31.65981321
Iteration 397, loss = 31.21372914
Iteration 398, loss = 32.41220894
Iteration 399, loss = 31.38686058
Iteration 400, loss = 31.80614349
Iteration 401, loss = 31.35185031
Iteration 402, loss = 31.37176487
Iteration 403, loss = 31.06746094
Iteration 404, loss = 31.01462622
Iteration 405, loss = 30.54467013
Iteration 406, loss = 30.78267941
Iteration 407, loss = 31.25554017
Iteration 408, loss = 30.97448098
Iteration 409, loss = 30.44770278
Iteration 410, loss = 30.61159538
Iteration 411, loss = 30.90930960
Iteration 412, loss = 32.17119347
Iteration 413, loss = 33.35461478
Iteration 414, loss = 31.14427252
Iteration 415, loss = 31.95300307
Iteration 416, loss = 30.99431008
Iteration 417, loss = 32.11394072
Iteration 418, loss = 30.93579143
Iteration 419, loss = 31.11960565
Iteration 420, loss = 31.19366383
Iteration 421, loss = 31.12567808
Iteration 422, loss = 30.14254744
Iteration 423, loss = 31.56043542
Iteration 424, loss = 31.70228313
Iteration 425, loss = 31.19302135
Iteration 426, loss = 30.59795759
Iteration 427, loss = 32.57063613
Iteration 428, loss = 31.09666301
Iteration 429, loss = 30.71174627
Iteration 430, loss = 30.83394908
Iteration 431, loss = 31.48242244
Iteration 432, loss = 31.09505003
Iteration 433, loss = 30.57851039
Iteration 434, loss = 31.26511238
Iteration 435, loss = 30.72144098
Iteration 436, loss = 30.97691472
Iteration 437, loss = 30.97521831
Iteration 438, loss = 30.52700886
Iteration 439, loss = 30.94810599
Iteration 440, loss = 30.39638895
Iteration 441, loss = 30.59152152
Iteration 442, loss = 30.58238240
Iteration 443, loss = 30.50764090
Iteration 444, loss = 30.32456266
Iteration 445, loss = 30.39329384
Iteration 446, loss = 30.48623902
Iteration 447, loss = 31.71287359
Iteration 448, loss = 32.33207809
Iteration 449, loss = 31.60644231
Iteration 450, loss = 31.19669129
Iteration 451, loss = 30.37373183
Iteration 452, loss = 30.52954717
Iteration 453, loss = 30.36238761
Iteration 454, loss = 30.83733474
Iteration 455, loss = 31.25568034
Iteration 456, loss = 31.35963208
Iteration 457, loss = 30.98544725
Iteration 458, loss = 30.89738645
Iteration 459, loss = 30.49981879
Iteration 460, loss = 30.94625599
Iteration 461, loss = 30.80829871
Iteration 462, loss = 32.03890706
Iteration 463, loss = 32.43623463
Iteration 464, loss = 33.81357025
Iteration 465, loss = 32.17924193
Iteration 466, loss = 33.50447657
Iteration 467, loss = 33.21318121
Iteration 468, loss = 33.32626188
Iteration 469, loss = 32.83087395
Iteration 470, loss = 33.22013513
Iteration 471, loss = 33.68316990
Iteration 472, loss = 32.92831005
Iteration 473, loss = 31.41131028
Iteration 474, loss = 33.40550707
Iteration 475, loss = 32.49610539
Iteration 476, loss = 33.31812270
Iteration 477, loss = 31.17356772
Iteration 478, loss = 31.44874529
Iteration 479, loss = 31.10614318
Iteration 480, loss = 30.60474697
Iteration 481, loss = 31.46028549
Iteration 482, loss = 30.95700677
Iteration 483, loss = 30.69873899
Iteration 484, loss = 30.55360066
Iteration 485, loss = 30.64629925
Iteration 486, loss = 30.56584038
Iteration 487, loss = 30.46292865
Iteration 488, loss = 30.80664786
Iteration 489, loss = 30.37333085
Iteration 490, loss = 30.90455181
Iteration 491, loss = 30.81670970
Iteration 492, loss = 34.46866814
Iteration 493, loss = 35.39711322
Iteration 494, loss = 35.63147226
Iteration 495, loss = 32.85089476
Iteration 496, loss = 33.72161587
Iteration 497, loss = 31.12471992
Iteration 498, loss = 31.83897036
Iteration 499, loss = 30.73954942
Iteration 500, loss = 31.92990946
Iteration 501, loss = 32.68805295
Iteration 502, loss = 32.20744401
Iteration 503, loss = 31.90014563
Iteration 504, loss = 32.10460252
Iteration 505, loss = 31.05220138
Iteration 506, loss = 31.05426454
Iteration 507, loss = 30.50461698
Iteration 508, loss = 30.79852303
Iteration 509, loss = 30.61466639
Iteration 510, loss = 30.36536257
Iteration 511, loss = 30.60042837
Iteration 512, loss = 31.09303412
Iteration 513, loss = 30.42732484
Iteration 514, loss = 30.42619819
Iteration 515, loss = 30.29849492
Iteration 516, loss = 30.60250959
Iteration 517, loss = 31.77380740
Iteration 518, loss = 31.22656298
Iteration 519, loss = 31.66675626
Iteration 520, loss = 32.03526166
Iteration 521, loss = 33.58178107
Iteration 522, loss = 31.54992554
Iteration 523, loss = 30.94728477
Iteration 524, loss = 32.86727235
Iteration 525, loss = 31.87735474
Iteration 526, loss = 31.36585361
Iteration 527, loss = 30.45036384
Iteration 528, loss = 30.49844154
Iteration 529, loss = 30.37687120
Iteration 530, loss = 31.34128375
Iteration 531, loss = 30.00654374
Iteration 532, loss = 31.08583665
Iteration 533, loss = 31.11244947
Iteration 534, loss = 31.39705068
Iteration 535, loss = 30.11876136
Iteration 536, loss = 30.09202354
Iteration 537, loss = 30.96810053
Iteration 538, loss = 32.41499821
Iteration 539, loss = 31.59879231
Iteration 540, loss = 32.59696446
Iteration 541, loss = 32.65138773
Iteration 542, loss = 31.35342240
Iteration 543, loss = 31.66794385
Iteration 544, loss = 31.60307350
Iteration 545, loss = 31.67774297
Iteration 546, loss = 31.01397500
Iteration 547, loss = 31.00614664
Iteration 548, loss = 30.68218081
Iteration 549, loss = 30.48821187
Iteration 550, loss = 30.54681796
Iteration 551, loss = 30.33900101
Iteration 552, loss = 30.84683200
Iteration 553, loss = 32.14838242
Iteration 554, loss = 31.20887313
Iteration 555, loss = 30.33586351
Iteration 556, loss = 29.83683055
Iteration 557, loss = 30.57558246
Iteration 558, loss = 30.19304610
Iteration 559, loss = 30.01343981
Iteration 560, loss = 30.23587378
Iteration 561, loss = 30.62897144
Iteration 562, loss = 30.38959856
Iteration 563, loss = 30.42482917
Iteration 564, loss = 30.08602784
Iteration 565, loss = 30.32841160
Iteration 566, loss = 30.36859184
Iteration 567, loss = 30.00801415
Iteration 568, loss = 30.44739914
Iteration 569, loss = 30.71347688
Iteration 570, loss = 30.27459966
Iteration 571, loss = 30.20166206
Iteration 572, loss = 30.02731860
Iteration 573, loss = 30.20547623
Iteration 574, loss = 30.51190095
Iteration 575, loss = 30.92319112
Iteration 576, loss = 31.29376900
Iteration 577, loss = 30.57470588
Iteration 578, loss = 31.63100687
Iteration 579, loss = 32.63466250
Iteration 580, loss = 32.02231147
Iteration 581, loss = 31.09140565
Iteration 582, loss = 31.36546177
Iteration 583, loss = 31.14688517
Iteration 584, loss = 31.83388579
Iteration 585, loss = 31.50871638
Iteration 586, loss = 30.59165067
Iteration 587, loss = 30.41838310
Iteration 588, loss = 30.24016294
Iteration 589, loss = 31.16772593
Iteration 590, loss = 30.76046627
Iteration 591, loss = 30.96195718
Iteration 592, loss = 30.16109273
Iteration 593, loss = 30.02435285
Iteration 594, loss = 30.19539117
Iteration 595, loss = 29.95652539
Iteration 596, loss = 30.43133055
Iteration 597, loss = 31.02854433
Iteration 598, loss = 31.32442617
Iteration 599, loss = 32.24520999
Iteration 600, loss = 31.22621778
Iteration 601, loss = 30.76544694
Iteration 602, loss = 30.13014042
Iteration 603, loss = 30.04620179
Iteration 604, loss = 30.53366771
Iteration 605, loss = 30.40377312
Iteration 606, loss = 30.14094894
Iteration 607, loss = 30.27974362
Iteration 608, loss = 31.18564760
Iteration 609, loss = 30.67107863
Iteration 610, loss = 31.05184976
Iteration 611, loss = 30.51370181
Iteration 612, loss = 30.75007843
Iteration 613, loss = 31.50825102
Iteration 614, loss = 31.18769307
Iteration 615, loss = 32.01538342
Iteration 616, loss = 31.39171794
Iteration 617, loss = 31.79097402
Iteration 618, loss = 30.34041000
Iteration 619, loss = 30.31537443
Iteration 620, loss = 32.19642012
Iteration 621, loss = 31.59816406
Iteration 622, loss = 32.35519001
Iteration 623, loss = 31.33114442
Iteration 624, loss = 30.68481355
Iteration 625, loss = 30.26340497
Iteration 626, loss = 30.10599825
Iteration 627, loss = 30.32868886
Iteration 628, loss = 29.82205807
Iteration 629, loss = 30.04401528
Iteration 630, loss = 30.41028325
Iteration 631, loss = 30.44220550
Iteration 632, loss = 31.05950988
Iteration 633, loss = 30.21121635
Iteration 634, loss = 30.01681552
Iteration 635, loss = 29.96286096
Iteration 636, loss = 30.34264721
Iteration 637, loss = 31.15555607
Iteration 638, loss = 31.53064274
Iteration 639, loss = 32.31121330
Iteration 640, loss = 31.20440753
Iteration 641, loss = 32.71249160
Iteration 642, loss = 30.68281082
Iteration 643, loss = 33.10456858
Iteration 644, loss = 32.77840654
Iteration 645, loss = 31.81437815
Iteration 646, loss = 31.76450415
Iteration 647, loss = 30.44130321
Iteration 648, loss = 32.53419599
Iteration 649, loss = 31.25340118
Iteration 650, loss = 31.56898357
Iteration 651, loss = 31.53841725
Iteration 652, loss = 31.11637791
Iteration 653, loss = 30.99093256
Iteration 654, loss = 30.57381743
Iteration 655, loss = 31.26087270
Iteration 656, loss = 32.32663210
Iteration 657, loss = 31.36309202
Iteration 658, loss = 30.41591896
Iteration 659, loss = 30.47108994
Iteration 660, loss = 31.10761634
Iteration 661, loss = 29.74503992
Iteration 662, loss = 30.36923962
Iteration 663, loss = 30.26582596
Iteration 664, loss = 29.92134769
Iteration 665, loss = 30.89696001
Iteration 666, loss = 30.57692627
Iteration 667, loss = 30.96815991
Iteration 668, loss = 31.46583183
Iteration 669, loss = 32.49899746
Iteration 670, loss = 30.84448458
Iteration 671, loss = 31.12547528
Iteration 672, loss = 30.78047216
Iteration 673, loss = 31.31441885
Iteration 674, loss = 30.56465382
Iteration 675, loss = 29.94005594
Iteration 676, loss = 30.91422704
Iteration 677, loss = 30.75150518
Iteration 678, loss = 31.05051348
Iteration 679, loss = 31.12837424
Iteration 680, loss = 31.12097506
Iteration 681, loss = 31.09871597
Iteration 682, loss = 31.32652154
Iteration 683, loss = 30.58260192
Iteration 684, loss = 30.92713363
Iteration 685, loss = 31.48711084
Iteration 686, loss = 32.21033405
Iteration 687, loss = 32.67435638
Iteration 688, loss = 30.97861751
Iteration 689, loss = 30.52669814
Iteration 690, loss = 30.53720047
Iteration 691, loss = 30.76370842
Iteration 692, loss = 30.03926986
Iteration 693, loss = 31.14793006
Iteration 694, loss = 30.41376881
Iteration 695, loss = 30.43205415
Iteration 696, loss = 30.59219240
Iteration 697, loss = 29.84160669
Iteration 698, loss = 29.76998839
Iteration 699, loss = 29.60430080
Iteration 700, loss = 30.05514687
Iteration 701, loss = 30.01528609
Iteration 702, loss = 30.47094412
Iteration 703, loss = 31.85989806
Iteration 704, loss = 31.40707800
Iteration 705, loss = 31.86035635
Iteration 706, loss = 30.80718418
Iteration 707, loss = 31.14820393
Iteration 708, loss = 30.47063413
Iteration 709, loss = 30.34473314
Iteration 710, loss = 30.05944968
Iteration 711, loss = 30.22534991
Iteration 712, loss = 31.05527749
Iteration 713, loss = 32.62885656
Iteration 714, loss = 32.55335053
Iteration 715, loss = 31.74016098
Iteration 716, loss = 31.34557767
Iteration 717, loss = 31.05086741
Iteration 718, loss = 31.46949703
Iteration 719, loss = 30.47649250
Iteration 720, loss = 31.40080055
Iteration 721, loss = 30.30245410
Iteration 722, loss = 30.24094684
Iteration 723, loss = 31.35873620
Iteration 724, loss = 30.38500689
Iteration 725, loss = 30.77287207
Iteration 726, loss = 30.66739245
Iteration 727, loss = 30.34631631
Iteration 728, loss = 30.02979236
Iteration 729, loss = 30.64084238
Iteration 730, loss = 30.26260520
Iteration 731, loss = 30.16092583
Iteration 732, loss = 29.64479907
Iteration 733, loss = 29.62846260
Iteration 734, loss = 29.83782183
Iteration 735, loss = 29.73888447
Iteration 736, loss = 30.08383591
Iteration 737, loss = 30.02112872
Iteration 738, loss = 30.68960223
Iteration 739, loss = 30.55605229
Iteration 740, loss = 31.79966654
Iteration 741, loss = 30.16199342
Iteration 742, loss = 29.64477769
Iteration 743, loss = 31.17224246
Iteration 744, loss = 29.56801771
Iteration 745, loss = 30.96779612
Iteration 746, loss = 31.19593810
Iteration 747, loss = 31.11716507
Iteration 748, loss = 30.79320750
Iteration 749, loss = 30.08517442
Iteration 750, loss = 30.37874846
Iteration 751, loss = 29.82769061
Iteration 752, loss = 29.82705113
Iteration 753, loss = 29.59709987
Iteration 754, loss = 29.59089184
Iteration 755, loss = 30.80209743
Iteration 756, loss = 29.54775052
Iteration 757, loss = 29.55649120
Iteration 758, loss = 29.68029679
Iteration 759, loss = 30.19486560
Iteration 760, loss = 30.40327010
Iteration 761, loss = 29.93674495
Iteration 762, loss = 29.68245402
Iteration 763, loss = 29.79698223
Iteration 764, loss = 29.64823567
Iteration 765, loss = 29.73102225
Iteration 766, loss = 31.00634227
Iteration 767, loss = 31.98797210
Iteration 768, loss = 31.16314002
Iteration 769, loss = 32.56309180
Iteration 770, loss = 32.02049615
Iteration 771, loss = 31.05867849
Iteration 772, loss = 31.06200059
Iteration 773, loss = 30.79417576
Iteration 774, loss = 31.68066533
Iteration 775, loss = 30.80496594
Iteration 776, loss = 29.66716858
Iteration 777, loss = 29.88930895
Iteration 778, loss = 29.99979476
Iteration 779, loss = 30.82441510
Iteration 780, loss = 30.17270876
Iteration 781, loss = 30.16960753
Iteration 782, loss = 30.31839149
Iteration 783, loss = 29.79363961
Iteration 784, loss = 29.54178201
Iteration 785, loss = 29.63945499
Iteration 786, loss = 29.53596784
Iteration 787, loss = 31.06585335
Iteration 788, loss = 29.65478071
Iteration 789, loss = 30.15159295
Iteration 790, loss = 30.25339550
Iteration 791, loss = 29.41958697
Iteration 792, loss = 30.69696169
Iteration 793, loss = 29.80206420
Iteration 794, loss = 29.77650963
Iteration 795, loss = 29.82481980
Iteration 796, loss = 29.82841700
Iteration 797, loss = 29.81934311
Iteration 798, loss = 30.17511002
Iteration 799, loss = 31.00662275
Iteration 800, loss = 31.04724638
Iteration 801, loss = 31.30571627
Iteration 802, loss = 31.98926574
Iteration 803, loss = 30.27094414
Iteration 804, loss = 30.87602593
Iteration 805, loss = 30.53448479
Iteration 806, loss = 31.52966486
Iteration 807, loss = 31.04516623
Iteration 808, loss = 31.55781401
Iteration 809, loss = 31.32710092
Iteration 810, loss = 30.30808142
Iteration 811, loss = 30.25820937
Iteration 812, loss = 30.44608080
Iteration 813, loss = 29.87523979
Iteration 814, loss = 29.96266116
Iteration 815, loss = 29.95079532
Iteration 816, loss = 29.98316246
Iteration 817, loss = 29.90475948
Iteration 818, loss = 30.14417692
Iteration 819, loss = 30.24184529
Iteration 820, loss = 29.63371970
Iteration 821, loss = 30.56612637
Iteration 822, loss = 30.29719924
Iteration 823, loss = 29.99990093
Iteration 824, loss = 30.04884305
Iteration 825, loss = 29.56937360
Iteration 826, loss = 29.89701089
Iteration 827, loss = 29.29701906
Iteration 828, loss = 30.06601050
Iteration 829, loss = 30.95759543
Iteration 830, loss = 31.25429320
Iteration 831, loss = 32.22259919
Iteration 832, loss = 30.37640768
Iteration 833, loss = 31.05219375
Iteration 834, loss = 30.42189607
Iteration 835, loss = 32.71874455
Iteration 836, loss = 33.45868068
Iteration 837, loss = 31.53898139
Iteration 838, loss = 31.59404013
Iteration 839, loss = 31.16969312
Iteration 840, loss = 31.37424485
Iteration 841, loss = 31.02876350
Iteration 842, loss = 30.27889741
Iteration 843, loss = 33.42202368
Iteration 844, loss = 33.27074903
Iteration 845, loss = 32.38483804
Iteration 846, loss = 32.88638106
Iteration 847, loss = 32.91950099
Iteration 848, loss = 30.64769776
Iteration 849, loss = 32.47356992
Iteration 850, loss = 31.78983631
Iteration 851, loss = 32.04705676
Iteration 852, loss = 31.57927206
Iteration 853, loss = 30.43296983
Iteration 854, loss = 30.64788149
Iteration 855, loss = 29.93202302
Iteration 856, loss = 29.80324417
Iteration 857, loss = 29.98871508
Iteration 858, loss = 30.36911989
Iteration 859, loss = 30.38817013
Iteration 860, loss = 30.15974654
Iteration 861, loss = 30.04724144
Iteration 862, loss = 30.24444009
Iteration 863, loss = 30.11196294
Iteration 864, loss = 29.82982616
Iteration 865, loss = 29.48783163
Iteration 866, loss = 30.11498516
Iteration 867, loss = 30.45008882
Iteration 868, loss = 29.72211491
Iteration 869, loss = 30.32014439
Iteration 870, loss = 31.65793768
Iteration 871, loss = 31.56711511
Iteration 872, loss = 32.07806151
Iteration 873, loss = 32.21950743
Iteration 874, loss = 34.71990649
Iteration 875, loss = 35.34752821
Iteration 876, loss = 33.64059672
Iteration 877, loss = 34.26353272
Iteration 878, loss = 35.47017815
Iteration 879, loss = 35.52869456
Iteration 880, loss = 35.59039600
Iteration 881, loss = 34.55284852
Iteration 882, loss = 36.76388934
Iteration 883, loss = 35.23259874
Iteration 884, loss = 35.35260537
Iteration 885, loss = 33.95564748
Iteration 886, loss = 32.27767562
Iteration 887, loss = 33.16977361
Iteration 888, loss = 30.63684561
Iteration 889, loss = 31.32658413
Iteration 890, loss = 31.90759084
Iteration 891, loss = 31.80417202
Iteration 892, loss = 31.03640450
Iteration 893, loss = 30.02920176
Iteration 894, loss = 29.78313480
Iteration 895, loss = 29.74801893
Iteration 896, loss = 30.01143800
Iteration 897, loss = 30.61064108
Iteration 898, loss = 30.05968986
Iteration 899, loss = 30.81289538
Iteration 900, loss = 30.98726313
Iteration 901, loss = 31.14599606
Iteration 902, loss = 31.73245932
Iteration 903, loss = 29.67614751
Iteration 904, loss = 30.56234971
Iteration 905, loss = 29.64811802
Iteration 906, loss = 30.60712148
Iteration 907, loss = 29.97168208
Iteration 908, loss = 30.13488570
Iteration 909, loss = 30.48283976
Iteration 910, loss = 30.64297261
Iteration 911, loss = 30.43420259
Iteration 912, loss = 30.85543847
Iteration 913, loss = 31.62275548
Iteration 914, loss = 32.22801929
Iteration 915, loss = 30.50492342
Iteration 916, loss = 30.18304651
Iteration 917, loss = 29.50721762
Iteration 918, loss = 30.74916503
Iteration 919, loss = 29.93595604
Iteration 920, loss = 30.09060331
Iteration 921, loss = 30.00119329
Iteration 922, loss = 30.07488522
Iteration 923, loss = 29.39579165
Iteration 924, loss = 29.90678700
Iteration 925, loss = 30.81130608
Iteration 926, loss = 30.78063613
Iteration 927, loss = 29.70818995
Iteration 928, loss = 30.59578162
Iteration 929, loss = 31.50359003
Iteration 930, loss = 32.08589708
Iteration 931, loss = 30.68902621
Iteration 932, loss = 30.26738796
Iteration 933, loss = 29.56303864
Iteration 934, loss = 29.88884620
Iteration 935, loss = 30.11758345
Iteration 936, loss = 30.74344289
Iteration 937, loss = 29.16271663
Iteration 938, loss = 29.65628341
Iteration 939, loss = 29.75880270
Iteration 940, loss = 29.45514960
Iteration 941, loss = 29.76895940
Iteration 942, loss = 29.83394942
Iteration 943, loss = 30.35268658
Iteration 944, loss = 30.95648584
Iteration 945, loss = 30.91409023
Iteration 946, loss = 30.13836275
Iteration 947, loss = 30.06498154
Iteration 948, loss = 30.24312325
Iteration 949, loss = 30.04779864
Iteration 950, loss = 29.91822584
Iteration 951, loss = 32.42397309
Iteration 952, loss = 32.67824366
Iteration 953, loss = 31.69465833
Iteration 954, loss = 30.06600755
Iteration 955, loss = 29.92184076
Iteration 956, loss = 29.90072906
Iteration 957, loss = 29.70525840
Iteration 958, loss = 29.80467017
Iteration 959, loss = 30.16867649
Iteration 960, loss = 30.06569341
Iteration 961, loss = 29.71636973
Iteration 962, loss = 29.98330165
Iteration 963, loss = 29.99658684
Iteration 964, loss = 30.53211140
Iteration 965, loss = 29.74160385
Iteration 966, loss = 29.71399533
Iteration 967, loss = 30.87519697
Iteration 968, loss = 30.74478532
Iteration 969, loss = 30.38794536
Iteration 970, loss = 33.62054399
Iteration 971, loss = 33.09907325
Iteration 972, loss = 32.19845772
Iteration 973, loss = 31.57727277
Iteration 974, loss = 29.36326011
Iteration 975, loss = 29.77287529
Iteration 976, loss = 30.71479224
Iteration 977, loss = 30.33607658
Iteration 978, loss = 30.40647759
Iteration 979, loss = 30.28523536
Iteration 980, loss = 30.36874761
Iteration 981, loss = 30.61027331
Iteration 982, loss = 31.11894688
Iteration 983, loss = 30.96488453
Iteration 984, loss = 29.98134008
Iteration 985, loss = 30.16642358
Iteration 986, loss = 30.32535131
Iteration 987, loss = 30.55960993
Iteration 988, loss = 31.28707947
Iteration 989, loss = 31.21391641
Iteration 990, loss = 31.18858633
Iteration 991, loss = 31.31757975
Iteration 992, loss = 30.05397514
Iteration 993, loss = 29.60714409
Iteration 994, loss = 29.62178782
Iteration 995, loss = 29.85835947
Iteration 996, loss = 29.74771453
Iteration 997, loss = 29.74144681
Iteration 998, loss = 29.34407075
Iteration 999, loss = 29.90961783
Iteration 1000, loss = 30.80568347
*** fcst ***
[[43.75556411 42.82801302 41.66180188]
 [42.44482843 41.80029971 40.95403746]
 [41.13409276 40.86717086 40.24906319]
 [39.82335708 39.95186253 39.54408893]
 [38.5126214  39.05531782 38.83911466]
 [37.20188572 37.99780243 38.00830526]
 [35.89115005 36.65190458 36.24314274]
 [34.56791324 35.28575233 33.98471372]
 [33.16639053 33.43142213 31.78558084]
 [31.68516741 28.35643701 31.92449518]
 [30.60026273 26.9805605  29.91321702]
 [29.52144492 25.65725655 27.92385457]
 [28.44208715 25.77186291 26.55460012]
 [29.19472524 25.90195848 27.39872579]
 [30.37369803 26.68522088 28.80669573]
 [32.33780874 29.695815   29.61319396]
 [36.99818128 35.63825896 34.28728858]
 [41.74217395 40.86413563 40.76347336]
 [46.15437422 44.17408287 44.03711074]
 [50.12628433 45.96320925 46.29485357]
 [48.81554865 45.83671992 47.10262733]
 [47.50481297 44.53595886 46.26107199]
 [46.1940773  43.35592585 44.16906905]
 [44.88334162 42.34813845 42.03838929]
 [43.57260594 41.41128138 41.32869081]
 [42.26187026 40.51046481 40.61899232]
 [40.95113459 39.61392011 39.90929384]
 [39.64039891 38.71737541 39.20071191]
 [38.32966323 37.8208307  38.49218289]
 [37.0044991  36.84951089 37.74653092]
 [35.60297639 35.49842197 36.39715668]
 [34.20145368 34.13226972 34.11800549]
 [32.75885509 32.76611746 31.91830049]
 [34.29313713 31.01659039 35.18131755]
 [33.45080757 29.65507379 33.68345983]
 [32.37136753 29.04118648 31.69829496]
 [31.29132555 29.14346177 29.69947164]
 [31.84964564 28.88326647 29.74544096]
 [32.70501007 28.2339024  30.31028945]
 [31.56281253 29.68662928 29.88535373]
 [33.2574975  34.36679671 30.29206017]
 [38.15661637 37.81004703 35.33620308]
 [42.90523756 41.62627977 41.21552941]
 [47.11240999 43.4751029  44.00891029]
 [48.63259048 44.18236409 44.70813762]
 [47.3218548  43.07577075 44.54794752]
 [46.01111912 42.05892418 43.28852793]
 [44.70038345 41.12239555 41.70527822]
 [43.38964777 40.1875822  40.99557973]
 [42.07891209 39.27929358 40.28588125]
 [40.76817642 38.37943299 39.57618276]
 [39.44108497 37.48288829 38.86648428]
 [38.03956226 36.58634358 38.14659744]
 [36.63803955 35.68979888 37.40737523]
 [35.21101264 34.34493936 36.40139783]
 [33.74983756 32.97878711 34.26434424]
 [32.28866249 31.61263485 32.0415251 ]
 [20.48224614 16.211177   19.07445322]
 [18.83828961 14.71813271 18.15662349]
 [17.38096474 13.44596266 17.35343924]
 [16.34961551 13.94675693 16.55025499]
 [19.38473672 13.78596863 16.37378408]
 [23.0051387  17.50584667 19.92567792]
 [32.27783515 25.25768688 28.39093426]
 [42.14718419 37.18291175 40.6354441 ]
 [47.47967444 44.29230541 45.60334598]
 [50.65751974 46.47574544 47.89889797]
 [51.45805446 47.56709762 49.8037647 ]
 [50.47072853 46.76071577 49.18058181]
 [49.48221924 45.89124144 48.26448667]
 [48.33807967 45.01181746 47.35225588]
 [47.09004493 44.13239348 46.44858479]
 [45.77930926 43.15207425 44.24498111]
 [44.46857358 41.79808217 43.40297767]
 [43.1578379  40.49534957 42.57220369]
 [41.84710222 39.31671544 41.73577262]
 [40.53636655 38.08246572 40.88841312]
 [39.22563087 36.01263019 39.57606499]
 [37.91489519 33.58647043 37.28812158]
 [36.60415951 31.08311145 34.76480827]
 [35.29342384 28.55125491 32.14026491]
 [22.38389003 20.26019248 21.27714761]
 [20.88259547 19.17977342 20.4478505 ]
 [19.44731977 17.79346311 19.63997974]
 [18.17744172 17.53051637 18.94935282]
 [20.74548051 17.60172171 18.65091018]
 [24.51410986 20.32296885 21.79169087]
 [33.01684423 26.96454945 29.43014036]
 [42.89983093 38.3437712  41.39444511]
 [51.92910457 45.70983165 47.10133497]
 [52.56364496 47.86081794 50.04827354]
 [51.57631903 49.49926803 50.37394652]
 [50.5705552  48.6402391  49.45785138]
 [49.42641562 47.72414857 48.54175624]
 [48.21782244 46.81149255 47.6256611 ]
 [46.90708676 45.55362281 45.895137  ]
 [45.59635109 44.19338982 43.76584301]
 [44.28561541 42.88342247 43.06086875]
 [42.97487973 41.68288204 42.3500595 ]
 [41.66414405 40.48234161 41.51362843]
 [40.35340838 39.27066533 40.67256495]]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV   42.74846 2021-12-17 15:00:00          200
1    CroixChodes_SP_CM_FT002.F.PV  41.733055 2021-12-17 16:00:00          200
2    CroixChodes_SP_CM_FT002.F.PV  40.750109 2021-12-17 17:00:00          200
3    CroixChodes_SP_CM_FT002.F.PV  39.773103 2021-12-17 18:00:00          200
4    CroixChodes_SP_CM_FT002.F.PV  38.802351 2021-12-17 19:00:00          200
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV  44.518528 2021-12-21 15:00:00          200
97   CroixChodes_SP_CM_FT002.F.PV  43.409969 2021-12-21 16:00:00          200
98   CroixChodes_SP_CM_FT002.F.PV   42.33594 2021-12-21 17:00:00          200
99   CroixChodes_SP_CM_FT002.F.PV  41.220038 2021-12-21 18:00:00          200
100  CroixChodes_SP_CM_FT002.F.PV   40.09888 2021-12-21 19:00:00          200

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT002.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV   42.74846 2021-12-17 15:00:00          200
1    CroixChodes_SP_CM_FT002.F.PV  41.733055 2021-12-17 16:00:00          200
2    CroixChodes_SP_CM_FT002.F.PV  40.750109 2021-12-17 17:00:00          200
3    CroixChodes_SP_CM_FT002.F.PV  39.773103 2021-12-17 18:00:00          200
4    CroixChodes_SP_CM_FT002.F.PV  38.802351 2021-12-17 19:00:00          200
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV  44.518528 2021-12-21 15:00:00          200
97   CroixChodes_SP_CM_FT002.F.PV  43.409969 2021-12-21 16:00:00          200
98   CroixChodes_SP_CM_FT002.F.PV   42.33594 2021-12-21 17:00:00          200
99   CroixChodes_SP_CM_FT002.F.PV  41.220038 2021-12-21 18:00:00          200
100  CroixChodes_SP_CM_FT002.F.PV   40.09888 2021-12-21 19:00:00          200

[101 rows x 4 columns]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV  43.755564 2021-12-17 15:00:00          201
1    CroixChodes_SP_CM_FT002.F.PV  42.444828 2021-12-17 16:00:00          201
2    CroixChodes_SP_CM_FT002.F.PV  41.134093 2021-12-17 17:00:00          201
3    CroixChodes_SP_CM_FT002.F.PV  39.823357 2021-12-17 18:00:00          201
4    CroixChodes_SP_CM_FT002.F.PV  38.512621 2021-12-17 19:00:00          201
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV  45.596351 2021-12-21 15:00:00          201
97   CroixChodes_SP_CM_FT002.F.PV  44.285615 2021-12-21 16:00:00          201
98   CroixChodes_SP_CM_FT002.F.PV   42.97488 2021-12-21 17:00:00          201
99   CroixChodes_SP_CM_FT002.F.PV  41.664144 2021-12-21 18:00:00          201
100  CroixChodes_SP_CM_FT002.F.PV  40.353408 2021-12-21 19:00:00          201

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT002.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV  43.755564 2021-12-17 15:00:00          201
1    CroixChodes_SP_CM_FT002.F.PV  42.444828 2021-12-17 16:00:00          201
2    CroixChodes_SP_CM_FT002.F.PV  41.134093 2021-12-17 17:00:00          201
3    CroixChodes_SP_CM_FT002.F.PV  39.823357 2021-12-17 18:00:00          201
4    CroixChodes_SP_CM_FT002.F.PV  38.512621 2021-12-17 19:00:00          201
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV  45.596351 2021-12-21 15:00:00          201
97   CroixChodes_SP_CM_FT002.F.PV  44.285615 2021-12-21 16:00:00          201
98   CroixChodes_SP_CM_FT002.F.PV   42.97488 2021-12-21 17:00:00          201
99   CroixChodes_SP_CM_FT002.F.PV  41.664144 2021-12-21 18:00:00          201
100  CroixChodes_SP_CM_FT002.F.PV  40.353408 2021-12-21 19:00:00          201

[101 rows x 4 columns]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV  42.828013 2021-12-17 15:00:00          202
1    CroixChodes_SP_CM_FT002.F.PV    41.8003 2021-12-17 16:00:00          202
2    CroixChodes_SP_CM_FT002.F.PV  40.867171 2021-12-17 17:00:00          202
3    CroixChodes_SP_CM_FT002.F.PV  39.951863 2021-12-17 18:00:00          202
4    CroixChodes_SP_CM_FT002.F.PV  39.055318 2021-12-17 19:00:00          202
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV   44.19339 2021-12-21 15:00:00          202
97   CroixChodes_SP_CM_FT002.F.PV  42.883422 2021-12-21 16:00:00          202
98   CroixChodes_SP_CM_FT002.F.PV  41.682882 2021-12-21 17:00:00          202
99   CroixChodes_SP_CM_FT002.F.PV  40.482342 2021-12-21 18:00:00          202
100  CroixChodes_SP_CM_FT002.F.PV  39.270665 2021-12-21 19:00:00          202

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT002.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV  42.828013 2021-12-17 15:00:00          202
1    CroixChodes_SP_CM_FT002.F.PV    41.8003 2021-12-17 16:00:00          202
2    CroixChodes_SP_CM_FT002.F.PV  40.867171 2021-12-17 17:00:00          202
3    CroixChodes_SP_CM_FT002.F.PV  39.951863 2021-12-17 18:00:00          202
4    CroixChodes_SP_CM_FT002.F.PV  39.055318 2021-12-17 19:00:00          202
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV   44.19339 2021-12-21 15:00:00          202
97   CroixChodes_SP_CM_FT002.F.PV  42.883422 2021-12-21 16:00:00          202
98   CroixChodes_SP_CM_FT002.F.PV  41.682882 2021-12-21 17:00:00          202
99   CroixChodes_SP_CM_FT002.F.PV  40.482342 2021-12-21 18:00:00          202
100  CroixChodes_SP_CM_FT002.F.PV  39.270665 2021-12-21 19:00:00          202

[101 rows x 4 columns]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV  41.661802 2021-12-17 15:00:00          203
1    CroixChodes_SP_CM_FT002.F.PV  40.954037 2021-12-17 16:00:00          203
2    CroixChodes_SP_CM_FT002.F.PV  40.249063 2021-12-17 17:00:00          203
3    CroixChodes_SP_CM_FT002.F.PV  39.544089 2021-12-17 18:00:00          203
4    CroixChodes_SP_CM_FT002.F.PV  38.839115 2021-12-17 19:00:00          203
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV  43.765843 2021-12-21 15:00:00          203
97   CroixChodes_SP_CM_FT002.F.PV  43.060869 2021-12-21 16:00:00          203
98   CroixChodes_SP_CM_FT002.F.PV   42.35006 2021-12-21 17:00:00          203
99   CroixChodes_SP_CM_FT002.F.PV  41.513628 2021-12-21 18:00:00          203
100  CroixChodes_SP_CM_FT002.F.PV  40.672565 2021-12-21 19:00:00          203

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT002.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT002.F.PV  41.661802 2021-12-17 15:00:00          203
1    CroixChodes_SP_CM_FT002.F.PV  40.954037 2021-12-17 16:00:00          203
2    CroixChodes_SP_CM_FT002.F.PV  40.249063 2021-12-17 17:00:00          203
3    CroixChodes_SP_CM_FT002.F.PV  39.544089 2021-12-17 18:00:00          203
4    CroixChodes_SP_CM_FT002.F.PV  38.839115 2021-12-17 19:00:00          203
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT002.F.PV  43.765843 2021-12-21 15:00:00          203
97   CroixChodes_SP_CM_FT002.F.PV  43.060869 2021-12-21 16:00:00          203
98   CroixChodes_SP_CM_FT002.F.PV   42.35006 2021-12-21 17:00:00          203
99   CroixChodes_SP_CM_FT002.F.PV  41.513628 2021-12-21 18:00:00          203
100  CroixChodes_SP_CM_FT002.F.PV  40.672565 2021-12-21 19:00:00          203

[101 rows x 4 columns]


5 cpt.inputdata CroixChodes_SP_CM_FT003.F.PV cpt.ao 60.0 1440.0 30240.0 MLPDYNAMIC
DELETE FROM cpt.ao WHERE scadaid='CroixChodes_SP_CM_FT003.F.PV'... done
SELECT * FROM cpt.inputdata WHERE scadaid='CroixChodes_SP_CM_FT003.F.PV' ORDER BY scadadatetime DESC... done
*** ldate, fdate ***
2021-12-17 14:15:00 2021-11-18 19:30:00
>>> Water demand prediction : TOF 2021-12-20 18:00:00
*** start_train, stop_train, start_pred, stop_pred, toftime ***
2021-11-26 15:00:00 2021-12-17 15:00:00 2021-12-17 15:00:00 2021-12-21 19:00:00 2021-12-20 18:00:00
SELECT * FROM cpt.inputdata WHERE scadaid='CroixChodes_SP_CM_FT003.F.PV' ORDER BY scadadatetime ASC, rnum ASC... done
*** df_in ***
        rnum       scadadatetime                       scadaid  scadavalue scadaquality
0     333866 2021-11-18 19:30:00  CroixChodes_SP_CM_FT003.F.PV       0.000         None
1     333905 2021-11-18 20:00:00  CroixChodes_SP_CM_FT003.F.PV       0.000         None
2     333915 2021-11-18 20:00:00  CroixChodes_SP_CM_FT003.F.PV       0.000         None
3     333945 2021-11-18 20:15:00  CroixChodes_SP_CM_FT003.F.PV       0.000         None
4     333955 2021-11-18 20:15:00  CroixChodes_SP_CM_FT003.F.PV       0.000         None
...      ...                 ...                           ...         ...          ...
6358  498784 2021-12-17 14:00:00  CroixChodes_SP_CM_FT003.F.PV      -2.835         None
6359  498803 2021-12-17 14:00:00  CroixChodes_SP_CM_FT003.F.PV      -2.835         None
6360  498824 2021-12-17 14:00:00  CroixChodes_SP_CM_FT003.F.PV      -2.835         None
6361  498840 2021-12-17 14:15:00  CroixChodes_SP_CM_FT003.F.PV      -5.424         None
6362  498866 2021-12-17 14:15:00  CroixChodes_SP_CM_FT003.F.PV      -5.424         None

[6363 rows x 5 columns]
*** X_train, y_train, df_X, df_y, X_pred ***
[[330. 330. 330. ... 351. 351. 351.]
 [  5.   5.   5. ...   5.   5.   5.]
 [ 15.  16.  17. ...  12.  13.  14.]] [ 6.6328      4.9225      7.02116667  0.74666667  3.62816667  4.53783333
  6.25766667 -1.2885     -5.82883333 -5.778      -5.77583333 -5.73733333
 -5.73383333 -1.0685      7.6112      3.1575     -2.12016667  5.18583333
  0.36466667  4.32666667  6.91583333  4.4045      7.14625     2.1245
  6.9655      3.197       3.39094444  3.58488889  3.77883333  3.97277778
  4.16672222  4.36066667  4.55461111  4.74855556  4.9425      5.13644444
  5.33038889  5.52433333  5.71827778  5.91222222  6.10616667  6.30011111
  6.49405556  6.688       5.293       5.819       1.51483333  6.51083333
  3.153      -0.4715     -5.752      -5.73883333 -5.717       2.67716667
  0.4585      4.07366667  5.48566667  0.39033333  6.03216667 -5.68583333
 -5.712      -5.7095     -5.70116667  0.9295     -2.21683333 -5.62216667
  3.63116667 -1.99733333 -1.96916667  3.5005     -0.23666667  4.70816667
 -2.63666667 -3.66733333 -1.81875     1.1185      2.17883333 -3.61516667
 -2.214      -6.03716667  0.5596      4.39766667 -0.40883333  1.0315
  0.6625     -3.4695     -2.75583333  3.12933333  2.156      -1.84533333
 -2.08916667  4.6455      1.05166667  1.68983333 -1.2695      3.714
 -2.02616667  1.99083333  0.0265      3.84483333 -0.3225      2.02833333
  2.19016667 -3.78266667  0.93        1.8872      4.17216667  1.775
 -5.67783333  4.15483333 -5.64833333  2.5985      2.87366667  2.77983333
  4.25166667  2.47333333  1.381       2.8515     -2.46633333  2.204
 -3.74583333 -1.37783333  3.015       1.5615      1.583       1.454
  1.375       5.19083333  3.50783333  4.87816667  5.97383333  4.25183333
 -5.37433333 -0.84816667 -0.359       1.09433333 -0.91666667  0.33783333
  0.5865      0.6335      2.74975     5.1285     -0.03675     4.78933333
  4.45766667  3.68183333  1.31433333  5.581       4.58816667  3.70583333
  5.70233333  4.31966667  2.4485      5.3995      4.33533333  3.68116667
  5.31966667  1.86716667 -1.8825      1.39733333  4.87625     5.10116667
  5.75716667 -3.01366667 -1.3445      3.96166667 -3.3112      1.46383333
  3.5105      1.18483333  0.54033333  2.2562      6.28533333  7.80016667
 -5.627       3.48016667 -0.20016667  7.7955     -5.538      -3.33583333
 -4.09283333  7.83233333  6.96933333  7.734       1.13383333  3.2735
  3.4865     -1.09383333  7.41416667  3.084       7.86383333  1.2345
  3.44366667  5.74333333  5.00933333 -5.59516667 -5.57216667 -2.26275
 -1.93133333  2.9125      7.75033333  1.49633333  6.37833333 -5.576
 -5.57066667 -5.58016667 -5.55716667  3.21316667  6.572       2.32975
  2.13883333  6.34566667  6.13783333  8.0625      0.64583333  6.3095
 -5.56533333 -5.59133333 -3.378       2.72016667  4.73016667  2.68233333
  5.3535      7.2252      1.8705     -5.6225     -5.55966667 -5.57766667
 -5.56283333 -5.561       1.618       5.8695      6.4304     -1.489
 -1.79083333 -0.4446     -5.42116667 -0.17033333  3.83666667 -3.861
 -5.501       5.3912      1.07466667  0.656       0.47816667  4.1115
 -0.32883333 -3.78733333  2.5335      1.17033333 -2.34266667 -1.41183333
 -1.8625      1.17416667  1.56833333 -0.42433333  1.7825      1.208
 -3.93        1.153       2.345       3.15916667  3.49733333  3.28716667
  1.4155      5.01816667  5.38366667  0.586       1.37583333 -0.1885
  0.76933333  4.77766667 -5.95733333 -1.78366667 -4.22816667  1.32666667
  5.25933333  0.61016667 -6.00783333  3.1015      1.65416667  0.88433333
  3.439      -1.93133333  0.8905      5.5305     -1.13483333  5.00416667
 -2.55016667  0.42233333 -3.00033333 -2.94166667 -3.80683333 -3.53816667
  3.26083333  0.53316667  2.23433333  0.09083333 -0.31816667  1.40616667
 -2.153      -1.86016667 -2.20516667 -0.35733333  0.54383333  3.10083333
  6.24616667 -0.46066667 -4.50966667  3.43966667  1.7005      2.98
 -1.42166667 -1.43666667 -1.835      -6.518      -6.32813889 -6.13827778
 -5.94841667 -5.75855556 -5.56869444 -5.37883333 -5.18897222 -4.99911111
 -4.80925    -4.61938889 -4.42952778 -4.23966667 -4.04980556 -3.85994444
 -3.67008333 -3.48022222 -3.29036111 -3.1005     -2.91063889 -2.72077778
 -2.53091667 -2.34105556 -2.15119444 -1.96133333 -1.77147222 -1.58161111
 -1.39175    -1.20188889 -1.01202778 -0.82216667 -0.63230556 -0.44244444
 -0.25258333 -0.06272222  0.12713889  0.317       0.50686111  0.69672222
  0.88658333  1.07644444  1.26630556  1.45616667  1.64602778  1.83588889
  2.02575     2.21561111  2.40547222  2.59533333  2.78519444  2.97505556
  3.16491667  3.35477778  3.54463889  3.7345      3.92436111  4.11422222
  4.30408333  4.49394444  4.68380556  4.87366667  5.06352778  5.25338889
  5.44325     5.63311111  5.82297222  6.01283333  6.20269444  6.39255556
  6.58241667  6.77227778  6.96213889  7.152       4.45083333  1.34866667
 -1.5815     -5.526      -5.51266667 -5.54283333 -5.52316667 -5.49716667
 -5.475       3.0325      7.83416667  2.685       2.50987963  2.33475926
  2.15963889  1.98451852  1.80939815  1.63427778  1.45915741  1.28403704
  1.10891667  0.9337963   0.75867593  0.58355556  0.40843519  0.23331481
  0.05819444 -0.11692593 -0.2920463  -0.46716667 -0.64228704 -0.81740741
 -0.99252778 -1.16764815 -1.34276852 -1.51788889 -1.69300926 -1.86812963
 -2.04325    -2.21837037 -2.39349074 -2.56861111 -2.74373148 -2.91885185
 -3.09397222 -3.26909259 -3.44421296 -3.61933333  1.49        4.23583333
 -2.61866667  0.62833333 -0.03333333  2.10016667  0.94866667 -1.8535
 -2.18033333 -0.82416667 -2.25733333  3.83816667  3.65133333  5.55283333
  2.5578     -0.183       2.004       2.39788889  2.79177778  3.18566667
  3.57955556  3.97344444  4.36733333  4.76122222  5.15511111  5.549
 -3.0288     -3.28763529 -3.54647059 -3.80530588 -4.06414118 -4.32297647
 -4.58181176 -4.84064706 -5.09948235 -5.35831765 -5.61715294 -5.87598824
 -6.13482353 -6.39365882 -6.65249412 -6.91132941 -7.17016471 -7.429
  3.76066667 -0.54333333 -1.97016667  0.27433333  2.659       3.336
  1.73333333  5.72583333  3.43083333  1.99483333 -2.6355     -3.66783333
 -0.64666667 -0.102      -0.5905      3.61633333  5.27433333 -5.31966667
  6.8625      6.88883333  2.64166667  7.8295      3.589      -3.698     ]                        DOY   WD    HR
2021-11-26 15:00:00  330.0  5.0  15.0
2021-11-26 16:00:00  330.0  5.0  16.0
2021-11-26 17:00:00  330.0  5.0  17.0
2021-11-26 18:00:00  330.0  5.0  18.0
2021-11-26 19:00:00  330.0  5.0  19.0
...                    ...  ...   ...
2021-12-21 15:00:00  355.0  2.0  15.0
2021-12-21 16:00:00  355.0  2.0  16.0
2021-12-21 17:00:00  355.0  2.0  17.0
2021-12-21 18:00:00  355.0  2.0  18.0
2021-12-21 19:00:00  355.0  2.0  19.0

[605 rows x 3 columns] 2021-11-26 15:00:00    6.632800
2021-11-26 16:00:00    4.922500
2021-11-26 17:00:00    7.021167
2021-11-26 18:00:00    0.746667
2021-11-26 19:00:00    3.628167
                         ...   
2021-12-21 15:00:00    0.000000
2021-12-21 16:00:00    0.000000
2021-12-21 17:00:00    0.000000
2021-12-21 18:00:00    0.000000
2021-12-21 19:00:00    0.000000
Freq: 60T, Name: scadavalue, Length: 605, dtype: float64 [[351. 351. 351. 351. 351. 351. 351. 351. 351. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354.
  354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 355. 355. 355.
  355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355.
  355. 355. 355.]
 [  5.   5.   5.   5.   5.   5.   5.   5.   5.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.]
 [ 15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.   3.   4.
    5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.
   19.  20.  21.  22.  23.   0.   1.   2.   3.   4.   5.   6.   7.   8.
    9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.
   23.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
   13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.
    3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.
   17.  18.  19.]]
Run 0
Iteration 1, loss = 5792.76673314
Iteration 2, loss = 379.52554791
Iteration 3, loss = 131.64736625
Iteration 4, loss = 32.88908048
Iteration 5, loss = 45.07739996
Iteration 6, loss = 24.34834381
Iteration 7, loss = 13.58293092
Iteration 8, loss = 18.36844921
Iteration 9, loss = 8.25498972
Iteration 10, loss = 10.05900415
Iteration 11, loss = 8.67132767
Iteration 12, loss = 7.84328084
Iteration 13, loss = 8.68793796
Iteration 14, loss = 7.60925843
Iteration 15, loss = 7.59892825
Iteration 16, loss = 7.83806792
Iteration 17, loss = 8.30080817
Iteration 18, loss = 7.95965811
Iteration 19, loss = 7.85887919
Iteration 20, loss = 7.68779237
Iteration 21, loss = 7.57992956
Iteration 22, loss = 7.51629449
Iteration 23, loss = 7.57320955
Iteration 24, loss = 7.51969797
Iteration 25, loss = 7.54256994
Iteration 26, loss = 7.57891060
Iteration 27, loss = 7.42883482
Iteration 28, loss = 7.58421364
Iteration 29, loss = 7.71017312
Iteration 30, loss = 7.51316061
Iteration 31, loss = 7.49003199
Iteration 32, loss = 7.48758911
Iteration 33, loss = 7.43064946
Iteration 34, loss = 7.39266705
Iteration 35, loss = 7.43238067
Iteration 36, loss = 7.45395399
Iteration 37, loss = 7.37213011
Iteration 38, loss = 7.33468664
Iteration 39, loss = 7.29798514
Iteration 40, loss = 7.28597318
Iteration 41, loss = 7.23282143
Iteration 42, loss = 7.22504398
Iteration 43, loss = 7.21553125
Iteration 44, loss = 7.28162032
Iteration 45, loss = 7.32454099
Iteration 46, loss = 7.10905224
Iteration 47, loss = 7.14035489
Iteration 48, loss = 7.09494811
Iteration 49, loss = 7.08720790
Iteration 50, loss = 7.49162154
Iteration 51, loss = 7.09749438
Iteration 52, loss = 7.48593007
Iteration 53, loss = 6.99582225
Iteration 54, loss = 7.18511551
Iteration 55, loss = 7.02350237
Iteration 56, loss = 7.35938043
Iteration 57, loss = 7.53335556
Iteration 58, loss = 7.45617773
Iteration 59, loss = 7.89733376
Iteration 60, loss = 7.59158501
Iteration 61, loss = 7.88033615
Iteration 62, loss = 7.58717703
Iteration 63, loss = 7.39513138
Iteration 64, loss = 7.44841567
Iteration 65, loss = 7.27543847
Iteration 66, loss = 7.14436382
Iteration 67, loss = 7.32616399
Iteration 68, loss = 7.36887063
Iteration 69, loss = 7.31943731
Iteration 70, loss = 7.35115933
Iteration 71, loss = 7.54760948
Iteration 72, loss = 7.53156248
Iteration 73, loss = 7.55007053
Iteration 74, loss = 7.62378715
Iteration 75, loss = 7.19820098
Iteration 76, loss = 7.13786757
Iteration 77, loss = 6.97198618
Iteration 78, loss = 7.33703380
Iteration 79, loss = 6.88172326
Iteration 80, loss = 7.00761531
Iteration 81, loss = 7.07034584
Iteration 82, loss = 7.13436822
Iteration 83, loss = 7.06647519
Iteration 84, loss = 7.15118529
Iteration 85, loss = 7.06820775
Iteration 86, loss = 6.98832290
Iteration 87, loss = 7.04085442
Iteration 88, loss = 7.02912689
Iteration 89, loss = 7.02696642
Iteration 90, loss = 7.06808577
Iteration 91, loss = 7.12612610
Iteration 92, loss = 6.97073234
Iteration 93, loss = 7.04868843
Iteration 94, loss = 6.92462258
Iteration 95, loss = 6.94290307
Iteration 96, loss = 6.90776131
Iteration 97, loss = 6.97123323
Iteration 98, loss = 7.05623601
Iteration 99, loss = 6.98093672
Iteration 100, loss = 7.28174911
Iteration 101, loss = 7.09213496
Iteration 102, loss = 7.02344161
Iteration 103, loss = 7.04656126
Iteration 104, loss = 7.29949776
Iteration 105, loss = 6.94534856
Iteration 106, loss = 6.97066261
Iteration 107, loss = 7.08610511
Iteration 108, loss = 6.90541683
Iteration 109, loss = 6.94305925
Iteration 110, loss = 6.95132509
Iteration 111, loss = 7.12789795
Iteration 112, loss = 6.98987591
Iteration 113, loss = 7.03146604
Iteration 114, loss = 7.12617191
Iteration 115, loss = 7.15136129
Iteration 116, loss = 7.40585691
Iteration 117, loss = 7.14233004
Iteration 118, loss = 6.87761328
Iteration 119, loss = 6.96532480
Iteration 120, loss = 6.87865353
Iteration 121, loss = 6.97769225
Iteration 122, loss = 6.86321685
Iteration 123, loss = 7.01969623
Iteration 124, loss = 6.92212356
Iteration 125, loss = 7.17089511
Iteration 126, loss = 6.88252213
Iteration 127, loss = 6.88376309
Iteration 128, loss = 7.10057445
Iteration 129, loss = 6.95301545
Iteration 130, loss = 6.90204589
Iteration 131, loss = 6.90272086
Iteration 132, loss = 6.91795815
Iteration 133, loss = 6.94065062
Iteration 134, loss = 6.99430451
Iteration 135, loss = 6.93195072
Iteration 136, loss = 7.05014007
Iteration 137, loss = 7.01974797
Iteration 138, loss = 6.87179976
Iteration 139, loss = 6.91355916
Iteration 140, loss = 6.97444484
Iteration 141, loss = 6.94014856
Iteration 142, loss = 7.40307634
Iteration 143, loss = 7.88470269
Iteration 144, loss = 7.84849816
Iteration 145, loss = 7.65405507
Iteration 146, loss = 7.53664377
Iteration 147, loss = 7.12280080
Iteration 148, loss = 6.90206686
Iteration 149, loss = 7.09641274
Iteration 150, loss = 6.91055585
Iteration 151, loss = 6.98521625
Iteration 152, loss = 7.06296607
Iteration 153, loss = 6.92733279
Iteration 154, loss = 6.94551140
Iteration 155, loss = 6.95525660
Iteration 156, loss = 7.10985787
Iteration 157, loss = 6.96849631
Iteration 158, loss = 6.88423460
Iteration 159, loss = 6.94700885
Iteration 160, loss = 6.90605185
Iteration 161, loss = 6.84935398
Iteration 162, loss = 6.83052505
Iteration 163, loss = 6.90738978
Iteration 164, loss = 6.90677142
Iteration 165, loss = 6.94003562
Iteration 166, loss = 7.00890837
Iteration 167, loss = 7.22945383
Iteration 168, loss = 7.15679532
Iteration 169, loss = 7.42404981
Iteration 170, loss = 7.40682734
Iteration 171, loss = 7.45042832
Iteration 172, loss = 7.49398927
Iteration 173, loss = 7.24206263
Iteration 174, loss = 6.91901128
Iteration 175, loss = 6.83538535
Iteration 176, loss = 6.89035510
Iteration 177, loss = 6.87797443
Iteration 178, loss = 6.99224104
Iteration 179, loss = 7.17700246
Iteration 180, loss = 7.02324763
Iteration 181, loss = 7.54619355
Iteration 182, loss = 7.68051465
Iteration 183, loss = 7.88090287
Iteration 184, loss = 7.30966476
Iteration 185, loss = 7.08991769
Iteration 186, loss = 7.39066154
Iteration 187, loss = 7.17056424
Iteration 188, loss = 6.84145447
Iteration 189, loss = 6.85012774
Iteration 190, loss = 6.86350878
Iteration 191, loss = 6.83000738
Iteration 192, loss = 6.81892880
Iteration 193, loss = 6.84223607
Iteration 194, loss = 6.99799252
Iteration 195, loss = 6.88918140
Iteration 196, loss = 7.13531101
Iteration 197, loss = 6.94229591
Iteration 198, loss = 6.91667040
Iteration 199, loss = 7.02170259
Iteration 200, loss = 6.94694740
Iteration 201, loss = 7.32242124
Iteration 202, loss = 7.27247715
Iteration 203, loss = 7.68871182
Iteration 204, loss = 7.65258358
Iteration 205, loss = 7.08455229
Iteration 206, loss = 7.13608815
Iteration 207, loss = 7.06747714
Iteration 208, loss = 7.11933408
Iteration 209, loss = 7.01120699
Iteration 210, loss = 6.90081037
Iteration 211, loss = 6.76769591
Iteration 212, loss = 6.77509473
Iteration 213, loss = 6.87703508
Iteration 214, loss = 6.93541818
Iteration 215, loss = 7.31941607
Iteration 216, loss = 7.02518573
Iteration 217, loss = 7.01097637
Iteration 218, loss = 6.85494372
Iteration 219, loss = 6.76318885
Iteration 220, loss = 6.81572280
Iteration 221, loss = 6.78524352
Iteration 222, loss = 6.76550683
Iteration 223, loss = 6.77332146
Iteration 224, loss = 6.80605629
Iteration 225, loss = 6.81624428
Iteration 226, loss = 6.97517099
Iteration 227, loss = 6.73277505
Iteration 228, loss = 6.86974456
Iteration 229, loss = 6.91876329
Iteration 230, loss = 6.85280638
Iteration 231, loss = 7.01947971
Iteration 232, loss = 6.81130968
Iteration 233, loss = 6.77489185
Iteration 234, loss = 6.70338052
Iteration 235, loss = 6.79795548
Iteration 236, loss = 6.88151369
Iteration 237, loss = 6.74335994
Iteration 238, loss = 7.05531326
Iteration 239, loss = 6.73471503
Iteration 240, loss = 6.81434325
Iteration 241, loss = 6.72666541
Iteration 242, loss = 6.81552912
Iteration 243, loss = 6.88678904
Iteration 244, loss = 6.87875730
Iteration 245, loss = 6.84069587
Iteration 246, loss = 6.91093251
Iteration 247, loss = 6.83168181
Iteration 248, loss = 6.67879304
Iteration 249, loss = 6.87585211
Iteration 250, loss = 6.92887114
Iteration 251, loss = 7.14558572
Iteration 252, loss = 6.91198824
Iteration 253, loss = 6.98275363
Iteration 254, loss = 6.89333463
Iteration 255, loss = 6.69605455
Iteration 256, loss = 6.75938528
Iteration 257, loss = 6.84906009
Iteration 258, loss = 7.05469882
Iteration 259, loss = 6.71930374
Iteration 260, loss = 6.86606434
Iteration 261, loss = 6.85747648
Iteration 262, loss = 6.79914073
Iteration 263, loss = 7.08918405
Iteration 264, loss = 6.89352401
Iteration 265, loss = 6.78163914
Iteration 266, loss = 6.90763201
Iteration 267, loss = 6.79290247
Iteration 268, loss = 6.69919938
Iteration 269, loss = 6.68661400
Iteration 270, loss = 6.73060456
Iteration 271, loss = 6.79407862
Iteration 272, loss = 6.68525301
Iteration 273, loss = 6.70327133
Iteration 274, loss = 6.71056564
Iteration 275, loss = 6.84126368
Iteration 276, loss = 6.76301439
Iteration 277, loss = 6.92511882
Iteration 278, loss = 6.74096849
Iteration 279, loss = 6.68419533
Iteration 280, loss = 6.68298737
Iteration 281, loss = 6.73233233
Iteration 282, loss = 6.93626624
Iteration 283, loss = 6.86888217
Iteration 284, loss = 6.89765495
Iteration 285, loss = 6.67596220
Iteration 286, loss = 6.66692686
Iteration 287, loss = 6.65480363
Iteration 288, loss = 6.71440131
Iteration 289, loss = 6.64912564
Iteration 290, loss = 6.66319165
Iteration 291, loss = 6.66173719
Iteration 292, loss = 6.67105750
Iteration 293, loss = 6.66888400
Iteration 294, loss = 6.73545128
Iteration 295, loss = 6.73635315
Iteration 296, loss = 6.64300765
Iteration 297, loss = 6.64032518
Iteration 298, loss = 6.72015274
Iteration 299, loss = 6.84076811
Iteration 300, loss = 7.17920874
Iteration 301, loss = 6.76179476
Iteration 302, loss = 6.88291295
Iteration 303, loss = 6.88657902
Iteration 304, loss = 6.87198693
Iteration 305, loss = 7.04725662
Iteration 306, loss = 6.95711049
Iteration 307, loss = 6.76805033
Iteration 308, loss = 6.66975020
Iteration 309, loss = 6.75469125
Iteration 310, loss = 7.09047369
Iteration 311, loss = 6.93682660
Iteration 312, loss = 6.67970370
Iteration 313, loss = 6.64729020
Iteration 314, loss = 6.72336085
Iteration 315, loss = 6.71870623
Iteration 316, loss = 6.73749225
Iteration 317, loss = 7.13477875
Iteration 318, loss = 6.69304563
Iteration 319, loss = 6.61718192
Iteration 320, loss = 6.67412353
Iteration 321, loss = 6.74521792
Iteration 322, loss = 6.73175702
Iteration 323, loss = 6.78770267
Iteration 324, loss = 6.64872673
Iteration 325, loss = 6.61428434
Iteration 326, loss = 6.67453167
Iteration 327, loss = 6.68870621
Iteration 328, loss = 6.70107268
Iteration 329, loss = 6.61307692
Iteration 330, loss = 6.61178864
Iteration 331, loss = 6.69625459
Iteration 332, loss = 6.65480795
Iteration 333, loss = 6.60128575
Iteration 334, loss = 6.65094834
Iteration 335, loss = 6.58304857
Iteration 336, loss = 6.84567162
Iteration 337, loss = 6.96378518
Iteration 338, loss = 6.62350645
Iteration 339, loss = 6.68070784
Iteration 340, loss = 6.76045161
Iteration 341, loss = 7.05114183
Iteration 342, loss = 6.62287522
Iteration 343, loss = 6.94144631
Iteration 344, loss = 6.83812006
Iteration 345, loss = 6.77935893
Iteration 346, loss = 7.04584907
Iteration 347, loss = 6.88288378
Iteration 348, loss = 6.67049251
Iteration 349, loss = 6.73235641
Iteration 350, loss = 6.66741571
Iteration 351, loss = 6.64195725
Iteration 352, loss = 6.59334768
Iteration 353, loss = 6.66026789
Iteration 354, loss = 6.90894136
Iteration 355, loss = 6.91194630
Iteration 356, loss = 6.71771951
Iteration 357, loss = 6.77983043
Iteration 358, loss = 6.74717204
Iteration 359, loss = 6.71671595
Iteration 360, loss = 6.57803953
Iteration 361, loss = 6.58826744
Iteration 362, loss = 6.61352883
Iteration 363, loss = 6.65831623
Iteration 364, loss = 6.72551359
Iteration 365, loss = 6.62265488
Iteration 366, loss = 6.57138980
Iteration 367, loss = 6.64575119
Iteration 368, loss = 6.61413540
Iteration 369, loss = 6.68265345
Iteration 370, loss = 6.58906249
Iteration 371, loss = 6.72882907
Iteration 372, loss = 6.71924177
Iteration 373, loss = 6.61355798
Iteration 374, loss = 6.64988810
Iteration 375, loss = 6.53503506
Iteration 376, loss = 6.70530678
Iteration 377, loss = 6.68435202
Iteration 378, loss = 6.54762912
Iteration 379, loss = 6.64359280
Iteration 380, loss = 6.67503020
Iteration 381, loss = 6.98511925
Iteration 382, loss = 6.90996927
Iteration 383, loss = 7.17649434
Iteration 384, loss = 6.62826759
Iteration 385, loss = 6.59129048
Iteration 386, loss = 6.59665465
Iteration 387, loss = 6.63118274
Iteration 388, loss = 6.60719164
Iteration 389, loss = 6.81020233
Iteration 390, loss = 6.85318826
Iteration 391, loss = 7.02497716
Iteration 392, loss = 6.77637086
Iteration 393, loss = 7.19973353
Iteration 394, loss = 7.62872791
Iteration 395, loss = 7.16093291
Iteration 396, loss = 6.70173550
Iteration 397, loss = 6.60156897
Iteration 398, loss = 6.54112712
Iteration 399, loss = 6.68724942
Iteration 400, loss = 7.18779553
Iteration 401, loss = 6.77718060
Iteration 402, loss = 6.66152299
Iteration 403, loss = 6.66555075
Iteration 404, loss = 6.57444341
Iteration 405, loss = 6.60544595
Iteration 406, loss = 6.54057073
Iteration 407, loss = 6.52629386
Iteration 408, loss = 6.60390022
Iteration 409, loss = 6.79102855
Iteration 410, loss = 6.68267790
Iteration 411, loss = 6.95374609
Iteration 412, loss = 6.64102657
Iteration 413, loss = 6.53231384
Iteration 414, loss = 6.53382762
Iteration 415, loss = 6.59088049
Iteration 416, loss = 6.52702861
Iteration 417, loss = 6.64571856
Iteration 418, loss = 6.63163498
Iteration 419, loss = 6.52825294
Iteration 420, loss = 6.55580663
Iteration 421, loss = 6.64179883
Iteration 422, loss = 6.64863029
Iteration 423, loss = 6.52699208
Iteration 424, loss = 6.59196750
Iteration 425, loss = 6.61967884
Iteration 426, loss = 6.82044682
Iteration 427, loss = 6.60442823
Iteration 428, loss = 6.60078646
Iteration 429, loss = 6.54730798
Iteration 430, loss = 6.49681955
Iteration 431, loss = 6.57337373
Iteration 432, loss = 7.14155760
Iteration 433, loss = 7.13022717
Iteration 434, loss = 7.01313825
Iteration 435, loss = 7.05352914
Iteration 436, loss = 6.66185435
Iteration 437, loss = 6.78933963
Iteration 438, loss = 6.61106278
Iteration 439, loss = 6.58756182
Iteration 440, loss = 6.64352516
Iteration 441, loss = 6.87785084
Iteration 442, loss = 6.61358220
Iteration 443, loss = 6.49791845
Iteration 444, loss = 6.51285386
Iteration 445, loss = 6.49966811
Iteration 446, loss = 6.51036926
Iteration 447, loss = 6.52413787
Iteration 448, loss = 6.52012562
Iteration 449, loss = 6.50878497
Iteration 450, loss = 6.49967829
Iteration 451, loss = 6.49740387
Iteration 452, loss = 6.53074858
Iteration 453, loss = 6.49772779
Iteration 454, loss = 6.50096413
Iteration 455, loss = 6.51338695
Iteration 456, loss = 6.59733939
Iteration 457, loss = 6.61566034
Iteration 458, loss = 6.53780499
Iteration 459, loss = 6.47265538
Iteration 460, loss = 6.48651554
Iteration 461, loss = 6.65793545
Iteration 462, loss = 6.52815011
Iteration 463, loss = 6.53932909
Iteration 464, loss = 6.61118316
Iteration 465, loss = 6.56744070
Iteration 466, loss = 6.53562561
Iteration 467, loss = 6.48827298
Iteration 468, loss = 6.50446753
Iteration 469, loss = 6.51449348
Iteration 470, loss = 6.50023162
Iteration 471, loss = 6.49677426
Iteration 472, loss = 6.44146657
Iteration 473, loss = 6.61873737
Iteration 474, loss = 6.74682624
Iteration 475, loss = 6.74896348
Iteration 476, loss = 6.49421856
Iteration 477, loss = 6.61672908
Iteration 478, loss = 6.65542070
Iteration 479, loss = 6.56589832
Iteration 480, loss = 6.50152747
Iteration 481, loss = 6.61502796
Iteration 482, loss = 6.50965234
Iteration 483, loss = 6.45374568
Iteration 484, loss = 6.58774734
Iteration 485, loss = 6.71994396
Iteration 486, loss = 6.53985329
Iteration 487, loss = 6.50140397
Iteration 488, loss = 6.44673415
Iteration 489, loss = 6.48528031
Iteration 490, loss = 6.49925314
Iteration 491, loss = 6.45438346
Iteration 492, loss = 6.49557542
Iteration 493, loss = 6.52859730
Iteration 494, loss = 6.60546915
Iteration 495, loss = 6.55298168
Iteration 496, loss = 6.48082432
Iteration 497, loss = 6.55026945
Iteration 498, loss = 6.49889642
Iteration 499, loss = 6.47712654
Iteration 500, loss = 6.58104117
Iteration 501, loss = 6.54058819
Iteration 502, loss = 6.46673929
Iteration 503, loss = 6.50700274
Iteration 504, loss = 6.50738119
Iteration 505, loss = 6.51885093
Iteration 506, loss = 6.47801729
Iteration 507, loss = 6.50444681
Iteration 508, loss = 6.43443657
Iteration 509, loss = 6.46245314
Iteration 510, loss = 6.61091386
Iteration 511, loss = 7.00046469
Iteration 512, loss = 7.16160398
Iteration 513, loss = 7.03193458
Iteration 514, loss = 6.68390381
Iteration 515, loss = 6.53073434
Iteration 516, loss = 6.62131574
Iteration 517, loss = 6.50747104
Iteration 518, loss = 6.67265682
Iteration 519, loss = 6.53014134
Iteration 520, loss = 6.58118257
Iteration 521, loss = 6.49873352
Iteration 522, loss = 6.45816501
Iteration 523, loss = 6.47229665
Iteration 524, loss = 6.44069849
Iteration 525, loss = 6.44427878
Iteration 526, loss = 6.59991287
Iteration 527, loss = 6.76343976
Iteration 528, loss = 6.62852505
Iteration 529, loss = 6.53689180
Iteration 530, loss = 6.58941542
Iteration 531, loss = 6.51441763
Iteration 532, loss = 6.55588256
Iteration 533, loss = 6.51988783
Iteration 534, loss = 6.61964017
Iteration 535, loss = 6.46185813
Iteration 536, loss = 6.57496790
Iteration 537, loss = 6.62058939
Iteration 538, loss = 6.48837742
Iteration 539, loss = 6.51546898
Iteration 540, loss = 6.57319774
Iteration 541, loss = 6.53715758
Iteration 542, loss = 6.45731857
Iteration 543, loss = 6.56307265
Iteration 544, loss = 6.73725749
Iteration 545, loss = 6.75542254
Iteration 546, loss = 6.46477971
Iteration 547, loss = 6.67309174
Iteration 548, loss = 6.47440577
Iteration 549, loss = 6.68241071
Iteration 550, loss = 6.54771030
Iteration 551, loss = 6.46774433
Iteration 552, loss = 6.60659391
Iteration 553, loss = 6.78793169
Iteration 554, loss = 6.88399018
Iteration 555, loss = 6.86244721
Iteration 556, loss = 6.60968361
Iteration 557, loss = 6.53026628
Iteration 558, loss = 6.56499855
Iteration 559, loss = 6.58152677
Iteration 560, loss = 6.46757369
Iteration 561, loss = 6.44151525
Iteration 562, loss = 6.49914699
Iteration 563, loss = 6.55104312
Iteration 564, loss = 6.43387462
Iteration 565, loss = 6.46136796
Iteration 566, loss = 6.49038239
Iteration 567, loss = 6.43108067
Iteration 568, loss = 6.50418029
Iteration 569, loss = 6.52185850
Iteration 570, loss = 6.45113883
Iteration 571, loss = 6.44788627
Iteration 572, loss = 6.46486715
Iteration 573, loss = 6.45070101
Iteration 574, loss = 6.44892254
Iteration 575, loss = 6.43484057
Iteration 576, loss = 6.49577860
Iteration 577, loss = 6.38542451
Iteration 578, loss = 6.41916141
Iteration 579, loss = 6.49326344
Iteration 580, loss = 6.47076589
Iteration 581, loss = 6.47286877
Iteration 582, loss = 6.40522910
Iteration 583, loss = 6.57060647
Iteration 584, loss = 6.64226982
Iteration 585, loss = 6.50867486
Iteration 586, loss = 6.48155335
Iteration 587, loss = 6.46475861
Iteration 588, loss = 6.55003665
Iteration 589, loss = 6.62756949
Iteration 590, loss = 6.64937184
Iteration 591, loss = 6.53824333
Iteration 592, loss = 6.61120135
Iteration 593, loss = 6.47284835
Iteration 594, loss = 6.64890609
Iteration 595, loss = 6.62521393
Iteration 596, loss = 6.52634607
Iteration 597, loss = 6.50359685
Iteration 598, loss = 6.62227132
Iteration 599, loss = 6.68344324
Iteration 600, loss = 6.74781233
Iteration 601, loss = 6.59010698
Iteration 602, loss = 6.56818504
Iteration 603, loss = 6.79413165
Iteration 604, loss = 6.64447067
Iteration 605, loss = 6.61214983
Iteration 606, loss = 6.77311150
Iteration 607, loss = 6.46412668
Iteration 608, loss = 6.54083201
Iteration 609, loss = 6.45633560
Iteration 610, loss = 6.45057340
Iteration 611, loss = 6.47533033
Iteration 612, loss = 6.54527007
Iteration 613, loss = 6.60705541
Iteration 614, loss = 6.78146451
Iteration 615, loss = 6.91528459
Iteration 616, loss = 6.51173580
Iteration 617, loss = 6.64802104
Iteration 618, loss = 6.52603261
Iteration 619, loss = 7.05419474
Iteration 620, loss = 7.02732512
Iteration 621, loss = 6.75301838
Iteration 622, loss = 6.70687492
Iteration 623, loss = 6.70096236
Iteration 624, loss = 6.49681086
Iteration 625, loss = 6.45890422
Iteration 626, loss = 6.44309813
Iteration 627, loss = 6.43098894
Iteration 628, loss = 6.41205679
Iteration 629, loss = 6.44436179
Iteration 630, loss = 6.42222922
Iteration 631, loss = 6.50419726
Iteration 632, loss = 6.37439730
Iteration 633, loss = 6.36690701
Iteration 634, loss = 6.41054229
Iteration 635, loss = 6.46672229
Iteration 636, loss = 6.38387760
Iteration 637, loss = 6.38300311
Iteration 638, loss = 6.41580774
Iteration 639, loss = 6.42236263
Iteration 640, loss = 6.46896978
Iteration 641, loss = 6.43006460
Iteration 642, loss = 6.36999407
Iteration 643, loss = 6.65490714
Iteration 644, loss = 6.45267143
Iteration 645, loss = 6.42221471
Iteration 646, loss = 6.46005959
Iteration 647, loss = 6.51464399
Iteration 648, loss = 6.52329027
Iteration 649, loss = 6.38335800
Iteration 650, loss = 6.78162697
Iteration 651, loss = 6.92885196
Iteration 652, loss = 6.76875425
Iteration 653, loss = 6.75117247
Iteration 654, loss = 6.67415388
Iteration 655, loss = 6.70680928
Iteration 656, loss = 6.58473907
Iteration 657, loss = 6.42173234
Iteration 658, loss = 6.45624496
Iteration 659, loss = 6.38798313
Iteration 660, loss = 6.46212227
Iteration 661, loss = 6.42924452
Iteration 662, loss = 6.34950093
Iteration 663, loss = 6.44844290
Iteration 664, loss = 6.62908599
Iteration 665, loss = 6.69810148
Iteration 666, loss = 6.68188759
Iteration 667, loss = 6.46154914
Iteration 668, loss = 6.69963828
Iteration 669, loss = 6.47079377
Iteration 670, loss = 6.40249373
Iteration 671, loss = 6.39672750
Iteration 672, loss = 6.39225738
Iteration 673, loss = 6.41946197
Iteration 674, loss = 6.40386016
Iteration 675, loss = 6.42865204
Iteration 676, loss = 6.38897481
Iteration 677, loss = 6.48950594
Iteration 678, loss = 6.56862330
Iteration 679, loss = 6.36396945
Iteration 680, loss = 6.57726500
Iteration 681, loss = 7.28346879
Iteration 682, loss = 6.68521874
Iteration 683, loss = 6.59360921
Iteration 684, loss = 6.58628076
Iteration 685, loss = 6.73056929
Iteration 686, loss = 7.20695908
Iteration 687, loss = 6.69537949
Iteration 688, loss = 6.79101688
Iteration 689, loss = 6.72128400
Iteration 690, loss = 6.75548390
Iteration 691, loss = 6.86974685
Iteration 692, loss = 6.74762947
Iteration 693, loss = 6.60355850
Iteration 694, loss = 7.25973609
Iteration 695, loss = 6.59977700
Iteration 696, loss = 6.67326522
Iteration 697, loss = 6.92541284
Iteration 698, loss = 6.51019619
Iteration 699, loss = 6.55978128
Iteration 700, loss = 6.64460879
Iteration 701, loss = 6.53983197
Iteration 702, loss = 6.60292589
Iteration 703, loss = 6.48789020
Iteration 704, loss = 6.41715805
Iteration 705, loss = 6.48861155
Iteration 706, loss = 6.46560287
Iteration 707, loss = 6.42404009
Iteration 708, loss = 6.45013225
Iteration 709, loss = 6.39926089
Iteration 710, loss = 6.48840602
Iteration 711, loss = 6.40836940
Iteration 712, loss = 6.38657939
Iteration 713, loss = 6.43458397
Iteration 714, loss = 6.37020431
Iteration 715, loss = 6.36387602
Iteration 716, loss = 6.48640369
Iteration 717, loss = 6.41461428
Iteration 718, loss = 6.36557414
Iteration 719, loss = 6.40504659
Iteration 720, loss = 6.45299010
Iteration 721, loss = 6.35872628
Iteration 722, loss = 6.42479774
Iteration 723, loss = 6.42428787
Iteration 724, loss = 6.61866556
Iteration 725, loss = 6.55127649
Iteration 726, loss = 6.56893920
Iteration 727, loss = 6.67243839
Iteration 728, loss = 6.57730642
Iteration 729, loss = 6.52817926
Iteration 730, loss = 6.47691401
Iteration 731, loss = 6.47076757
Iteration 732, loss = 6.36475255
Iteration 733, loss = 6.43752316
Iteration 734, loss = 6.52875464
Iteration 735, loss = 6.55535651
Iteration 736, loss = 6.57228406
Iteration 737, loss = 6.63157040
Iteration 738, loss = 6.58635962
Iteration 739, loss = 6.54974963
Iteration 740, loss = 6.58078955
Iteration 741, loss = 6.48239057
Iteration 742, loss = 6.45385169
Iteration 743, loss = 6.50215769
Iteration 744, loss = 6.55908185
Iteration 745, loss = 6.44163878
Iteration 746, loss = 6.43983361
Iteration 747, loss = 6.56184797
Iteration 748, loss = 6.35536531
Iteration 749, loss = 6.34546415
Iteration 750, loss = 6.38423095
Iteration 751, loss = 6.38433140
Iteration 752, loss = 6.39109998
Iteration 753, loss = 6.35834251
Iteration 754, loss = 6.32954422
Iteration 755, loss = 6.40531010
Iteration 756, loss = 6.37896556
Iteration 757, loss = 6.67795542
Iteration 758, loss = 6.58417352
Iteration 759, loss = 6.41980739
Iteration 760, loss = 6.46930567
Iteration 761, loss = 6.41075630
Iteration 762, loss = 6.40964245
Iteration 763, loss = 6.40117625
Iteration 764, loss = 6.37599608
Iteration 765, loss = 6.54980008
Iteration 766, loss = 6.62630542
Iteration 767, loss = 6.63599715
Iteration 768, loss = 6.70653832
Iteration 769, loss = 6.69812291
Iteration 770, loss = 6.95857050
Iteration 771, loss = 6.46552366
Iteration 772, loss = 6.47105006
Iteration 773, loss = 6.56229712
Iteration 774, loss = 6.48639069
Iteration 775, loss = 6.46467307
Iteration 776, loss = 6.38604267
Iteration 777, loss = 6.35547793
Iteration 778, loss = 6.39408833
Iteration 779, loss = 6.44031938
Iteration 780, loss = 6.40784304
Iteration 781, loss = 6.39607128
Iteration 782, loss = 6.34307470
Iteration 783, loss = 6.38733374
Iteration 784, loss = 6.48762049
Iteration 785, loss = 6.53419628
Iteration 786, loss = 6.65136740
Iteration 787, loss = 6.59210584
Iteration 788, loss = 6.58332926
Iteration 789, loss = 6.36426140
Iteration 790, loss = 6.64628262
Iteration 791, loss = 6.40706598
Iteration 792, loss = 6.40783946
Iteration 793, loss = 6.45201381
Iteration 794, loss = 6.32988684
Iteration 795, loss = 6.38645429
Iteration 796, loss = 6.37979060
Iteration 797, loss = 6.45926397
Iteration 798, loss = 6.37941341
Iteration 799, loss = 6.43743599
Iteration 800, loss = 6.48360230
Iteration 801, loss = 6.34769135
Iteration 802, loss = 6.48160899
Iteration 803, loss = 6.45189389
Iteration 804, loss = 6.42725087
Iteration 805, loss = 6.33899353
Iteration 806, loss = 6.45852823
Iteration 807, loss = 6.40544160
Iteration 808, loss = 6.39859851
Iteration 809, loss = 6.44646985
Iteration 810, loss = 6.31651133
Iteration 811, loss = 6.31872494
Iteration 812, loss = 6.43226656
Iteration 813, loss = 6.43902227
Iteration 814, loss = 6.34261900
Iteration 815, loss = 6.37041743
Iteration 816, loss = 6.38312497
Iteration 817, loss = 6.40146014
Iteration 818, loss = 6.40600227
Iteration 819, loss = 6.39962429
Iteration 820, loss = 6.37601127
Iteration 821, loss = 6.40838944
Iteration 822, loss = 6.52334199
Iteration 823, loss = 6.33418778
Iteration 824, loss = 6.41642245
Iteration 825, loss = 6.32620050
Iteration 826, loss = 6.35415061
Iteration 827, loss = 6.36947305
Iteration 828, loss = 6.38464943
Iteration 829, loss = 6.34100660
Iteration 830, loss = 6.34168224
Iteration 831, loss = 6.64704781
Iteration 832, loss = 6.46157101
Iteration 833, loss = 6.33702192
Iteration 834, loss = 6.37866349
Iteration 835, loss = 6.39446269
Iteration 836, loss = 6.40152404
Iteration 837, loss = 6.30522169
Iteration 838, loss = 6.38561759
Iteration 839, loss = 6.44854441
Iteration 840, loss = 6.58534936
Iteration 841, loss = 6.42114688
Iteration 842, loss = 6.46415175
Iteration 843, loss = 6.48791724
Iteration 844, loss = 6.41483035
Iteration 845, loss = 6.37114172
Iteration 846, loss = 6.38462850
Iteration 847, loss = 6.32340060
Iteration 848, loss = 6.36551180
Iteration 849, loss = 6.30304331
Iteration 850, loss = 6.32876887
Iteration 851, loss = 6.45119476
Iteration 852, loss = 6.73996628
Iteration 853, loss = 6.58331366
Iteration 854, loss = 6.43690762
Iteration 855, loss = 6.57455021
Iteration 856, loss = 6.47154377
Iteration 857, loss = 6.54059393
Iteration 858, loss = 6.76080532
Iteration 859, loss = 6.52877480
Iteration 860, loss = 6.70598871
Iteration 861, loss = 6.70152437
Iteration 862, loss = 6.90774254
Iteration 863, loss = 6.81987014
Iteration 864, loss = 6.53564922
Iteration 865, loss = 6.57494979
Iteration 866, loss = 6.66163059
Iteration 867, loss = 6.53623523
Iteration 868, loss = 6.46897622
Iteration 869, loss = 6.50125683
Iteration 870, loss = 6.35850644
Iteration 871, loss = 6.45908479
Iteration 872, loss = 6.42800716
Iteration 873, loss = 6.35838365
Iteration 874, loss = 6.35867688
Iteration 875, loss = 6.33067236
Iteration 876, loss = 6.34370298
Iteration 877, loss = 6.32061388
Iteration 878, loss = 6.33145969
Iteration 879, loss = 6.30184867
Iteration 880, loss = 6.30825192
Iteration 881, loss = 6.37156968
Iteration 882, loss = 6.36578223
Iteration 883, loss = 6.53895574
Iteration 884, loss = 6.60058961
Iteration 885, loss = 6.36018215
Iteration 886, loss = 6.45258386
Iteration 887, loss = 6.58065978
Iteration 888, loss = 6.55141558
Iteration 889, loss = 6.46552378
Iteration 890, loss = 6.34209386
Iteration 891, loss = 6.33764927
Iteration 892, loss = 6.37628499
Iteration 893, loss = 6.43914879
Iteration 894, loss = 6.38437593
Iteration 895, loss = 6.29146632
Iteration 896, loss = 6.36931655
Iteration 897, loss = 6.32302981
Iteration 898, loss = 6.32541604
Iteration 899, loss = 6.43297441
Iteration 900, loss = 6.52325208
Iteration 901, loss = 6.33279233
Iteration 902, loss = 6.37875675
Iteration 903, loss = 6.33039767
Iteration 904, loss = 6.29416636
Iteration 905, loss = 6.33712678
Iteration 906, loss = 6.33693889
Iteration 907, loss = 6.33373749
Iteration 908, loss = 6.42534549
Iteration 909, loss = 6.28149213
Iteration 910, loss = 6.30216725
Iteration 911, loss = 6.45068308
Iteration 912, loss = 6.31430706
Iteration 913, loss = 6.38750507
Iteration 914, loss = 6.36261699
Iteration 915, loss = 6.32194037
Iteration 916, loss = 6.28766110
Iteration 917, loss = 6.29832782
Iteration 918, loss = 6.31250281
Iteration 919, loss = 6.30407360
Iteration 920, loss = 6.28521554
Iteration 921, loss = 6.30822157
Iteration 922, loss = 6.36228900
Iteration 923, loss = 6.30522221
Iteration 924, loss = 6.31136062
Iteration 925, loss = 6.27261354
Iteration 926, loss = 6.27808883
Iteration 927, loss = 6.28741026
Iteration 928, loss = 6.35460854
Iteration 929, loss = 6.27657553
Iteration 930, loss = 6.29208972
Iteration 931, loss = 6.34382681
Iteration 932, loss = 6.35304693
Iteration 933, loss = 6.53495431
Iteration 934, loss = 6.42775876
Iteration 935, loss = 6.29651427
Iteration 936, loss = 6.46810193
Iteration 937, loss = 6.79156679
Iteration 938, loss = 6.46230686
Iteration 939, loss = 6.39670872
Iteration 940, loss = 6.51292904
Iteration 941, loss = 6.30580455
Iteration 942, loss = 6.43380996
Iteration 943, loss = 6.39681950
Iteration 944, loss = 6.41753520
Iteration 945, loss = 6.28895502
Iteration 946, loss = 6.45896753
Iteration 947, loss = 6.34496128
Iteration 948, loss = 6.37980054
Iteration 949, loss = 6.35032943
Iteration 950, loss = 6.35430296
Iteration 951, loss = 6.57460462
Iteration 952, loss = 6.32573678
Iteration 953, loss = 6.38127851
Iteration 954, loss = 6.52348622
Iteration 955, loss = 6.38824535
Iteration 956, loss = 6.28854520
Iteration 957, loss = 6.34361279
Iteration 958, loss = 6.34479737
Iteration 959, loss = 6.26630432
Iteration 960, loss = 6.25609857
Iteration 961, loss = 6.29874832
Iteration 962, loss = 6.40194258
Iteration 963, loss = 6.43485483
Iteration 964, loss = 6.44884595
Iteration 965, loss = 6.32487864
Iteration 966, loss = 6.43072173
Iteration 967, loss = 6.35493401
Iteration 968, loss = 6.36458027
Iteration 969, loss = 6.30298720
Iteration 970, loss = 6.30703112
Iteration 971, loss = 6.33316688
Iteration 972, loss = 6.46786151
Iteration 973, loss = 6.33372696
Iteration 974, loss = 6.27660902
Iteration 975, loss = 6.26478957
Iteration 976, loss = 6.39555157
Iteration 977, loss = 6.39542050
Iteration 978, loss = 6.39674583
Iteration 979, loss = 6.29914874
Iteration 980, loss = 6.34600193
Iteration 981, loss = 6.51779955
Iteration 982, loss = 6.66528487
Iteration 983, loss = 6.58720704
Iteration 984, loss = 6.42017653
Iteration 985, loss = 6.44565960
Iteration 986, loss = 6.51372694
Iteration 987, loss = 6.44302585
Iteration 988, loss = 6.38337841
Iteration 989, loss = 6.49284164
Iteration 990, loss = 6.29149877
Iteration 991, loss = 6.35299567
Iteration 992, loss = 6.45117002
Iteration 993, loss = 6.30356581
Iteration 994, loss = 6.46207304
Iteration 995, loss = 6.83004088
Iteration 996, loss = 6.96901835
Iteration 997, loss = 6.30194133
Iteration 998, loss = 6.97165501
Iteration 999, loss = 6.77993527
Iteration 1000, loss = 6.85628820
Run 1
Iteration 1, loss = 975.54869082
Iteration 2, loss = 503.75579331
Iteration 3, loss = 27.10940074
Iteration 4, loss = 61.05800914
Iteration 5, loss = 31.29201878
Iteration 6, loss = 8.59695672
Iteration 7, loss = 13.33492082
Iteration 8, loss = 7.82889863
Iteration 9, loss = 8.87331103
Iteration 10, loss = 7.65155968
Iteration 11, loss = 7.45583541
Iteration 12, loss = 7.74786782
Iteration 13, loss = 7.78214425
Iteration 14, loss = 8.08595654
Iteration 15, loss = 7.52704748
Iteration 16, loss = 7.45353009
Iteration 17, loss = 7.54785389
Iteration 18, loss = 7.65947481
Iteration 19, loss = 7.50535666
Iteration 20, loss = 7.67047826
Iteration 21, loss = 7.54966488
Iteration 22, loss = 7.48939714
Iteration 23, loss = 7.39340866
Iteration 24, loss = 7.40244918
Iteration 25, loss = 7.35878480
Iteration 26, loss = 7.41208767
Iteration 27, loss = 7.49577314
Iteration 28, loss = 7.37601341
Iteration 29, loss = 7.33127601
Iteration 30, loss = 7.31935029
Iteration 31, loss = 7.30304208
Iteration 32, loss = 7.22565813
Iteration 33, loss = 7.28977703
Iteration 34, loss = 7.36658672
Iteration 35, loss = 7.17558448
Iteration 36, loss = 7.22074446
Iteration 37, loss = 7.11286589
Iteration 38, loss = 7.20380510
Iteration 39, loss = 7.25112394
Iteration 40, loss = 7.16636687
Iteration 41, loss = 7.07458701
Iteration 42, loss = 7.04706517
Iteration 43, loss = 7.14111035
Iteration 44, loss = 7.08182556
Iteration 45, loss = 7.07281807
Iteration 46, loss = 7.09182300
Iteration 47, loss = 7.01161845
Iteration 48, loss = 7.00163575
Iteration 49, loss = 6.99786295
Iteration 50, loss = 7.00572913
Iteration 51, loss = 6.92238025
Iteration 52, loss = 6.98242559
Iteration 53, loss = 7.09161208
Iteration 54, loss = 6.98252755
Iteration 55, loss = 6.96257133
Iteration 56, loss = 6.97149618
Iteration 57, loss = 6.96544353
Iteration 58, loss = 6.91742194
Iteration 59, loss = 6.83927045
Iteration 60, loss = 6.92365548
Iteration 61, loss = 7.06962039
Iteration 62, loss = 6.93482001
Iteration 63, loss = 6.85122720
Iteration 64, loss = 6.85303817
Iteration 65, loss = 6.81712240
Iteration 66, loss = 6.80369294
Iteration 67, loss = 7.08179128
Iteration 68, loss = 6.88622239
Iteration 69, loss = 6.81698146
Iteration 70, loss = 6.82387768
Iteration 71, loss = 6.90064689
Iteration 72, loss = 6.83684798
Iteration 73, loss = 6.93409752
Iteration 74, loss = 6.87114543
Iteration 75, loss = 6.84090230
Iteration 76, loss = 6.78583670
Iteration 77, loss = 6.86396060
Iteration 78, loss = 6.93072720
Iteration 79, loss = 6.96426291
Iteration 80, loss = 6.97809955
Iteration 81, loss = 6.97267378
Iteration 82, loss = 6.90664450
Iteration 83, loss = 6.84415145
Iteration 84, loss = 6.89198647
Iteration 85, loss = 6.75850594
Iteration 86, loss = 6.76074349
Iteration 87, loss = 6.74860630
Iteration 88, loss = 6.93033130
Iteration 89, loss = 6.77904087
Iteration 90, loss = 6.66266442
Iteration 91, loss = 6.69696436
Iteration 92, loss = 6.79058923
Iteration 93, loss = 6.85253699
Iteration 94, loss = 6.71087174
Iteration 95, loss = 6.64866305
Iteration 96, loss = 6.62924454
Iteration 97, loss = 6.66706222
Iteration 98, loss = 6.68444599
Iteration 99, loss = 6.61599214
Iteration 100, loss = 6.74712190
Iteration 101, loss = 7.07995025
Iteration 102, loss = 6.94783016
Iteration 103, loss = 6.86440631
Iteration 104, loss = 6.87661226
Iteration 105, loss = 6.78997181
Iteration 106, loss = 6.80512559
Iteration 107, loss = 6.82079141
Iteration 108, loss = 6.71673250
Iteration 109, loss = 6.68603769
Iteration 110, loss = 6.66465266
Iteration 111, loss = 6.68005877
Iteration 112, loss = 6.74115115
Iteration 113, loss = 6.69847539
Iteration 114, loss = 6.89992075
Iteration 115, loss = 6.68544593
Iteration 116, loss = 6.99612797
Iteration 117, loss = 6.71985019
Iteration 118, loss = 6.67443203
Iteration 119, loss = 6.70814588
Iteration 120, loss = 6.82900844
Iteration 121, loss = 6.66647087
Iteration 122, loss = 6.72924667
Iteration 123, loss = 6.69175593
Iteration 124, loss = 6.58760025
Iteration 125, loss = 6.67183332
Iteration 126, loss = 6.74593871
Iteration 127, loss = 6.65772030
Iteration 128, loss = 6.63762781
Iteration 129, loss = 6.57941767
Iteration 130, loss = 6.63137993
Iteration 131, loss = 6.72651992
Iteration 132, loss = 6.59834708
Iteration 133, loss = 6.66200450
Iteration 134, loss = 6.72135453
Iteration 135, loss = 6.64063023
Iteration 136, loss = 6.61346020
Iteration 137, loss = 6.57292024
Iteration 138, loss = 6.91438690
Iteration 139, loss = 6.64147755
Iteration 140, loss = 6.63175234
Iteration 141, loss = 6.64378245
Iteration 142, loss = 6.84030172
Iteration 143, loss = 6.68397692
Iteration 144, loss = 6.82439621
Iteration 145, loss = 6.75545651
Iteration 146, loss = 6.70125891
Iteration 147, loss = 6.90152866
Iteration 148, loss = 6.60895504
Iteration 149, loss = 6.59567539
Iteration 150, loss = 6.74025460
Iteration 151, loss = 6.76074284
Iteration 152, loss = 6.58834752
Iteration 153, loss = 6.83712046
Iteration 154, loss = 6.54641419
Iteration 155, loss = 6.69348230
Iteration 156, loss = 6.55463832
Iteration 157, loss = 6.60587775
Iteration 158, loss = 6.69714915
Iteration 159, loss = 7.02498179
Iteration 160, loss = 6.70583001
Iteration 161, loss = 6.59475764
Iteration 162, loss = 6.75986101
Iteration 163, loss = 6.67775365
Iteration 164, loss = 6.61753597
Iteration 165, loss = 6.66316312
Iteration 166, loss = 6.61418277
Iteration 167, loss = 6.60391169
Iteration 168, loss = 6.63764521
Iteration 169, loss = 6.60565478
Iteration 170, loss = 6.74818431
Iteration 171, loss = 6.57648245
Iteration 172, loss = 6.64535047
Iteration 173, loss = 6.87809802
Iteration 174, loss = 6.72314367
Iteration 175, loss = 6.65329889
Iteration 176, loss = 6.60485368
Iteration 177, loss = 6.70404022
Iteration 178, loss = 7.00058052
Iteration 179, loss = 6.91791084
Iteration 180, loss = 7.03626301
Iteration 181, loss = 7.10387170
Iteration 182, loss = 7.18800095
Iteration 183, loss = 7.01657719
Iteration 184, loss = 6.91164170
Iteration 185, loss = 6.90039844
Iteration 186, loss = 6.87605299
Iteration 187, loss = 6.84853834
Iteration 188, loss = 6.80307625
Iteration 189, loss = 6.74014077
Iteration 190, loss = 6.79253625
Iteration 191, loss = 7.38512758
Iteration 192, loss = 6.93511645
Iteration 193, loss = 7.03708570
Iteration 194, loss = 6.92345221
Iteration 195, loss = 6.93455785
Iteration 196, loss = 6.86584122
Iteration 197, loss = 6.87732925
Iteration 198, loss = 6.85856365
Iteration 199, loss = 6.86826204
Iteration 200, loss = 6.80571123
Iteration 201, loss = 6.83752885
Iteration 202, loss = 6.74230069
Iteration 203, loss = 6.71606528
Iteration 204, loss = 6.72748264
Iteration 205, loss = 6.67707167
Iteration 206, loss = 6.66098234
Iteration 207, loss = 6.63799807
Iteration 208, loss = 6.67549902
Iteration 209, loss = 6.65073743
Iteration 210, loss = 6.63986820
Iteration 211, loss = 6.64818810
Iteration 212, loss = 6.59960189
Iteration 213, loss = 6.65562635
Iteration 214, loss = 6.71039782
Iteration 215, loss = 6.62063594
Iteration 216, loss = 6.82732657
Iteration 217, loss = 6.93700494
Iteration 218, loss = 7.02272164
Iteration 219, loss = 6.84495058
Iteration 220, loss = 6.72451719
Iteration 221, loss = 6.68985465
Iteration 222, loss = 6.60566415
Iteration 223, loss = 6.63575053
Iteration 224, loss = 6.64888333
Iteration 225, loss = 6.85999586
Iteration 226, loss = 6.74630257
Iteration 227, loss = 6.80229226
Iteration 228, loss = 6.88199291
Iteration 229, loss = 6.90112634
Iteration 230, loss = 6.74953553
Iteration 231, loss = 6.64984049
Iteration 232, loss = 6.61599885
Iteration 233, loss = 6.58706409
Iteration 234, loss = 6.80242173
Iteration 235, loss = 6.58373244
Iteration 236, loss = 6.61259149
Iteration 237, loss = 6.72487601
Iteration 238, loss = 6.66333058
Iteration 239, loss = 6.66351762
Iteration 240, loss = 6.71850073
Iteration 241, loss = 6.65554037
Iteration 242, loss = 6.63738767
Iteration 243, loss = 6.67034420
Iteration 244, loss = 6.66054148
Iteration 245, loss = 6.86897995
Iteration 246, loss = 6.68142381
Iteration 247, loss = 6.86663010
Iteration 248, loss = 6.78102737
Iteration 249, loss = 6.57090680
Iteration 250, loss = 6.83407816
Iteration 251, loss = 6.78907620
Iteration 252, loss = 6.67510421
Iteration 253, loss = 6.76260200
Iteration 254, loss = 6.58306665
Iteration 255, loss = 6.57469499
Iteration 256, loss = 6.59774053
Iteration 257, loss = 6.64641884
Iteration 258, loss = 6.69892364
Iteration 259, loss = 6.58235238
Iteration 260, loss = 6.65691003
Iteration 261, loss = 6.57343354
Iteration 262, loss = 6.60020795
Iteration 263, loss = 6.64349733
Iteration 264, loss = 6.69663771
Iteration 265, loss = 6.52773702
Iteration 266, loss = 6.60464605
Iteration 267, loss = 6.58769112
Iteration 268, loss = 6.54407243
Iteration 269, loss = 6.65489905
Iteration 270, loss = 6.82965692
Iteration 271, loss = 6.87030450
Iteration 272, loss = 6.73358757
Iteration 273, loss = 6.79508833
Iteration 274, loss = 6.63881529
Iteration 275, loss = 6.59033740
Iteration 276, loss = 6.61419651
Iteration 277, loss = 6.53257687
Iteration 278, loss = 6.58023492
Iteration 279, loss = 6.74084094
Iteration 280, loss = 6.55994756
Iteration 281, loss = 6.60967001
Iteration 282, loss = 6.65406888
Iteration 283, loss = 6.52170311
Iteration 284, loss = 6.56437595
Iteration 285, loss = 6.53041591
Iteration 286, loss = 6.58835259
Iteration 287, loss = 6.52069840
Iteration 288, loss = 6.52393481
Iteration 289, loss = 6.50396634
Iteration 290, loss = 6.49345054
Iteration 291, loss = 6.57422465
Iteration 292, loss = 6.51797403
Iteration 293, loss = 6.52724342
Iteration 294, loss = 6.65928771
Iteration 295, loss = 6.54584452
Iteration 296, loss = 6.48862197
Iteration 297, loss = 6.57707209
Iteration 298, loss = 6.69208764
Iteration 299, loss = 6.51569258
Iteration 300, loss = 6.54580486
Iteration 301, loss = 6.48953066
Iteration 302, loss = 6.47895354
Iteration 303, loss = 6.62478161
Iteration 304, loss = 6.92045996
Iteration 305, loss = 7.52884535
Iteration 306, loss = 7.40894306
Iteration 307, loss = 7.09903184
Iteration 308, loss = 7.42199231
Iteration 309, loss = 7.12145722
Iteration 310, loss = 7.10173981
Iteration 311, loss = 7.10859635
Iteration 312, loss = 6.99698125
Iteration 313, loss = 6.99678086
Iteration 314, loss = 7.00594970
Iteration 315, loss = 6.92130343
Iteration 316, loss = 6.90262278
Iteration 317, loss = 6.86661298
Iteration 318, loss = 6.89018251
Iteration 319, loss = 6.86612412
Iteration 320, loss = 6.84101968
Iteration 321, loss = 6.80261245
Iteration 322, loss = 6.84453788
Iteration 323, loss = 6.77402376
Iteration 324, loss = 6.76656732
Iteration 325, loss = 6.76461394
Iteration 326, loss = 6.79702461
Iteration 327, loss = 6.74801090
Iteration 328, loss = 6.81330641
Iteration 329, loss = 6.77855320
Iteration 330, loss = 6.74276171
Iteration 331, loss = 6.78245127
Iteration 332, loss = 6.78439252
Iteration 333, loss = 6.73780005
Iteration 334, loss = 6.78024489
Iteration 335, loss = 6.87583338
Iteration 336, loss = 6.77202833
Iteration 337, loss = 6.79032310
Iteration 338, loss = 6.76914995
Iteration 339, loss = 6.81918061
Iteration 340, loss = 6.84181726
Iteration 341, loss = 6.74735209
Iteration 342, loss = 6.75313727
Iteration 343, loss = 6.72537276
Iteration 344, loss = 6.77738538
Iteration 345, loss = 6.74034147
Iteration 346, loss = 6.70916677
Iteration 347, loss = 6.73066118
Iteration 348, loss = 6.73266040
Iteration 349, loss = 6.75652477
Iteration 350, loss = 6.69609653
Iteration 351, loss = 6.84191467
Iteration 352, loss = 6.70108303
Iteration 353, loss = 6.68451754
Iteration 354, loss = 6.69210803
Iteration 355, loss = 6.73826874
Iteration 356, loss = 6.73404167
Iteration 357, loss = 6.62469154
Iteration 358, loss = 6.80332752
Iteration 359, loss = 6.70349347
Iteration 360, loss = 6.68995575
Iteration 361, loss = 6.63681022
Iteration 362, loss = 6.69996938
Iteration 363, loss = 6.70941321
Iteration 364, loss = 6.63225229
Iteration 365, loss = 6.65034820
Iteration 366, loss = 6.73056049
Iteration 367, loss = 6.64101863
Iteration 368, loss = 6.69156467
Iteration 369, loss = 6.67869400
Iteration 370, loss = 6.65883241
Iteration 371, loss = 6.67306565
Iteration 372, loss = 6.73387121
Iteration 373, loss = 6.63717710
Iteration 374, loss = 6.66624705
Iteration 375, loss = 6.63142566
Iteration 376, loss = 6.63402208
Iteration 377, loss = 6.63523400
Iteration 378, loss = 6.66751595
Iteration 379, loss = 6.70393426
Iteration 380, loss = 6.63413244
Iteration 381, loss = 6.67235641
Iteration 382, loss = 6.63079546
Iteration 383, loss = 6.63556385
Iteration 384, loss = 6.76506418
Iteration 385, loss = 6.68890503
Iteration 386, loss = 6.65941084
Iteration 387, loss = 6.67307530
Iteration 388, loss = 6.66273673
Iteration 389, loss = 6.59842756
Iteration 390, loss = 6.58755733
Iteration 391, loss = 6.63649494
Iteration 392, loss = 6.58036411
Iteration 393, loss = 6.60900230
Iteration 394, loss = 6.60745325
Iteration 395, loss = 6.58196516
Iteration 396, loss = 6.60953484
Iteration 397, loss = 6.59314306
Iteration 398, loss = 6.60450078
Iteration 399, loss = 6.73986077
Iteration 400, loss = 6.70088826
Iteration 401, loss = 6.68723817
Iteration 402, loss = 6.63960172
Iteration 403, loss = 6.69691960
Iteration 404, loss = 6.61630901
Iteration 405, loss = 6.61149699
Iteration 406, loss = 6.59197079
Iteration 407, loss = 6.69302777
Iteration 408, loss = 6.54934054
Iteration 409, loss = 6.63769900
Iteration 410, loss = 6.61532269
Iteration 411, loss = 6.60898922
Iteration 412, loss = 6.85428532
Iteration 413, loss = 6.67341940
Iteration 414, loss = 6.58194577
Iteration 415, loss = 6.64935586
Iteration 416, loss = 6.64710926
Iteration 417, loss = 6.60239332
Iteration 418, loss = 6.59459279
Iteration 419, loss = 6.57189751
Iteration 420, loss = 6.59052233
Iteration 421, loss = 6.68887262
Iteration 422, loss = 6.68403411
Iteration 423, loss = 6.69267255
Iteration 424, loss = 6.66429269
Iteration 425, loss = 6.64605439
Iteration 426, loss = 6.54108049
Iteration 427, loss = 6.53210189
Iteration 428, loss = 6.60747586
Iteration 429, loss = 6.67379621
Iteration 430, loss = 6.72144836
Iteration 431, loss = 6.62526668
Iteration 432, loss = 6.92627117
Iteration 433, loss = 6.94712295
Iteration 434, loss = 7.05841618
Iteration 435, loss = 6.89813130
Iteration 436, loss = 6.81757029
Iteration 437, loss = 6.67280962
Iteration 438, loss = 6.59903124
Iteration 439, loss = 6.61145489
Iteration 440, loss = 6.61460735
Iteration 441, loss = 6.57997462
Iteration 442, loss = 6.62918407
Iteration 443, loss = 6.60531310
Iteration 444, loss = 6.57612862
Iteration 445, loss = 6.56393753
Iteration 446, loss = 6.58286532
Iteration 447, loss = 6.59516013
Iteration 448, loss = 6.65351049
Iteration 449, loss = 6.62574557
Iteration 450, loss = 6.61099314
Iteration 451, loss = 6.53086692
Iteration 452, loss = 6.70201605
Iteration 453, loss = 6.52065441
Iteration 454, loss = 6.52521709
Iteration 455, loss = 6.54015759
Iteration 456, loss = 6.53821804
Iteration 457, loss = 6.52346612
Iteration 458, loss = 6.49711962
Iteration 459, loss = 6.58287043
Iteration 460, loss = 6.56643266
Iteration 461, loss = 6.56812756
Iteration 462, loss = 6.50159921
Iteration 463, loss = 6.50211634
Iteration 464, loss = 6.51674617
Iteration 465, loss = 6.56757743
Iteration 466, loss = 6.53444912
Iteration 467, loss = 6.65096828
Iteration 468, loss = 6.54661631
Iteration 469, loss = 6.72082584
Iteration 470, loss = 6.50011144
Iteration 471, loss = 6.49096398
Iteration 472, loss = 6.88613188
Iteration 473, loss = 6.59770012
Iteration 474, loss = 6.69784359
Iteration 475, loss = 6.81363464
Iteration 476, loss = 6.87413779
Iteration 477, loss = 6.61776149
Iteration 478, loss = 6.54108886
Iteration 479, loss = 6.56417274
Iteration 480, loss = 6.54437012
Iteration 481, loss = 6.58649101
Iteration 482, loss = 6.52062635
Iteration 483, loss = 6.61010474
Iteration 484, loss = 6.61294700
Iteration 485, loss = 6.65434550
Iteration 486, loss = 6.78098731
Iteration 487, loss = 6.77026238
Iteration 488, loss = 6.68982074
Iteration 489, loss = 6.63733456
Iteration 490, loss = 6.55196829
Iteration 491, loss = 6.49412567
Iteration 492, loss = 6.52205985
Iteration 493, loss = 6.50907970
Iteration 494, loss = 6.47883159
Iteration 495, loss = 6.51645462
Iteration 496, loss = 6.56180085
Iteration 497, loss = 6.53272691
Iteration 498, loss = 6.49314301
Iteration 499, loss = 6.47912899
Iteration 500, loss = 6.50423672
Iteration 501, loss = 6.48983502
Iteration 502, loss = 6.52332467
Iteration 503, loss = 6.48739712
Iteration 504, loss = 6.49265286
Iteration 505, loss = 6.46749643
Iteration 506, loss = 6.47508939
Iteration 507, loss = 6.48283238
Iteration 508, loss = 6.48068793
Iteration 509, loss = 6.46017998
Iteration 510, loss = 6.46177344
Iteration 511, loss = 6.55193552
Iteration 512, loss = 6.67562354
Iteration 513, loss = 6.61147978
Iteration 514, loss = 6.69175667
Iteration 515, loss = 6.47010637
Iteration 516, loss = 6.89488004
Iteration 517, loss = 6.98609191
Iteration 518, loss = 6.99659076
Iteration 519, loss = 6.79782099
Iteration 520, loss = 6.57543886
Iteration 521, loss = 6.66377353
Iteration 522, loss = 6.68590376
Iteration 523, loss = 6.64590351
Iteration 524, loss = 6.60816009
Iteration 525, loss = 6.68922234
Iteration 526, loss = 6.53852248
Iteration 527, loss = 6.68132123
Iteration 528, loss = 6.87628221
Iteration 529, loss = 6.51568117
Iteration 530, loss = 6.81820198
Iteration 531, loss = 6.52034621
Iteration 532, loss = 6.60848841
Iteration 533, loss = 6.64040009
Iteration 534, loss = 6.61719591
Iteration 535, loss = 6.56540013
Iteration 536, loss = 6.53140294
Iteration 537, loss = 6.47826558
Iteration 538, loss = 6.53721414
Iteration 539, loss = 6.71599761
Iteration 540, loss = 6.54453682
Iteration 541, loss = 6.63187839
Iteration 542, loss = 6.52341333
Iteration 543, loss = 6.49896527
Iteration 544, loss = 6.49596946
Iteration 545, loss = 6.47035545
Iteration 546, loss = 6.46982168
Iteration 547, loss = 6.51475001
Iteration 548, loss = 6.51503556
Iteration 549, loss = 6.47004913
Iteration 550, loss = 6.43900153
Iteration 551, loss = 6.43903851
Iteration 552, loss = 6.51647978
Iteration 553, loss = 6.46579754
Iteration 554, loss = 6.43527479
Iteration 555, loss = 6.45304092
Iteration 556, loss = 6.47378603
Iteration 557, loss = 6.45573285
Iteration 558, loss = 6.45108918
Iteration 559, loss = 6.42297414
Iteration 560, loss = 6.44244296
Iteration 561, loss = 6.51507657
Iteration 562, loss = 6.49571774
Iteration 563, loss = 6.47155277
Iteration 564, loss = 6.41621614
Iteration 565, loss = 6.52356725
Iteration 566, loss = 6.51305898
Iteration 567, loss = 6.60686141
Iteration 568, loss = 6.60960070
Iteration 569, loss = 6.43574032
Iteration 570, loss = 6.46495337
Iteration 571, loss = 6.46591473
Iteration 572, loss = 6.43148302
Iteration 573, loss = 6.45281092
Iteration 574, loss = 6.40139561
Iteration 575, loss = 6.45821240
Iteration 576, loss = 6.46760188
Iteration 577, loss = 6.48372501
Iteration 578, loss = 6.48812266
Iteration 579, loss = 6.40601784
Iteration 580, loss = 6.53789496
Iteration 581, loss = 6.50490111
Iteration 582, loss = 6.42670269
Iteration 583, loss = 6.59030761
Iteration 584, loss = 6.83475460
Iteration 585, loss = 6.79170585
Iteration 586, loss = 6.78787895
Iteration 587, loss = 6.75289367
Iteration 588, loss = 6.61571100
Iteration 589, loss = 6.75634579
Iteration 590, loss = 6.75394850
Iteration 591, loss = 6.50729316
Iteration 592, loss = 6.74644071
Iteration 593, loss = 6.52521734
Iteration 594, loss = 6.59130674
Iteration 595, loss = 6.49447055
Iteration 596, loss = 6.43449296
Iteration 597, loss = 6.46712354
Iteration 598, loss = 6.42415713
Iteration 599, loss = 6.43360254
Iteration 600, loss = 6.42254913
Iteration 601, loss = 6.46094641
Iteration 602, loss = 6.39887115
Iteration 603, loss = 6.53141328
Iteration 604, loss = 6.47112865
Iteration 605, loss = 6.43366411
Iteration 606, loss = 6.57421632
Iteration 607, loss = 6.37961599
Iteration 608, loss = 6.43571321
Iteration 609, loss = 6.41842318
Iteration 610, loss = 6.45453943
Iteration 611, loss = 6.42572044
Iteration 612, loss = 6.45756321
Iteration 613, loss = 6.39631061
Iteration 614, loss = 6.56530759
Iteration 615, loss = 6.59300728
Iteration 616, loss = 6.48058256
Iteration 617, loss = 6.54169212
Iteration 618, loss = 6.73942898
Iteration 619, loss = 6.76251487
Iteration 620, loss = 6.89351741
Iteration 621, loss = 6.59514552
Iteration 622, loss = 6.48603781
Iteration 623, loss = 6.60301281
Iteration 624, loss = 6.46715512
Iteration 625, loss = 6.61023305
Iteration 626, loss = 6.56245888
Iteration 627, loss = 6.56798667
Iteration 628, loss = 6.46612208
Iteration 629, loss = 6.43883999
Iteration 630, loss = 6.39026455
Iteration 631, loss = 6.44860822
Iteration 632, loss = 6.41380682
Iteration 633, loss = 6.57348464
Iteration 634, loss = 6.43331655
Iteration 635, loss = 6.51314491
Iteration 636, loss = 6.44042910
Iteration 637, loss = 6.41091273
Iteration 638, loss = 6.50486793
Iteration 639, loss = 6.63301123
Iteration 640, loss = 6.46821312
Iteration 641, loss = 6.64809844
Iteration 642, loss = 6.67484262
Iteration 643, loss = 6.73548064
Iteration 644, loss = 6.53713685
Iteration 645, loss = 6.40080372
Iteration 646, loss = 6.52084999
Iteration 647, loss = 6.38878043
Iteration 648, loss = 6.46341325
Iteration 649, loss = 6.46738688
Iteration 650, loss = 6.40550820
Iteration 651, loss = 6.43616648
Iteration 652, loss = 6.40746938
Iteration 653, loss = 6.38471178
Iteration 654, loss = 6.35746929
Iteration 655, loss = 6.40982116
Iteration 656, loss = 6.42575603
Iteration 657, loss = 6.54295104
Iteration 658, loss = 6.38401326
Iteration 659, loss = 6.38621432
Iteration 660, loss = 6.54155259
Iteration 661, loss = 6.37195147
Iteration 662, loss = 6.34133711
Iteration 663, loss = 6.42696865
Iteration 664, loss = 6.43952027
Iteration 665, loss = 6.38222038
Iteration 666, loss = 6.42474661
Iteration 667, loss = 6.37182622
Iteration 668, loss = 6.51343343
Iteration 669, loss = 6.40630988
Iteration 670, loss = 6.38997476
Iteration 671, loss = 6.34938384
Iteration 672, loss = 6.40148096
Iteration 673, loss = 6.33288515
Iteration 674, loss = 6.37205364
Iteration 675, loss = 6.32407684
Iteration 676, loss = 6.36247589
Iteration 677, loss = 6.35060205
Iteration 678, loss = 6.36954359
Iteration 679, loss = 6.36946646
Iteration 680, loss = 6.42146109
Iteration 681, loss = 6.34743831
Iteration 682, loss = 6.41617674
Iteration 683, loss = 6.38104625
Iteration 684, loss = 6.38992027
Iteration 685, loss = 6.36897800
Iteration 686, loss = 6.31796744
Iteration 687, loss = 6.36725149
Iteration 688, loss = 6.33674716
Iteration 689, loss = 6.35884619
Iteration 690, loss = 6.30898222
Iteration 691, loss = 6.34423720
Iteration 692, loss = 6.38987722
Iteration 693, loss = 6.31783725
Iteration 694, loss = 6.37699850
Iteration 695, loss = 6.43008004
Iteration 696, loss = 6.51480644
Iteration 697, loss = 6.30416391
Iteration 698, loss = 6.43618305
Iteration 699, loss = 6.34909054
Iteration 700, loss = 6.40554892
Iteration 701, loss = 6.30938781
Iteration 702, loss = 6.47840223
Iteration 703, loss = 6.66454055
Iteration 704, loss = 6.60341696
Iteration 705, loss = 6.33834500
Iteration 706, loss = 6.40936841
Iteration 707, loss = 6.38424073
Iteration 708, loss = 6.33307729
Iteration 709, loss = 6.39112290
Iteration 710, loss = 6.36819032
Iteration 711, loss = 6.32728714
Iteration 712, loss = 6.37755827
Iteration 713, loss = 6.40780870
Iteration 714, loss = 6.40345657
Iteration 715, loss = 6.55001283
Iteration 716, loss = 6.38780091
Iteration 717, loss = 6.33172433
Iteration 718, loss = 6.52803451
Iteration 719, loss = 6.55985947
Iteration 720, loss = 6.58199893
Iteration 721, loss = 6.31941177
Iteration 722, loss = 6.50717837
Iteration 723, loss = 6.49459566
Iteration 724, loss = 6.61127351
Iteration 725, loss = 6.42968038
Iteration 726, loss = 6.58786886
Iteration 727, loss = 6.44200754
Iteration 728, loss = 6.50549040
Iteration 729, loss = 6.41070780
Iteration 730, loss = 6.53920532
Iteration 731, loss = 6.37391778
Iteration 732, loss = 6.36701226
Iteration 733, loss = 6.34207551
Iteration 734, loss = 6.38484817
Iteration 735, loss = 6.35235931
Iteration 736, loss = 6.37496312
Iteration 737, loss = 6.37384085
Iteration 738, loss = 6.40217873
Iteration 739, loss = 6.32605941
Iteration 740, loss = 6.28760929
Iteration 741, loss = 6.38656856
Iteration 742, loss = 6.30820931
Iteration 743, loss = 6.36970853
Iteration 744, loss = 6.32646634
Iteration 745, loss = 6.29950863
Iteration 746, loss = 6.30795842
Iteration 747, loss = 6.40734550
Iteration 748, loss = 6.36118471
Iteration 749, loss = 6.46415975
Iteration 750, loss = 6.29989641
Iteration 751, loss = 6.29772463
Iteration 752, loss = 6.58182376
Iteration 753, loss = 6.40470944
Iteration 754, loss = 6.43827802
Iteration 755, loss = 6.43123407
Iteration 756, loss = 6.42153969
Iteration 757, loss = 6.30406641
Iteration 758, loss = 6.33517832
Iteration 759, loss = 6.32755601
Iteration 760, loss = 6.35516968
Iteration 761, loss = 6.35431335
Iteration 762, loss = 6.31627106
Iteration 763, loss = 6.30571873
Iteration 764, loss = 6.33572898
Iteration 765, loss = 6.37942687
Iteration 766, loss = 6.45562990
Iteration 767, loss = 6.40653735
Iteration 768, loss = 6.36324265
Iteration 769, loss = 6.34852249
Iteration 770, loss = 6.51569493
Iteration 771, loss = 6.57650280
Iteration 772, loss = 6.46579967
Iteration 773, loss = 6.42380783
Iteration 774, loss = 6.37629814
Iteration 775, loss = 6.32377030
Iteration 776, loss = 6.34211954
Iteration 777, loss = 6.27531740
Iteration 778, loss = 6.30152102
Iteration 779, loss = 6.27158362
Iteration 780, loss = 6.26358893
Iteration 781, loss = 6.34193869
Iteration 782, loss = 6.32518499
Iteration 783, loss = 6.34689943
Iteration 784, loss = 6.29185332
Iteration 785, loss = 6.32505296
Iteration 786, loss = 6.32507249
Iteration 787, loss = 6.45849179
Iteration 788, loss = 6.42736397
Iteration 789, loss = 6.35754926
Iteration 790, loss = 6.33146290
Iteration 791, loss = 6.26230585
Iteration 792, loss = 6.45544673
Iteration 793, loss = 6.53083917
Iteration 794, loss = 6.35087798
Iteration 795, loss = 6.39446521
Iteration 796, loss = 6.43576138
Iteration 797, loss = 6.32306901
Iteration 798, loss = 6.32202459
Iteration 799, loss = 6.32472787
Iteration 800, loss = 6.39305168
Iteration 801, loss = 6.25708615
Iteration 802, loss = 6.35794766
Iteration 803, loss = 6.36894694
Iteration 804, loss = 6.49328587
Iteration 805, loss = 6.33906732
Iteration 806, loss = 6.38343272
Iteration 807, loss = 6.46502929
Iteration 808, loss = 6.41940579
Iteration 809, loss = 6.31201074
Iteration 810, loss = 6.35250566
Iteration 811, loss = 6.51896612
Iteration 812, loss = 6.27966468
Iteration 813, loss = 6.36693119
Iteration 814, loss = 6.31761056
Iteration 815, loss = 6.34241437
Iteration 816, loss = 6.23130507
Iteration 817, loss = 6.35149422
Iteration 818, loss = 6.47512661
Iteration 819, loss = 6.32600102
Iteration 820, loss = 6.37728867
Iteration 821, loss = 6.33909368
Iteration 822, loss = 6.29043787
Iteration 823, loss = 6.25785834
Iteration 824, loss = 6.43572697
Iteration 825, loss = 6.36781093
Iteration 826, loss = 6.27300752
Iteration 827, loss = 6.41920497
Iteration 828, loss = 6.28386868
Iteration 829, loss = 6.29991497
Iteration 830, loss = 6.28612824
Iteration 831, loss = 6.30458501
Iteration 832, loss = 6.27166734
Iteration 833, loss = 6.31075349
Iteration 834, loss = 6.27983276
Iteration 835, loss = 6.36905780
Iteration 836, loss = 6.33379810
Iteration 837, loss = 6.37668975
Iteration 838, loss = 6.29007965
Iteration 839, loss = 6.25552620
Iteration 840, loss = 6.28922736
Iteration 841, loss = 6.30588202
Iteration 842, loss = 6.25968404
Iteration 843, loss = 6.30357224
Iteration 844, loss = 6.29847838
Iteration 845, loss = 6.31753374
Iteration 846, loss = 6.34365094
Iteration 847, loss = 6.37175506
Iteration 848, loss = 6.34108999
Iteration 849, loss = 6.42788571
Iteration 850, loss = 6.32174454
Iteration 851, loss = 6.26300907
Iteration 852, loss = 6.30756985
Iteration 853, loss = 6.25661649
Iteration 854, loss = 6.24342715
Iteration 855, loss = 6.28126257
Iteration 856, loss = 6.24325407
Iteration 857, loss = 6.32989992
Iteration 858, loss = 6.28586068
Iteration 859, loss = 6.30196767
Iteration 860, loss = 6.46233726
Iteration 861, loss = 6.45713849
Iteration 862, loss = 6.34885856
Iteration 863, loss = 6.39373043
Iteration 864, loss = 6.24171129
Iteration 865, loss = 6.37548149
Iteration 866, loss = 6.24512224
Iteration 867, loss = 6.32454604
Iteration 868, loss = 6.33729902
Iteration 869, loss = 6.38840567
Iteration 870, loss = 6.24115450
Iteration 871, loss = 6.33062564
Iteration 872, loss = 6.23683109
Iteration 873, loss = 6.29591061
Iteration 874, loss = 6.36165896
Iteration 875, loss = 6.31727429
Iteration 876, loss = 6.30995355
Iteration 877, loss = 6.35420475
Iteration 878, loss = 6.49673470
Iteration 879, loss = 6.30591649
Iteration 880, loss = 6.36595399
Iteration 881, loss = 6.35495949
Iteration 882, loss = 6.28862006
Iteration 883, loss = 6.24646130
Iteration 884, loss = 6.24738267
Iteration 885, loss = 6.30192897
Iteration 886, loss = 6.35374514
Iteration 887, loss = 6.32184098
Iteration 888, loss = 6.30555711
Iteration 889, loss = 6.33363781
Iteration 890, loss = 6.26624277
Iteration 891, loss = 6.23489233
Iteration 892, loss = 6.21178473
Iteration 893, loss = 6.19889861
Iteration 894, loss = 6.19990674
Iteration 895, loss = 6.20920020
Iteration 896, loss = 6.30036527
Iteration 897, loss = 6.38149641
Iteration 898, loss = 6.37440328
Iteration 899, loss = 6.36437475
Iteration 900, loss = 6.34715217
Iteration 901, loss = 6.41301112
Iteration 902, loss = 6.39583583
Iteration 903, loss = 6.34120728
Iteration 904, loss = 6.38293447
Iteration 905, loss = 6.33418272
Iteration 906, loss = 6.33803438
Iteration 907, loss = 6.27808194
Iteration 908, loss = 6.19577148
Iteration 909, loss = 6.38096597
Iteration 910, loss = 6.28620681
Iteration 911, loss = 6.26212503
Iteration 912, loss = 6.26167104
Iteration 913, loss = 6.23543632
Iteration 914, loss = 6.29028302
Iteration 915, loss = 6.28407832
Iteration 916, loss = 6.25410882
Iteration 917, loss = 6.18296600
Iteration 918, loss = 6.32701668
Iteration 919, loss = 6.32446027
Iteration 920, loss = 6.34167939
Iteration 921, loss = 6.23414798
Iteration 922, loss = 6.28241416
Iteration 923, loss = 6.42021049
Iteration 924, loss = 6.40457958
Iteration 925, loss = 6.29651797
Iteration 926, loss = 6.30253724
Iteration 927, loss = 6.39567303
Iteration 928, loss = 6.28384025
Iteration 929, loss = 6.30462036
Iteration 930, loss = 6.33017342
Iteration 931, loss = 6.25275709
Iteration 932, loss = 6.19410014
Iteration 933, loss = 6.28648706
Iteration 934, loss = 6.18326548
Iteration 935, loss = 6.29211745
Iteration 936, loss = 6.29522183
Iteration 937, loss = 6.25108070
Iteration 938, loss = 6.34158390
Iteration 939, loss = 6.26169976
Iteration 940, loss = 6.25624466
Iteration 941, loss = 6.17129789
Iteration 942, loss = 6.34475432
Iteration 943, loss = 6.25751332
Iteration 944, loss = 6.60676797
Iteration 945, loss = 6.31520130
Iteration 946, loss = 6.41990316
Iteration 947, loss = 6.31843834
Iteration 948, loss = 6.23036009
Iteration 949, loss = 6.23591198
Iteration 950, loss = 6.60106808
Iteration 951, loss = 6.37726226
Iteration 952, loss = 6.36540264
Iteration 953, loss = 6.38319197
Iteration 954, loss = 6.31067892
Iteration 955, loss = 6.28772835
Iteration 956, loss = 6.37615739
Iteration 957, loss = 6.17242271
Iteration 958, loss = 6.33759219
Iteration 959, loss = 6.23880774
Iteration 960, loss = 6.38452501
Iteration 961, loss = 6.20461871
Iteration 962, loss = 6.47239088
Iteration 963, loss = 6.27327974
Iteration 964, loss = 6.36174076
Iteration 965, loss = 6.29244990
Iteration 966, loss = 6.21108509
Iteration 967, loss = 6.25605261
Iteration 968, loss = 6.21139365
Iteration 969, loss = 6.22889882
Iteration 970, loss = 6.30433771
Iteration 971, loss = 6.24332292
Iteration 972, loss = 6.18060552
Iteration 973, loss = 6.28057216
Iteration 974, loss = 6.18586766
Iteration 975, loss = 6.50467050
Iteration 976, loss = 6.32311501
Iteration 977, loss = 6.28366710
Iteration 978, loss = 6.41006871
Iteration 979, loss = 6.20936697
Iteration 980, loss = 6.36224909
Iteration 981, loss = 6.27277589
Iteration 982, loss = 6.20200041
Iteration 983, loss = 6.17768756
Iteration 984, loss = 6.24951241
Iteration 985, loss = 6.43663190
Iteration 986, loss = 6.40875353
Iteration 987, loss = 6.19555012
Iteration 988, loss = 6.29117549
Iteration 989, loss = 6.53928780
Iteration 990, loss = 6.39487818
Iteration 991, loss = 6.47625375
Iteration 992, loss = 6.51624135
Iteration 993, loss = 6.60916483
Iteration 994, loss = 6.34779358
Iteration 995, loss = 6.33317522
Iteration 996, loss = 6.45515483
Iteration 997, loss = 6.12080579
Iteration 998, loss = 6.39859862
Iteration 999, loss = 6.36654241
Iteration 1000, loss = 6.16952603
Run 2
Iteration 1, loss = 2905.34568868
Iteration 2, loss = 137.50339012
Iteration 3, loss = 27.86280403
Iteration 4, loss = 23.39661532
Iteration 5, loss = 14.73048417
Iteration 6, loss = 10.18720487
Iteration 7, loss = 13.19166813
Iteration 8, loss = 8.70266933
Iteration 9, loss = 10.11898856
Iteration 10, loss = 9.73564151
Iteration 11, loss = 8.05443320
Iteration 12, loss = 8.79167358
Iteration 13, loss = 7.97603323
Iteration 14, loss = 10.05684076
Iteration 15, loss = 8.03527373
Iteration 16, loss = 7.87596848
Iteration 17, loss = 7.58507578
Iteration 18, loss = 8.08710442
Iteration 19, loss = 7.83017493
Iteration 20, loss = 7.76582252
Iteration 21, loss = 7.64651535
Iteration 22, loss = 7.54318313
Iteration 23, loss = 7.53665300
Iteration 24, loss = 7.54429714
Iteration 25, loss = 7.62531649
Iteration 26, loss = 7.70456386
Iteration 27, loss = 7.52770664
Iteration 28, loss = 7.59967396
Iteration 29, loss = 7.66231933
Iteration 30, loss = 7.48690546
Iteration 31, loss = 7.62110565
Iteration 32, loss = 7.82175701
Iteration 33, loss = 7.62337604
Iteration 34, loss = 7.56292209
Iteration 35, loss = 7.74318303
Iteration 36, loss = 7.59792336
Iteration 37, loss = 7.45504124
Iteration 38, loss = 7.46565285
Iteration 39, loss = 7.39679613
Iteration 40, loss = 7.40788091
Iteration 41, loss = 7.33134953
Iteration 42, loss = 7.36145834
Iteration 43, loss = 7.29618612
Iteration 44, loss = 7.52985466
Iteration 45, loss = 7.41249416
Iteration 46, loss = 7.38846047
Iteration 47, loss = 7.40682653
Iteration 48, loss = 7.20872428
Iteration 49, loss = 7.25244752
Iteration 50, loss = 7.12023663
Iteration 51, loss = 7.10386830
Iteration 52, loss = 7.16076187
Iteration 53, loss = 7.23433133
Iteration 54, loss = 7.12442628
Iteration 55, loss = 7.04667562
Iteration 56, loss = 7.06683460
Iteration 57, loss = 7.07661920
Iteration 58, loss = 7.28926306
Iteration 59, loss = 7.13231965
Iteration 60, loss = 7.05902262
Iteration 61, loss = 7.09181425
Iteration 62, loss = 7.12997395
Iteration 63, loss = 7.03885066
Iteration 64, loss = 7.32425524
Iteration 65, loss = 6.97900596
Iteration 66, loss = 6.96485059
Iteration 67, loss = 6.96938474
Iteration 68, loss = 7.10479535
Iteration 69, loss = 7.02187040
Iteration 70, loss = 6.94697426
Iteration 71, loss = 6.91467617
Iteration 72, loss = 7.00124088
Iteration 73, loss = 7.08332993
Iteration 74, loss = 6.91296411
Iteration 75, loss = 6.94727012
Iteration 76, loss = 6.90537658
Iteration 77, loss = 6.98828855
Iteration 78, loss = 6.93972135
Iteration 79, loss = 7.14520437
Iteration 80, loss = 6.99904871
Iteration 81, loss = 6.90658768
Iteration 82, loss = 7.01940780
Iteration 83, loss = 7.08763396
Iteration 84, loss = 6.93633142
Iteration 85, loss = 7.14895963
Iteration 86, loss = 7.12637825
Iteration 87, loss = 7.30495713
Iteration 88, loss = 7.35448260
Iteration 89, loss = 7.16133159
Iteration 90, loss = 6.91201467
Iteration 91, loss = 6.97422960
Iteration 92, loss = 6.95150549
Iteration 93, loss = 6.93841053
Iteration 94, loss = 6.89392068
Iteration 95, loss = 7.04311793
Iteration 96, loss = 7.11020288
Iteration 97, loss = 7.03326778
Iteration 98, loss = 6.94669190
Iteration 99, loss = 7.01236557
Iteration 100, loss = 7.35425481
Iteration 101, loss = 7.07074974
Iteration 102, loss = 7.12551984
Iteration 103, loss = 7.00242331
Iteration 104, loss = 6.97263590
Iteration 105, loss = 7.16519407
Iteration 106, loss = 7.15611793
Iteration 107, loss = 7.00048479
Iteration 108, loss = 6.90607163
Iteration 109, loss = 6.98824630
Iteration 110, loss = 7.08087054
Iteration 111, loss = 7.02659376
Iteration 112, loss = 6.95999246
Iteration 113, loss = 7.14051325
Iteration 114, loss = 7.08877135
Iteration 115, loss = 7.17417641
Iteration 116, loss = 6.96856229
Iteration 117, loss = 7.22784394
Iteration 118, loss = 7.80981938
Iteration 119, loss = 7.88092217
Iteration 120, loss = 7.21732981
Iteration 121, loss = 7.20959961
Iteration 122, loss = 7.63962819
Iteration 123, loss = 7.04712901
Iteration 124, loss = 7.04783383
Iteration 125, loss = 7.02200432
Iteration 126, loss = 6.87520265
Iteration 127, loss = 7.11256315
Iteration 128, loss = 6.98545544
Iteration 129, loss = 6.99669896
Iteration 130, loss = 7.05121423
Iteration 131, loss = 7.04760589
Iteration 132, loss = 6.93980264
Iteration 133, loss = 6.91649361
Iteration 134, loss = 7.07524806
Iteration 135, loss = 7.18413018
Iteration 136, loss = 6.98211076
Iteration 137, loss = 7.01360829
Iteration 138, loss = 6.95958772
Iteration 139, loss = 7.17415173
Iteration 140, loss = 7.02764862
Iteration 141, loss = 6.91656548
Iteration 142, loss = 6.94464634
Iteration 143, loss = 6.98487020
Iteration 144, loss = 6.98546576
Iteration 145, loss = 7.24244113
Iteration 146, loss = 7.02736793
Iteration 147, loss = 6.86221510
Iteration 148, loss = 7.31316792
Iteration 149, loss = 6.93446529
Iteration 150, loss = 6.91938032
Iteration 151, loss = 6.98618607
Iteration 152, loss = 6.91197431
Iteration 153, loss = 6.91663270
Iteration 154, loss = 6.93837326
Iteration 155, loss = 6.95175734
Iteration 156, loss = 7.29358183
Iteration 157, loss = 7.25643952
Iteration 158, loss = 7.07765394
Iteration 159, loss = 7.49909269
Iteration 160, loss = 7.08147941
Iteration 161, loss = 7.06681305
Iteration 162, loss = 6.88164001
Iteration 163, loss = 6.90152017
Iteration 164, loss = 7.00893260
Iteration 165, loss = 7.08222440
Iteration 166, loss = 7.03044103
Iteration 167, loss = 6.93190745
Iteration 168, loss = 6.97978147
Iteration 169, loss = 6.91468055
Iteration 170, loss = 6.95302222
Iteration 171, loss = 7.06967842
Iteration 172, loss = 7.23683231
Iteration 173, loss = 7.27487111
Iteration 174, loss = 6.96993113
Iteration 175, loss = 7.17458527
Iteration 176, loss = 7.16747367
Iteration 177, loss = 6.88411163
Iteration 178, loss = 7.08848537
Iteration 179, loss = 6.94685351
Iteration 180, loss = 7.04038834
Iteration 181, loss = 7.05562679
Iteration 182, loss = 7.04511067
Iteration 183, loss = 6.91640046
Iteration 184, loss = 6.91497720
Iteration 185, loss = 7.02414224
Iteration 186, loss = 7.08926591
Iteration 187, loss = 7.58687649
Iteration 188, loss = 7.27688765
Iteration 189, loss = 6.87336749
Iteration 190, loss = 6.97759226
Iteration 191, loss = 6.92485202
Iteration 192, loss = 7.10985045
Iteration 193, loss = 6.97667610
Iteration 194, loss = 6.98196417
Iteration 195, loss = 6.89688414
Iteration 196, loss = 6.89930916
Iteration 197, loss = 7.28342595
Iteration 198, loss = 7.06931813
Iteration 199, loss = 6.94655474
Iteration 200, loss = 6.99515606
Iteration 201, loss = 7.02268453
Iteration 202, loss = 7.09375257
Iteration 203, loss = 7.11866363
Iteration 204, loss = 7.03759918
Iteration 205, loss = 7.08354791
Iteration 206, loss = 7.14562384
Iteration 207, loss = 7.17578808
Iteration 208, loss = 7.18979541
Iteration 209, loss = 7.43820886
Iteration 210, loss = 7.11243319
Iteration 211, loss = 7.09833915
Iteration 212, loss = 7.52848072
Iteration 213, loss = 7.10730020
Iteration 214, loss = 7.04675831
Iteration 215, loss = 7.34809610
Iteration 216, loss = 6.95541723
Iteration 217, loss = 6.91458720
Iteration 218, loss = 6.91500658
Iteration 219, loss = 7.02508911
Iteration 220, loss = 7.35376149
Iteration 221, loss = 7.04026141
Iteration 222, loss = 6.87167130
Iteration 223, loss = 6.98661020
Iteration 224, loss = 6.92811805
Iteration 225, loss = 6.97235557
Iteration 226, loss = 6.91231338
Iteration 227, loss = 7.03059587
Iteration 228, loss = 6.95866982
Iteration 229, loss = 6.89409648
Iteration 230, loss = 6.89136735
Iteration 231, loss = 6.90540722
Iteration 232, loss = 6.91429332
Iteration 233, loss = 6.90711654
Iteration 234, loss = 6.97517418
Iteration 235, loss = 6.90805631
Iteration 236, loss = 6.98498335
Iteration 237, loss = 6.98164086
Iteration 238, loss = 7.05391024
Iteration 239, loss = 7.44914239
Iteration 240, loss = 7.12739845
Iteration 241, loss = 6.90060682
Iteration 242, loss = 6.93730033
Iteration 243, loss = 6.91190003
Iteration 244, loss = 6.92735065
Iteration 245, loss = 6.88428840
Iteration 246, loss = 6.94473937
Iteration 247, loss = 6.94304943
Iteration 248, loss = 6.91232944
Iteration 249, loss = 6.93925698
Iteration 250, loss = 6.99521137
Iteration 251, loss = 7.13775428
Iteration 252, loss = 7.08429058
Iteration 253, loss = 7.18680404
Iteration 254, loss = 7.10433592
Iteration 255, loss = 7.17189701
Iteration 256, loss = 7.02841826
Iteration 257, loss = 6.99191345
Iteration 258, loss = 6.90951011
Iteration 259, loss = 6.93359310
Iteration 260, loss = 7.02718171
Iteration 261, loss = 7.09981676
Iteration 262, loss = 7.09661796
Iteration 263, loss = 7.42696656
Iteration 264, loss = 7.17797346
Iteration 265, loss = 6.98845070
Iteration 266, loss = 7.40733259
Iteration 267, loss = 7.47917271
Iteration 268, loss = 6.91814738
Iteration 269, loss = 6.88897166
Iteration 270, loss = 7.18080157
Iteration 271, loss = 6.91317161
Iteration 272, loss = 7.67615440
Iteration 273, loss = 8.14082369
Iteration 274, loss = 7.97489621
Iteration 275, loss = 7.44251384
Iteration 276, loss = 7.04214446
Iteration 277, loss = 6.99790944
Iteration 278, loss = 6.94100640
Iteration 279, loss = 6.94620971
Iteration 280, loss = 7.07267824
Iteration 281, loss = 6.96878301
Iteration 282, loss = 6.90846572
Iteration 283, loss = 7.02308361
Iteration 284, loss = 6.93427788
Iteration 285, loss = 7.04679827
Iteration 286, loss = 7.05606723
Iteration 287, loss = 6.98973166
Iteration 288, loss = 7.07030797
Iteration 289, loss = 7.28046318
Iteration 290, loss = 7.24845823
Iteration 291, loss = 6.94845476
Iteration 292, loss = 6.88580985
Iteration 293, loss = 7.03524837
Iteration 294, loss = 7.03987084
Iteration 295, loss = 7.00798946
Iteration 296, loss = 7.47697295
Iteration 297, loss = 7.10562087
Iteration 298, loss = 6.96183814
Iteration 299, loss = 6.96169199
Iteration 300, loss = 6.90181971
Iteration 301, loss = 6.89329882
Iteration 302, loss = 6.96254022
Iteration 303, loss = 6.96003086
Iteration 304, loss = 6.90972652
Iteration 305, loss = 6.94606391
Iteration 306, loss = 6.91448955
Iteration 307, loss = 7.04623844
Iteration 308, loss = 7.20326601
Iteration 309, loss = 7.54925552
Iteration 310, loss = 8.37185974
Iteration 311, loss = 8.53805196
Iteration 312, loss = 7.99376595
Iteration 313, loss = 7.03161658
Iteration 314, loss = 7.10247381
Iteration 315, loss = 7.23647377
Iteration 316, loss = 7.03278910
Iteration 317, loss = 7.14334596
Iteration 318, loss = 6.96611706
Iteration 319, loss = 7.12491597
Iteration 320, loss = 6.96666893
Iteration 321, loss = 6.88528290
Iteration 322, loss = 7.10113213
Iteration 323, loss = 6.90177699
Iteration 324, loss = 6.91785442
Iteration 325, loss = 6.97306926
Iteration 326, loss = 6.93218229
Iteration 327, loss = 6.97772626
Iteration 328, loss = 6.92488555
Iteration 329, loss = 6.99122224
Iteration 330, loss = 6.90759844
Iteration 331, loss = 6.97528392
Iteration 332, loss = 6.92706070
Iteration 333, loss = 6.90043188
Iteration 334, loss = 6.89991620
Iteration 335, loss = 6.87549174
Iteration 336, loss = 6.91016469
Iteration 337, loss = 6.89790234
Iteration 338, loss = 6.92991007
Iteration 339, loss = 6.98096296
Iteration 340, loss = 6.91199541
Iteration 341, loss = 6.86475100
Iteration 342, loss = 6.96936929
Iteration 343, loss = 6.88423188
Iteration 344, loss = 6.99511808
Iteration 345, loss = 6.90498627
Iteration 346, loss = 6.91441744
Iteration 347, loss = 6.95879342
Iteration 348, loss = 6.97862850
Iteration 349, loss = 7.03353101
Iteration 350, loss = 6.87876185
Iteration 351, loss = 6.86306234
Iteration 352, loss = 6.94965808
Iteration 353, loss = 6.88848676
Iteration 354, loss = 6.93967036
Iteration 355, loss = 6.98904703
Iteration 356, loss = 6.97793500
Iteration 357, loss = 6.84738304
Iteration 358, loss = 6.92100772
Iteration 359, loss = 7.14231368
Iteration 360, loss = 6.93946122
Iteration 361, loss = 6.86576980
Iteration 362, loss = 6.98071549
Iteration 363, loss = 6.92475116
Iteration 364, loss = 6.95669464
Iteration 365, loss = 6.90992899
Iteration 366, loss = 6.99322080
Iteration 367, loss = 6.90859697
Iteration 368, loss = 6.91365710
Iteration 369, loss = 6.96096192
Iteration 370, loss = 7.05388691
Iteration 371, loss = 6.88167603
Iteration 372, loss = 6.99689054
Iteration 373, loss = 6.86184819
Iteration 374, loss = 7.00901480
Iteration 375, loss = 6.87675724
Iteration 376, loss = 7.22013572
Iteration 377, loss = 7.30310492
Iteration 378, loss = 7.08440212
Iteration 379, loss = 6.92105380
Iteration 380, loss = 7.00570569
Iteration 381, loss = 7.12199134
Iteration 382, loss = 7.00981852
Iteration 383, loss = 6.97876572
Iteration 384, loss = 6.89600502
Iteration 385, loss = 6.87564200
Iteration 386, loss = 6.98312313
Iteration 387, loss = 6.90188965
Iteration 388, loss = 6.93181215
Iteration 389, loss = 6.98267595
Iteration 390, loss = 6.87085523
Iteration 391, loss = 6.90695686
Iteration 392, loss = 6.88551732
Iteration 393, loss = 6.96688743
Iteration 394, loss = 6.95366490
Iteration 395, loss = 6.97251332
Iteration 396, loss = 6.84013055
Iteration 397, loss = 6.85229628
Iteration 398, loss = 7.07081973
Iteration 399, loss = 7.03949343
Iteration 400, loss = 6.99196218
Iteration 401, loss = 6.86326735
Iteration 402, loss = 6.97523203
Iteration 403, loss = 6.91866687
Iteration 404, loss = 6.90044872
Iteration 405, loss = 6.84869056
Iteration 406, loss = 6.88682937
Iteration 407, loss = 6.94254891
Iteration 408, loss = 7.08173161
Iteration 409, loss = 7.12125802
Iteration 410, loss = 6.92645025
Iteration 411, loss = 6.97137208
Iteration 412, loss = 6.97323701
Iteration 413, loss = 6.89466648
Iteration 414, loss = 6.91450518
Iteration 415, loss = 6.91587048
Iteration 416, loss = 6.82591384
Iteration 417, loss = 6.91994303
Iteration 418, loss = 6.83599939
Iteration 419, loss = 6.86303756
Iteration 420, loss = 7.02777394
Iteration 421, loss = 6.87780671
Iteration 422, loss = 7.01094472
Iteration 423, loss = 7.07469860
Iteration 424, loss = 7.29488231
Iteration 425, loss = 7.11239597
Iteration 426, loss = 6.81844684
Iteration 427, loss = 7.02218349
Iteration 428, loss = 6.96069476
Iteration 429, loss = 6.83325660
Iteration 430, loss = 6.83379733
Iteration 431, loss = 6.89154469
Iteration 432, loss = 6.80691054
Iteration 433, loss = 6.82631690
Iteration 434, loss = 6.84428391
Iteration 435, loss = 6.83614081
Iteration 436, loss = 6.79224003
Iteration 437, loss = 6.79681420
Iteration 438, loss = 6.85652102
Iteration 439, loss = 6.86860103
Iteration 440, loss = 6.96028917
Iteration 441, loss = 6.96125274
Iteration 442, loss = 6.83420003
Iteration 443, loss = 6.92888233
Iteration 444, loss = 6.81021733
Iteration 445, loss = 6.80687178
Iteration 446, loss = 6.85349888
Iteration 447, loss = 6.82955753
Iteration 448, loss = 6.78193533
Iteration 449, loss = 6.76705675
Iteration 450, loss = 6.84889898
Iteration 451, loss = 6.81846762
Iteration 452, loss = 6.81267535
Iteration 453, loss = 6.77354167
Iteration 454, loss = 6.77191656
Iteration 455, loss = 6.79786813
Iteration 456, loss = 6.82634551
Iteration 457, loss = 6.74849142
Iteration 458, loss = 6.74381306
Iteration 459, loss = 6.76384425
Iteration 460, loss = 6.84021732
Iteration 461, loss = 6.80045037
Iteration 462, loss = 6.82211378
Iteration 463, loss = 6.78185320
Iteration 464, loss = 6.73678710
Iteration 465, loss = 6.92611061
Iteration 466, loss = 6.84473996
Iteration 467, loss = 6.73828116
Iteration 468, loss = 6.73769815
Iteration 469, loss = 6.76086149
Iteration 470, loss = 6.71769967
Iteration 471, loss = 6.74042510
Iteration 472, loss = 6.87316411
Iteration 473, loss = 6.85633030
Iteration 474, loss = 6.82046237
Iteration 475, loss = 6.76756980
Iteration 476, loss = 6.92176417
Iteration 477, loss = 6.92751616
Iteration 478, loss = 6.76427130
Iteration 479, loss = 6.83374363
Iteration 480, loss = 6.72457981
Iteration 481, loss = 6.82481523
Iteration 482, loss = 6.73943775
Iteration 483, loss = 6.76794399
Iteration 484, loss = 6.80229707
Iteration 485, loss = 6.70494221
Iteration 486, loss = 6.77802197
Iteration 487, loss = 6.72824458
Iteration 488, loss = 6.76929722
Iteration 489, loss = 6.78817876
Iteration 490, loss = 6.76001408
Iteration 491, loss = 6.72990632
Iteration 492, loss = 6.79082211
Iteration 493, loss = 6.79665839
Iteration 494, loss = 6.70521686
Iteration 495, loss = 6.85926151
Iteration 496, loss = 6.83433986
Iteration 497, loss = 6.90537786
Iteration 498, loss = 6.75971175
Iteration 499, loss = 6.74245451
Iteration 500, loss = 6.66596768
Iteration 501, loss = 6.75315082
Iteration 502, loss = 6.80771806
Iteration 503, loss = 6.74244430
Iteration 504, loss = 6.73849879
Iteration 505, loss = 6.68463239
Iteration 506, loss = 6.77234056
Iteration 507, loss = 6.67637718
Iteration 508, loss = 6.77983344
Iteration 509, loss = 6.71374860
Iteration 510, loss = 6.74922618
Iteration 511, loss = 6.88364319
Iteration 512, loss = 6.98332576
Iteration 513, loss = 7.02626966
Iteration 514, loss = 6.86848503
Iteration 515, loss = 6.82400300
Iteration 516, loss = 6.79359234
Iteration 517, loss = 6.81772398
Iteration 518, loss = 6.68272673
Iteration 519, loss = 6.67031596
Iteration 520, loss = 6.65350436
Iteration 521, loss = 6.66357177
Iteration 522, loss = 6.74305018
Iteration 523, loss = 6.67206019
Iteration 524, loss = 6.64918756
Iteration 525, loss = 6.69486581
Iteration 526, loss = 6.73899456
Iteration 527, loss = 6.74018440
Iteration 528, loss = 6.66200739
Iteration 529, loss = 6.67968354
Iteration 530, loss = 6.66007005
Iteration 531, loss = 6.67654031
Iteration 532, loss = 6.69179670
Iteration 533, loss = 6.76691719
Iteration 534, loss = 6.63498659
Iteration 535, loss = 6.69841720
Iteration 536, loss = 6.79690499
Iteration 537, loss = 6.66865394
Iteration 538, loss = 6.63792013
Iteration 539, loss = 6.64755642
Iteration 540, loss = 6.68217349
Iteration 541, loss = 6.77981755
Iteration 542, loss = 6.83005271
Iteration 543, loss = 7.05359457
Iteration 544, loss = 6.76777251
Iteration 545, loss = 6.76817761
Iteration 546, loss = 6.78649113
Iteration 547, loss = 6.70472207
Iteration 548, loss = 6.62183283
Iteration 549, loss = 6.67476602
Iteration 550, loss = 6.63793284
Iteration 551, loss = 6.67856516
Iteration 552, loss = 6.76483610
Iteration 553, loss = 6.73365598
Iteration 554, loss = 6.68280488
Iteration 555, loss = 6.60800233
Iteration 556, loss = 6.68231380
Iteration 557, loss = 6.74653131
Iteration 558, loss = 6.71618444
Iteration 559, loss = 6.64351362
Iteration 560, loss = 6.75582538
Iteration 561, loss = 6.60790145
Iteration 562, loss = 6.80056247
Iteration 563, loss = 6.67174557
Iteration 564, loss = 6.66646829
Iteration 565, loss = 6.73850126
Iteration 566, loss = 6.62238498
Iteration 567, loss = 6.76943899
Iteration 568, loss = 6.63824645
Iteration 569, loss = 6.67716823
Iteration 570, loss = 6.64028225
Iteration 571, loss = 6.81679277
Iteration 572, loss = 6.73382552
Iteration 573, loss = 6.64656366
Iteration 574, loss = 6.65392737
Iteration 575, loss = 6.69404113
Iteration 576, loss = 6.62029426
Iteration 577, loss = 6.61215068
Iteration 578, loss = 6.61945071
Iteration 579, loss = 6.60052294
Iteration 580, loss = 6.64592175
Iteration 581, loss = 6.72953232
Iteration 582, loss = 6.62649718
Iteration 583, loss = 6.62158947
Iteration 584, loss = 6.57494634
Iteration 585, loss = 6.64764436
Iteration 586, loss = 6.77775503
Iteration 587, loss = 6.90734120
Iteration 588, loss = 6.78564100
Iteration 589, loss = 6.60454730
Iteration 590, loss = 6.71793971
Iteration 591, loss = 6.73500855
Iteration 592, loss = 6.69742302
Iteration 593, loss = 6.74965125
Iteration 594, loss = 6.68085094
Iteration 595, loss = 6.66178404
Iteration 596, loss = 6.65543723
Iteration 597, loss = 6.68040282
Iteration 598, loss = 6.62519585
Iteration 599, loss = 6.86021074
Iteration 600, loss = 6.85095121
Iteration 601, loss = 6.66245607
Iteration 602, loss = 6.62881594
Iteration 603, loss = 7.17957662
Iteration 604, loss = 7.13350257
Iteration 605, loss = 6.88646433
Iteration 606, loss = 6.69552859
Iteration 607, loss = 6.80959383
Iteration 608, loss = 6.67846940
Iteration 609, loss = 6.70384386
Iteration 610, loss = 6.67678351
Iteration 611, loss = 6.60699731
Iteration 612, loss = 6.63084712
Iteration 613, loss = 6.59671062
Iteration 614, loss = 6.65896320
Iteration 615, loss = 6.65344223
Iteration 616, loss = 6.59300661
Iteration 617, loss = 6.64397766
Iteration 618, loss = 6.65787645
Iteration 619, loss = 6.58989395
Iteration 620, loss = 6.62070479
Iteration 621, loss = 6.61524410
Iteration 622, loss = 6.66113577
Iteration 623, loss = 6.62227367
Iteration 624, loss = 6.65812646
Iteration 625, loss = 6.68090164
Iteration 626, loss = 6.67878349
Iteration 627, loss = 6.73462206
Iteration 628, loss = 6.59045375
Iteration 629, loss = 6.66872724
Iteration 630, loss = 6.72517806
Iteration 631, loss = 6.85489689
Iteration 632, loss = 6.84587915
Iteration 633, loss = 6.58278270
Iteration 634, loss = 6.73670844
Iteration 635, loss = 6.68608094
Iteration 636, loss = 6.60676575
Iteration 637, loss = 6.67815312
Iteration 638, loss = 6.71263011
Iteration 639, loss = 6.65737807
Iteration 640, loss = 6.70917970
Iteration 641, loss = 6.78174516
Iteration 642, loss = 6.60599338
Iteration 643, loss = 6.84020371
Iteration 644, loss = 6.59829008
Iteration 645, loss = 6.60593564
Iteration 646, loss = 6.60466905
Iteration 647, loss = 6.57747356
Iteration 648, loss = 6.59230582
Iteration 649, loss = 6.58129888
Iteration 650, loss = 6.58513847
Iteration 651, loss = 6.57653268
Iteration 652, loss = 6.56939032
Iteration 653, loss = 6.61357995
Iteration 654, loss = 6.60090860
Iteration 655, loss = 6.57154411
Iteration 656, loss = 6.66198048
Iteration 657, loss = 6.55385521
Iteration 658, loss = 6.71929612
Iteration 659, loss = 6.73046363
Iteration 660, loss = 6.69579955
Iteration 661, loss = 6.58374219
Iteration 662, loss = 6.67384571
Iteration 663, loss = 6.81715593
Iteration 664, loss = 6.64489276
Iteration 665, loss = 6.55150984
Iteration 666, loss = 6.78408548
Iteration 667, loss = 6.63694560
Iteration 668, loss = 6.53234039
Iteration 669, loss = 6.68253470
Iteration 670, loss = 6.61338408
Iteration 671, loss = 6.61674710
Iteration 672, loss = 6.58111285
Iteration 673, loss = 6.56208884
Iteration 674, loss = 6.60656230
Iteration 675, loss = 6.57778393
Iteration 676, loss = 6.54369401
Iteration 677, loss = 6.65389363
Iteration 678, loss = 6.56138942
Iteration 679, loss = 6.59712954
Iteration 680, loss = 6.58693447
Iteration 681, loss = 6.66111362
Iteration 682, loss = 6.55321216
Iteration 683, loss = 6.64439749
Iteration 684, loss = 6.63705047
Iteration 685, loss = 6.82742353
Iteration 686, loss = 6.79512295
Iteration 687, loss = 6.78859112
Iteration 688, loss = 6.74286895
Iteration 689, loss = 6.70564519
Iteration 690, loss = 6.70478423
Iteration 691, loss = 6.68433454
Iteration 692, loss = 6.75394780
Iteration 693, loss = 6.59109094
Iteration 694, loss = 6.61859440
Iteration 695, loss = 6.70250678
Iteration 696, loss = 6.59336358
Iteration 697, loss = 6.63485985
Iteration 698, loss = 6.57541692
Iteration 699, loss = 6.64520639
Iteration 700, loss = 6.68148659
Iteration 701, loss = 6.62136586
Iteration 702, loss = 6.56644538
Iteration 703, loss = 6.63431798
Iteration 704, loss = 6.71907655
Iteration 705, loss = 6.68175848
Iteration 706, loss = 6.60670225
Iteration 707, loss = 6.62827029
Iteration 708, loss = 6.63267309
Iteration 709, loss = 6.57024984
Iteration 710, loss = 6.58235758
Iteration 711, loss = 6.57377579
Iteration 712, loss = 6.59428337
Iteration 713, loss = 6.56167404
Iteration 714, loss = 6.59052949
Iteration 715, loss = 6.56348047
Iteration 716, loss = 6.56415405
Iteration 717, loss = 6.56266620
Iteration 718, loss = 6.53906469
Iteration 719, loss = 6.56467921
Iteration 720, loss = 6.58622416
Iteration 721, loss = 6.53846422
Iteration 722, loss = 6.58364823
Iteration 723, loss = 6.56857478
Iteration 724, loss = 6.54917118
Iteration 725, loss = 6.62959093
Iteration 726, loss = 6.55984939
Iteration 727, loss = 6.58071278
Iteration 728, loss = 6.60331623
Iteration 729, loss = 6.59005037
Iteration 730, loss = 6.59585137
Iteration 731, loss = 6.56646113
Iteration 732, loss = 6.56343930
Iteration 733, loss = 6.55031490
Iteration 734, loss = 6.54327089
Iteration 735, loss = 6.57797893
Iteration 736, loss = 6.55743753
Iteration 737, loss = 6.52644490
Iteration 738, loss = 6.58012992
Iteration 739, loss = 6.81089261
Iteration 740, loss = 6.49961256
Iteration 741, loss = 6.72274804
Iteration 742, loss = 6.73808397
Iteration 743, loss = 6.52631265
Iteration 744, loss = 6.57653173
Iteration 745, loss = 6.72812067
Iteration 746, loss = 6.60879954
Iteration 747, loss = 6.61574580
Iteration 748, loss = 6.64150022
Iteration 749, loss = 6.55754351
Iteration 750, loss = 6.58227301
Iteration 751, loss = 6.54088763
Iteration 752, loss = 6.54852694
Iteration 753, loss = 6.55955745
Iteration 754, loss = 6.52446357
Iteration 755, loss = 6.55328071
Iteration 756, loss = 6.55069591
Iteration 757, loss = 6.59347500
Iteration 758, loss = 6.64414084
Iteration 759, loss = 6.76692547
Iteration 760, loss = 6.59130018
Iteration 761, loss = 6.60591330
Iteration 762, loss = 6.69959887
Iteration 763, loss = 6.52241355
Iteration 764, loss = 6.65207181
Iteration 765, loss = 6.59041918
Iteration 766, loss = 6.49083234
Iteration 767, loss = 6.76236692
Iteration 768, loss = 6.53091046
Iteration 769, loss = 6.54118371
Iteration 770, loss = 6.70811298
Iteration 771, loss = 6.63431883
Iteration 772, loss = 6.59961364
Iteration 773, loss = 6.64813824
Iteration 774, loss = 6.59236451
Iteration 775, loss = 6.51940817
Iteration 776, loss = 6.54678642
Iteration 777, loss = 6.53397200
Iteration 778, loss = 6.53149796
Iteration 779, loss = 6.52718679
Iteration 780, loss = 6.52165960
Iteration 781, loss = 6.55578471
Iteration 782, loss = 6.61021080
Iteration 783, loss = 6.54162549
Iteration 784, loss = 6.55856256
Iteration 785, loss = 6.52480141
Iteration 786, loss = 6.51799250
Iteration 787, loss = 6.54867678
Iteration 788, loss = 6.55651463
Iteration 789, loss = 6.64537814
Iteration 790, loss = 6.50737040
Iteration 791, loss = 6.62650428
Iteration 792, loss = 6.74828568
Iteration 793, loss = 6.59728338
Iteration 794, loss = 6.69804137
Iteration 795, loss = 6.66775968
Iteration 796, loss = 6.64238407
Iteration 797, loss = 6.55844992
Iteration 798, loss = 6.65651243
Iteration 799, loss = 6.61675686
Iteration 800, loss = 6.53740457
Iteration 801, loss = 6.59249773
Iteration 802, loss = 6.52733591
Iteration 803, loss = 6.54138870
Iteration 804, loss = 6.52312928
Iteration 805, loss = 6.57166928
Iteration 806, loss = 6.49270776
Iteration 807, loss = 6.49746869
Iteration 808, loss = 6.53167317
Iteration 809, loss = 6.54035570
Iteration 810, loss = 6.54662603
Iteration 811, loss = 6.55746823
Iteration 812, loss = 6.56998266
Iteration 813, loss = 6.51102341
Iteration 814, loss = 6.51141596
Iteration 815, loss = 6.58359913
Iteration 816, loss = 6.53002390
Iteration 817, loss = 6.48641048
Iteration 818, loss = 6.50492814
Iteration 819, loss = 6.51740114
Iteration 820, loss = 6.65474772
Iteration 821, loss = 6.51555857
Iteration 822, loss = 6.53683189
Iteration 823, loss = 6.61989526
Iteration 824, loss = 6.52486250
Iteration 825, loss = 6.59687006
Iteration 826, loss = 6.49575954
Iteration 827, loss = 6.56636733
Iteration 828, loss = 6.55029210
Iteration 829, loss = 6.54112707
Iteration 830, loss = 6.51388069
Iteration 831, loss = 6.51203770
Iteration 832, loss = 6.51674363
Iteration 833, loss = 6.51524873
Iteration 834, loss = 6.45989590
Iteration 835, loss = 6.54375560
Iteration 836, loss = 6.56692934
Iteration 837, loss = 6.49807401
Iteration 838, loss = 6.59619870
Iteration 839, loss = 6.56410867
Iteration 840, loss = 6.46824177
Iteration 841, loss = 6.46927206
Iteration 842, loss = 6.49114469
Iteration 843, loss = 6.44577475
Iteration 844, loss = 6.50408301
Iteration 845, loss = 6.52857014
Iteration 846, loss = 6.55004717
Iteration 847, loss = 6.44153261
Iteration 848, loss = 6.54365495
Iteration 849, loss = 6.59740941
Iteration 850, loss = 6.51452691
Iteration 851, loss = 6.57301247
Iteration 852, loss = 6.50967011
Iteration 853, loss = 6.45438120
Iteration 854, loss = 6.48410937
Iteration 855, loss = 6.53397732
Iteration 856, loss = 6.55642774
Iteration 857, loss = 6.45137579
Iteration 858, loss = 6.63166041
Iteration 859, loss = 6.62171319
Iteration 860, loss = 6.56008784
Iteration 861, loss = 6.58406597
Iteration 862, loss = 6.65721266
Iteration 863, loss = 6.43946592
Iteration 864, loss = 6.59733504
Iteration 865, loss = 6.51847321
Iteration 866, loss = 6.50640705
Iteration 867, loss = 6.52076191
Iteration 868, loss = 6.47503372
Iteration 869, loss = 6.53613105
Iteration 870, loss = 6.48349589
Iteration 871, loss = 6.44510114
Iteration 872, loss = 6.48482027
Iteration 873, loss = 6.44948882
Iteration 874, loss = 6.46518342
Iteration 875, loss = 6.44939054
Iteration 876, loss = 6.48610592
Iteration 877, loss = 6.46274145
Iteration 878, loss = 6.44661728
Iteration 879, loss = 6.46677522
Iteration 880, loss = 6.56978949
Iteration 881, loss = 6.43968909
Iteration 882, loss = 6.57712286
Iteration 883, loss = 6.49206089
Iteration 884, loss = 6.43434286
Iteration 885, loss = 6.47869147
Iteration 886, loss = 6.58132006
Iteration 887, loss = 6.40090286
Iteration 888, loss = 6.69497342
Iteration 889, loss = 6.46934164
Iteration 890, loss = 6.44060361
Iteration 891, loss = 6.52418333
Iteration 892, loss = 6.50942622
Iteration 893, loss = 6.45774212
Iteration 894, loss = 6.46810357
Iteration 895, loss = 6.44474444
Iteration 896, loss = 6.46421847
Iteration 897, loss = 6.46179167
Iteration 898, loss = 6.51355009
Iteration 899, loss = 6.63995770
Iteration 900, loss = 6.37635979
Iteration 901, loss = 6.70789172
Iteration 902, loss = 6.73195686
Iteration 903, loss = 6.47320637
Iteration 904, loss = 6.60078432
Iteration 905, loss = 6.52102696
Iteration 906, loss = 6.46387394
Iteration 907, loss = 6.50124736
Iteration 908, loss = 6.44958242
Iteration 909, loss = 6.47912245
Iteration 910, loss = 6.42627933
Iteration 911, loss = 6.47171940
Iteration 912, loss = 6.43894258
Iteration 913, loss = 6.45231778
Iteration 914, loss = 6.42307804
Iteration 915, loss = 6.44666524
Iteration 916, loss = 6.45790236
Iteration 917, loss = 6.42998575
Iteration 918, loss = 6.43188648
Iteration 919, loss = 6.48952146
Iteration 920, loss = 6.49841409
Iteration 921, loss = 6.49753862
Iteration 922, loss = 6.45383429
Iteration 923, loss = 6.50485925
Iteration 924, loss = 6.45492170
Iteration 925, loss = 6.46454062
Iteration 926, loss = 6.44318771
Iteration 927, loss = 6.45711347
Iteration 928, loss = 6.47592306
Iteration 929, loss = 6.57372837
Iteration 930, loss = 6.57907994
Iteration 931, loss = 6.53090844
Iteration 932, loss = 6.49468428
Iteration 933, loss = 6.55024462
Iteration 934, loss = 6.42237893
Iteration 935, loss = 6.52310312
Iteration 936, loss = 6.44218933
Iteration 937, loss = 6.47246755
Iteration 938, loss = 6.39441176
Iteration 939, loss = 6.47985105
Iteration 940, loss = 6.42962477
Iteration 941, loss = 6.42257779
Iteration 942, loss = 6.45257934
Iteration 943, loss = 6.40895313
Iteration 944, loss = 6.43943346
Iteration 945, loss = 6.46570423
Iteration 946, loss = 6.53028672
Iteration 947, loss = 6.57686607
Iteration 948, loss = 6.58334088
Iteration 949, loss = 6.51108562
Iteration 950, loss = 6.62437171
Iteration 951, loss = 6.50664663
Iteration 952, loss = 6.42989761
Iteration 953, loss = 6.46242028
Iteration 954, loss = 6.44766565
Iteration 955, loss = 6.44162939
Iteration 956, loss = 6.45667599
Iteration 957, loss = 6.43886366
Iteration 958, loss = 6.39123366
Iteration 959, loss = 6.42098278
Iteration 960, loss = 6.42184026
Iteration 961, loss = 6.38763470
Iteration 962, loss = 6.41647198
Iteration 963, loss = 6.39728231
Iteration 964, loss = 6.40223563
Iteration 965, loss = 6.56089309
Iteration 966, loss = 6.58443243
Iteration 967, loss = 6.57818396
Iteration 968, loss = 6.68727457
Iteration 969, loss = 6.67088428
Iteration 970, loss = 6.52368226
Iteration 971, loss = 6.56557209
Iteration 972, loss = 6.66912319
Iteration 973, loss = 6.49205727
Iteration 974, loss = 6.50514148
Iteration 975, loss = 6.50362928
Iteration 976, loss = 6.45792387
Iteration 977, loss = 6.47453921
Iteration 978, loss = 6.57915971
Iteration 979, loss = 6.45184837
Iteration 980, loss = 6.53883387
Iteration 981, loss = 6.58740695
Iteration 982, loss = 6.41607144
Iteration 983, loss = 6.48426903
Iteration 984, loss = 6.41853156
Iteration 985, loss = 6.42870263
Iteration 986, loss = 6.43823665
Iteration 987, loss = 6.41776521
Iteration 988, loss = 6.42048062
Iteration 989, loss = 6.41477773
Iteration 990, loss = 6.43053932
Iteration 991, loss = 6.38178979
Iteration 992, loss = 6.47584273
Iteration 993, loss = 6.41544685
Iteration 994, loss = 6.43471176
Iteration 995, loss = 6.39675642
Iteration 996, loss = 6.37383600
Iteration 997, loss = 6.40249012
Iteration 998, loss = 6.37571131
Iteration 999, loss = 6.36813344
Iteration 1000, loss = 6.34858965
*** fcst ***
[[ 0.72432973  1.37330034  0.57472393]
 [ 0.88828398  1.28997211  0.73240463]
 [ 1.05280124  1.27898235  0.92949701]
 [ 1.20839073  1.26799259  1.09958381]
 [ 1.09234226  1.25700283  0.92638823]
 [ 0.9762938   1.24601307  0.7915678 ]
 [ 0.86078896  1.2350233   0.75701834]
 [ 0.78952969  1.22403354  0.7342179 ]
 [ 0.75161391  1.15147839  0.71141745]
 [-0.62976473 -0.53051442 -0.64695108]
 [-0.46384133 -0.62532323 -0.60199019]
 [-0.29791793 -0.28355197 -0.55615406]
 [-0.13199453  0.33141432 -0.49686066]
 [ 0.03392887  0.90136122 -0.34689535]
 [ 0.19985227  1.31005603 -0.12655049]
 [ 0.34861249  1.70717218  0.25048525]
 [ 0.49473129  2.10246541  0.62752099]
 [ 0.6408501   2.4198958   1.00455673]
 [ 0.78793921  2.81077406  1.38159247]
 [ 0.95189345  3.21289649  1.7586282 ]
 [ 1.1158477   3.68126304  2.13566394]
 [ 1.27980194  4.1285231   2.46296554]
 [ 1.44375619  4.01798574  2.63269926]
 [ 1.60771044  3.21516211  2.80243299]
 [ 1.77166468  2.2702184   2.69581984]
 [ 1.69326617  1.83495043  2.44076829]
 [ 1.57422254  1.59486304  2.21997105]
 [ 1.45728697  1.50933465  2.04375694]
 [ 1.3412385   1.50947827  1.80734142]
 [ 1.22560463  1.50962188  1.66608764]
 [ 1.11012238  1.5097655   1.52480119]
 [ 0.99464012  1.50362707  1.37138502]
 [ 0.87915787  1.49263731  1.21796884]
 [ 0.28448696  0.94447579  1.04270232]
 [ 0.45395906  1.37553695  1.29299671]
 [ 0.62343115  1.78118142  1.54329111]
 [ 0.79290324  2.17465159  1.7935855 ]
 [ 0.97368704  2.56812176  2.09145457]
 [ 1.17821639  2.96159194  2.46849031]
 [ 1.42390511  3.52309023  2.84552605]
 [ 1.66959382  4.20099196  3.22256179]
 [ 1.91600091  4.81671526  3.59959752]
 [ 2.16834639  5.27437407  3.97663326]
 [ 2.42069188  5.51349674  4.353669  ]
 [ 2.67303737  5.73231206  4.73070474]
 [ 2.94461044  5.88331531  4.93836729]
 [ 2.82678686  5.65421929  4.87121211]
 [ 2.41997647  5.29054144  4.35317744]
 [ 2.06265616  4.46029896  3.55958724]
 [ 1.94361252  3.59812711  2.82276224]
 [ 1.82456889  2.6446175   2.40549602]
 [ 1.70618321  2.06558125  2.02894764]
 [ 1.59042031  1.81811516  1.77462059]
 [ 1.47493805  1.71685361  1.61571361]
 [ 1.3594558   1.71699722  1.46229743]
 [ 1.24397354  1.71714084  1.30888126]
 [ 1.12849129  1.71728445  1.15546508]
 [-3.98848083 -2.98185233 -1.36973039]
 [-3.89857949 -2.7584896  -1.34946238]
 [-3.81026196 -2.59754238 -1.29699765]
 [-3.72194442 -2.99497666 -1.24014552]
 [-3.58412329 -3.12401521 -1.10745198]
 [-3.10392066 -1.75773951 -0.94777692]
 [-2.36560897 -0.13415112 -0.78810186]
 [-1.62711707  0.57977347 -0.62842681]
 [-0.61335287  0.54526096 -0.46875175]
 [ 0.26737842  0.4957066  -0.30907669]
 [ 0.3151607   0.45095293 -0.14940163]
 [ 0.35473275  0.40695212  0.01027343]
 [ 0.39430479  0.36295132  0.16994849]
 [ 0.43387684  0.31895051  0.15867397]
 [ 0.47347234  0.27359581  0.15361676]
 [ 0.51309376  0.22736612  0.14855956]
 [ 0.55217412  0.18113642  0.14350236]
 [ 0.59114149  0.13490672  0.15222014]
 [ 0.63010886  0.21031857  0.16377821]
 [ 0.66909557  0.17714314  0.17533628]
 [ 0.70876196  0.09958477  0.15460175]
 [ 0.74842834  0.02202639 -0.0983048 ]
 [ 0.76005555 -0.05228077 -0.35173852]
 [ 0.76131279 -0.04321295 -0.54962159]
 [-2.83051375 -0.85601856 -1.27838025]
 [-2.45455327 -0.44995951 -1.22672062]
 [-1.78033228 -0.17876074 -1.16989332]
 [-0.99547981 -0.11927163 -1.11242253]
 [-0.03376728 -0.50479227 -1.01428968]
 [ 0.13471623 -1.15865589 -0.85524986]
 [ 0.19275576 -0.71761827 -0.69621004]
 [ 0.25079528  0.22070986 -0.53717022]
 [ 0.30883481  0.63879209 -0.3781304 ]
 [ 0.36687434  0.5894976  -0.21909058]
 [ 0.42491387  0.54401275 -0.06005076]
 [ 0.4829534   0.49897115  0.09898905]
 [ 0.54099293  0.45274146  0.25802887]
 [ 0.58930486  0.40651176  0.25645603]
 [ 0.6288769   0.36082792  0.2508412 ]
 [ 0.66844895  0.44353074  0.24522636]
 [ 0.70779659  0.52623355  0.23961153]
 [ 0.74709932  0.60893636  0.24415052]
 [ 0.78316751  0.58241086  0.25515096]
 [ 0.78442475  0.50485249  0.2661514 ]]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV   0.890785 2021-12-17 15:00:00          200
1    CroixChodes_SP_CM_FT003.F.PV    0.97022 2021-12-17 16:00:00          200
2    CroixChodes_SP_CM_FT003.F.PV   1.087094 2021-12-17 17:00:00          200
3    CroixChodes_SP_CM_FT003.F.PV   1.191989 2021-12-17 18:00:00          200
4    CroixChodes_SP_CM_FT003.F.PV   1.091911 2021-12-17 19:00:00          200
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.452402 2021-12-21 15:00:00          200
97   CroixChodes_SP_CM_FT003.F.PV   0.491214 2021-12-21 16:00:00          200
98   CroixChodes_SP_CM_FT003.F.PV   0.533395 2021-12-21 17:00:00          200
99   CroixChodes_SP_CM_FT003.F.PV   0.540243 2021-12-21 18:00:00          200
100  CroixChodes_SP_CM_FT003.F.PV   0.518476 2021-12-21 19:00:00          200

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT003.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV   0.890785 2021-12-17 15:00:00          200
1    CroixChodes_SP_CM_FT003.F.PV    0.97022 2021-12-17 16:00:00          200
2    CroixChodes_SP_CM_FT003.F.PV   1.087094 2021-12-17 17:00:00          200
3    CroixChodes_SP_CM_FT003.F.PV   1.191989 2021-12-17 18:00:00          200
4    CroixChodes_SP_CM_FT003.F.PV   1.091911 2021-12-17 19:00:00          200
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.452402 2021-12-21 15:00:00          200
97   CroixChodes_SP_CM_FT003.F.PV   0.491214 2021-12-21 16:00:00          200
98   CroixChodes_SP_CM_FT003.F.PV   0.533395 2021-12-21 17:00:00          200
99   CroixChodes_SP_CM_FT003.F.PV   0.540243 2021-12-21 18:00:00          200
100  CroixChodes_SP_CM_FT003.F.PV   0.518476 2021-12-21 19:00:00          200

[101 rows x 4 columns]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV    0.72433 2021-12-17 15:00:00          201
1    CroixChodes_SP_CM_FT003.F.PV   0.888284 2021-12-17 16:00:00          201
2    CroixChodes_SP_CM_FT003.F.PV   1.052801 2021-12-17 17:00:00          201
3    CroixChodes_SP_CM_FT003.F.PV   1.208391 2021-12-17 18:00:00          201
4    CroixChodes_SP_CM_FT003.F.PV   1.092342 2021-12-17 19:00:00          201
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.668449 2021-12-21 15:00:00          201
97   CroixChodes_SP_CM_FT003.F.PV   0.707797 2021-12-21 16:00:00          201
98   CroixChodes_SP_CM_FT003.F.PV   0.747099 2021-12-21 17:00:00          201
99   CroixChodes_SP_CM_FT003.F.PV   0.783168 2021-12-21 18:00:00          201
100  CroixChodes_SP_CM_FT003.F.PV   0.784425 2021-12-21 19:00:00          201

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT003.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV    0.72433 2021-12-17 15:00:00          201
1    CroixChodes_SP_CM_FT003.F.PV   0.888284 2021-12-17 16:00:00          201
2    CroixChodes_SP_CM_FT003.F.PV   1.052801 2021-12-17 17:00:00          201
3    CroixChodes_SP_CM_FT003.F.PV   1.208391 2021-12-17 18:00:00          201
4    CroixChodes_SP_CM_FT003.F.PV   1.092342 2021-12-17 19:00:00          201
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.668449 2021-12-21 15:00:00          201
97   CroixChodes_SP_CM_FT003.F.PV   0.707797 2021-12-21 16:00:00          201
98   CroixChodes_SP_CM_FT003.F.PV   0.747099 2021-12-21 17:00:00          201
99   CroixChodes_SP_CM_FT003.F.PV   0.783168 2021-12-21 18:00:00          201
100  CroixChodes_SP_CM_FT003.F.PV   0.784425 2021-12-21 19:00:00          201

[101 rows x 4 columns]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV     1.3733 2021-12-17 15:00:00          202
1    CroixChodes_SP_CM_FT003.F.PV   1.289972 2021-12-17 16:00:00          202
2    CroixChodes_SP_CM_FT003.F.PV   1.278982 2021-12-17 17:00:00          202
3    CroixChodes_SP_CM_FT003.F.PV   1.267993 2021-12-17 18:00:00          202
4    CroixChodes_SP_CM_FT003.F.PV   1.257003 2021-12-17 19:00:00          202
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.443531 2021-12-21 15:00:00          202
97   CroixChodes_SP_CM_FT003.F.PV   0.526234 2021-12-21 16:00:00          202
98   CroixChodes_SP_CM_FT003.F.PV   0.608936 2021-12-21 17:00:00          202
99   CroixChodes_SP_CM_FT003.F.PV   0.582411 2021-12-21 18:00:00          202
100  CroixChodes_SP_CM_FT003.F.PV   0.504852 2021-12-21 19:00:00          202

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT003.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV     1.3733 2021-12-17 15:00:00          202
1    CroixChodes_SP_CM_FT003.F.PV   1.289972 2021-12-17 16:00:00          202
2    CroixChodes_SP_CM_FT003.F.PV   1.278982 2021-12-17 17:00:00          202
3    CroixChodes_SP_CM_FT003.F.PV   1.267993 2021-12-17 18:00:00          202
4    CroixChodes_SP_CM_FT003.F.PV   1.257003 2021-12-17 19:00:00          202
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.443531 2021-12-21 15:00:00          202
97   CroixChodes_SP_CM_FT003.F.PV   0.526234 2021-12-21 16:00:00          202
98   CroixChodes_SP_CM_FT003.F.PV   0.608936 2021-12-21 17:00:00          202
99   CroixChodes_SP_CM_FT003.F.PV   0.582411 2021-12-21 18:00:00          202
100  CroixChodes_SP_CM_FT003.F.PV   0.504852 2021-12-21 19:00:00          202

[101 rows x 4 columns]
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV   0.574724 2021-12-17 15:00:00          203
1    CroixChodes_SP_CM_FT003.F.PV   0.732405 2021-12-17 16:00:00          203
2    CroixChodes_SP_CM_FT003.F.PV   0.929497 2021-12-17 17:00:00          203
3    CroixChodes_SP_CM_FT003.F.PV   1.099584 2021-12-17 18:00:00          203
4    CroixChodes_SP_CM_FT003.F.PV   0.926388 2021-12-17 19:00:00          203
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.245226 2021-12-21 15:00:00          203
97   CroixChodes_SP_CM_FT003.F.PV   0.239612 2021-12-21 16:00:00          203
98   CroixChodes_SP_CM_FT003.F.PV   0.244151 2021-12-21 17:00:00          203
99   CroixChodes_SP_CM_FT003.F.PV   0.255151 2021-12-21 18:00:00          203
100  CroixChodes_SP_CM_FT003.F.PV   0.266151 2021-12-21 19:00:00          203

[101 rows x 4 columns]
WRITE TO DATABASE FOR CroixChodes_SP_CM_FT003.F.PV*** df_out ***
                          scadaid scadavalue       scadadatetime scadaquality
0    CroixChodes_SP_CM_FT003.F.PV   0.574724 2021-12-17 15:00:00          203
1    CroixChodes_SP_CM_FT003.F.PV   0.732405 2021-12-17 16:00:00          203
2    CroixChodes_SP_CM_FT003.F.PV   0.929497 2021-12-17 17:00:00          203
3    CroixChodes_SP_CM_FT003.F.PV   1.099584 2021-12-17 18:00:00          203
4    CroixChodes_SP_CM_FT003.F.PV   0.926388 2021-12-17 19:00:00          203
..                            ...        ...                 ...          ...
96   CroixChodes_SP_CM_FT003.F.PV   0.245226 2021-12-21 15:00:00          203
97   CroixChodes_SP_CM_FT003.F.PV   0.239612 2021-12-21 16:00:00          203
98   CroixChodes_SP_CM_FT003.F.PV   0.244151 2021-12-21 17:00:00          203
99   CroixChodes_SP_CM_FT003.F.PV   0.255151 2021-12-21 18:00:00          203
100  CroixChodes_SP_CM_FT003.F.PV   0.266151 2021-12-21 19:00:00          203

[101 rows x 4 columns]


6 cpt.inputdata MALMEDY_CM_FT005.F.PV cpt.ao 60.0 1440.0 30240.0 MLPDYNAMIC
DELETE FROM cpt.ao WHERE scadaid='MALMEDY_CM_FT005.F.PV'... done
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT005.F.PV' ORDER BY scadadatetime DESC... done
*** ldate, fdate ***
2021-12-17 14:15:00 2021-11-18 19:30:00
>>> Water demand prediction : TOF 2021-12-20 18:00:00
*** start_train, stop_train, start_pred, stop_pred, toftime ***
2021-11-26 15:00:00 2021-12-17 15:00:00 2021-12-17 15:00:00 2021-12-21 19:00:00 2021-12-20 18:00:00
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT005.F.PV' ORDER BY scadadatetime ASC, rnum ASC... done
*** df_in ***
        rnum       scadadatetime                scadaid  scadavalue scadaquality
0     333878 2021-11-18 19:30:00  MALMEDY_CM_FT005.F.PV       0.303         None
1     333904 2021-11-18 20:00:00  MALMEDY_CM_FT005.F.PV       0.693         None
2     333922 2021-11-18 20:00:00  MALMEDY_CM_FT005.F.PV       0.693         None
3     333944 2021-11-18 20:15:00  MALMEDY_CM_FT005.F.PV       0.693         None
4     333962 2021-11-18 20:15:00  MALMEDY_CM_FT005.F.PV       0.693         None
...      ...                 ...                    ...         ...          ...
6358  498789 2021-12-17 14:00:00  MALMEDY_CM_FT005.F.PV       0.283         None
6359  498796 2021-12-17 14:00:00  MALMEDY_CM_FT005.F.PV       0.283         None
6360  498830 2021-12-17 14:00:00  MALMEDY_CM_FT005.F.PV       0.283         None
6361  498852 2021-12-17 14:15:00  MALMEDY_CM_FT005.F.PV       0.283         None
6362  498871 2021-12-17 14:15:00  MALMEDY_CM_FT005.F.PV       0.283         None

[6363 rows x 5 columns]
*** X_train, y_train, df_X, df_y, X_pred ***
[[330. 330. 330. ... 351. 351. 351.]
 [  5.   5.   5. ...   5.   5.   5.]
 [ 15.  16.  17. ...  12.  13.  14.]] [2.28000000e-01 2.25333333e-01 4.42000000e-01 6.74500000e-01
 7.47000000e-01 7.47666667e-01 6.62000000e-01 5.76000000e-01
 6.77000000e-01 1.90000000e-01 1.74000000e-01 1.58000000e-01
 7.33333333e-02 2.80000000e-02 1.30000000e-02 1.52000000e-01
 1.22000000e-01 2.46166667e-01 9.42000000e-01 9.42000000e-01
 1.26900000e+00 1.48150000e+00 9.09000000e-01 5.60333333e-01
 4.65500000e-01 5.45000000e-01 6.27500000e-01 7.10000000e-01
 7.92500000e-01 8.75000000e-01 9.57500000e-01 1.04000000e+00
 1.12250000e+00 1.20500000e+00 1.28750000e+00 1.37000000e+00
 1.45250000e+00 1.53500000e+00 1.61750000e+00 1.70000000e+00
 1.78250000e+00 1.86500000e+00 1.94750000e+00 2.03000000e+00
 1.60600000e+00 1.34116667e+00 1.07700000e+00 6.16000000e-01
 4.79500000e-01 3.90833333e-01 6.30000000e-01 4.29000000e-01
 5.53000000e-01 6.91333333e-01 7.63000000e-01 4.39000000e-01
 2.21500000e-01 1.50000000e-01 1.34000000e-01 1.70000000e-02
 1.70000000e-02 1.50000000e-02 1.50000000e-02 4.10000000e-02
 3.59666667e-01 5.37833333e-01 6.32000000e-01 3.74000000e-01
 3.38000000e-01 3.01333333e-01 2.98000000e-01 3.04666667e-01
 2.09500000e-01 1.11000000e-01 2.54000000e-01 1.48000000e-01
 3.01500000e-01 5.08000000e-01 1.61000000e-01 3.53000000e-01
 2.64500000e-01 8.00000000e-02 7.80000000e-02 1.09333333e-01
 5.63333333e-02 1.75000000e-02 1.30000000e-02 3.80000000e-02
 2.98666667e-01 4.02333333e-01 2.69000000e-01 2.64000000e-01
 3.66500000e-01 4.69000000e-01 2.34000000e-01 1.70500000e-01
 1.07000000e-01 1.53000000e-01 2.10500000e-01 2.68000000e-01
 1.05200000e+00 6.54666667e-01 4.47500000e-01 4.05000000e-01
 3.73000000e-01 2.39000000e-01 1.21000000e-01 3.50000000e-02
 2.96666667e-02 2.70000000e-02 2.70000000e-02 1.14333333e-01
 2.25833333e-01 5.65000000e-01 4.10333333e-01 5.94500000e-01
 8.56000000e-01 3.11000000e-01 5.27500000e-01 7.44000000e-01
 3.46000000e-01 3.61000000e-01 3.76000000e-01 3.10000000e-01
 3.66000000e-01 4.22000000e-01 1.08000000e-01 3.90000000e-01
 3.10500000e-01 9.00000000e-02 5.10000000e-02 3.56666667e-02
 2.40000000e-02 8.45000000e-02 1.47000000e-01 1.20000000e-01
 4.80000000e-01 6.89500000e-01 7.19000000e-01 7.40000000e-01
 7.77333333e-01 7.04333333e-01 2.46000000e-01 2.18000000e-01
 1.82000000e-01 1.69833333e-01 1.99000000e-01 5.26000000e-01
 4.69500000e-01 4.00666667e-01 3.39000000e-01 6.70000000e-02
 1.03500000e-01 1.25500000e-01 5.30000000e-02 5.05000000e-02
 4.80000000e-02 4.80000000e-02 5.45000000e-02 6.10000000e-02
 4.93000000e-01 6.54333333e-01 3.81000000e-01 6.47000000e-01
 3.91500000e-01 1.38166667e-01 1.49000000e-01 1.19666667e-01
 4.66000000e-01 7.17833333e-01 1.72000000e-01 2.58666667e-01
 3.56500000e-01 4.11000000e-01 5.42000000e-01 2.56666667e-01
 2.99000000e-01 4.51833333e-01 2.91000000e-01 1.19666667e-01
 6.00000000e-02 5.65000000e-02 4.71666667e-02 8.30000000e-02
 2.04333333e-01 4.28000000e-01 6.41000000e-01 8.91000000e-01
 7.51000000e-01 6.95000000e-01 6.46666667e-01 3.35000000e-01
 1.10633333e+00 1.02400000e+00 5.98166667e-01 8.09000000e-01
 4.07000000e-01 4.39000000e-01 4.88500000e-01 5.76000000e-01
 3.63000000e-01 2.89000000e-01 1.78500000e-01 6.80000000e-02
 5.26666667e-02 1.15500000e-01 1.66666667e-01 7.00000000e-02
 2.48666667e-01 4.06000000e-01 6.87333333e-01 1.75400000e+00
 1.02700000e+00 7.52000000e-01 5.05500000e-01 6.48000000e-01
 3.15000000e-01 4.28500000e-01 5.12500000e-01 3.65000000e-01
 2.69000000e-01 2.52500000e-01 2.39833333e-01 2.59000000e-01
 1.65000000e-01 7.50000000e-02 5.63333333e-02 4.70000000e-02
 3.03333333e-02 2.20000000e-02 2.67000000e-01 2.67000000e-01
 3.88333333e-01 4.08500000e-01 3.97500000e-01 5.45000000e-01
 4.30333333e-01 3.25500000e-01 2.78000000e-01 3.14000000e-01
 3.55333333e-01 3.33000000e-01 1.18000000e-01 2.28000000e-01
 4.40666667e-01 4.96333333e-01 2.43000000e-01 3.07000000e-01
 1.25000000e-01 3.80000000e-02 4.00000000e-02 2.15000000e-02
 3.00000000e-03 3.85000000e-02 7.40000000e-02 7.60000000e-02
 3.14000000e-01 5.52000000e-01 6.71000000e-01 2.33000000e-01
 5.96333333e-01 7.00833333e-01 3.15000000e-01 1.79000000e-01
 1.35333333e-01 2.57000000e-01 1.92000000e-01 2.14000000e-01
 2.36000000e-01 8.19000000e-01 9.77500000e-01 9.74000000e-01
 1.64000000e-01 6.40000000e-02 3.85000000e-02 1.30000000e-02
 1.30000000e-02 1.00000000e-03 1.00000000e-03 4.83333333e-02
 3.83500000e-01 6.95000000e-01 1.76000000e-01 1.94000000e-01
 2.12000000e-01 1.28000000e-01 4.61000000e-01 7.94000000e-01
 4.42000000e-01 2.47333333e-01 1.74166667e-01 2.95000000e-01
 5.73666667e-01 6.42166667e-01 2.88000000e-01 6.59333333e-01
 5.22500000e-01 2.00000000e-01 2.00000000e-02 2.25000000e-02
 2.50000000e-02 1.90000000e-02 1.90000000e-02 1.89000000e-01
 5.51500000e-01 8.29000000e-01 4.18000000e-01 3.53000000e-01
 2.88000000e-01 7.08000000e-01 6.15000000e-01 1.50000000e-01
 6.40000000e-02 4.10000000e-02 6.10000000e-02 5.33000000e-01
 5.31711806e-01 5.30423611e-01 5.29135417e-01 5.27847222e-01
 5.26559028e-01 5.25270833e-01 5.23982639e-01 5.22694444e-01
 5.21406250e-01 5.20118056e-01 5.18829861e-01 5.17541667e-01
 5.16253472e-01 5.14965278e-01 5.13677083e-01 5.12388889e-01
 5.11100694e-01 5.09812500e-01 5.08524306e-01 5.07236111e-01
 5.05947917e-01 5.04659722e-01 5.03371528e-01 5.02083333e-01
 5.00795139e-01 4.99506944e-01 4.98218750e-01 4.96930556e-01
 4.95642361e-01 4.94354167e-01 4.93065972e-01 4.91777778e-01
 4.90489583e-01 4.89201389e-01 4.87913194e-01 4.86625000e-01
 4.85336806e-01 4.84048611e-01 4.82760417e-01 4.81472222e-01
 4.80184028e-01 4.78895833e-01 4.77607639e-01 4.76319444e-01
 4.75031250e-01 4.73743056e-01 4.72454861e-01 4.71166667e-01
 4.69878472e-01 4.68590278e-01 4.67302083e-01 4.66013889e-01
 4.64725694e-01 4.63437500e-01 4.62149306e-01 4.60861111e-01
 4.59572917e-01 4.58284722e-01 4.56996528e-01 4.55708333e-01
 4.54420139e-01 4.53131944e-01 4.51843750e-01 4.50555556e-01
 4.49267361e-01 4.47979167e-01 4.46690972e-01 4.45402778e-01
 4.44114583e-01 4.42826389e-01 4.41538194e-01 4.40250000e-01
 4.44000000e-01 1.40800000e+00 9.11500000e-01 4.18666667e-01
 3.82000000e-01 1.07000000e-01 8.10000000e-02 6.80000000e-02
 8.35000000e-02 9.90000000e-02 9.08333333e-02 5.00000000e-02
 6.32407407e-02 7.64814815e-02 8.97222222e-02 1.02962963e-01
 1.16203704e-01 1.29444444e-01 1.42685185e-01 1.55925926e-01
 1.69166667e-01 1.82407407e-01 1.95648148e-01 2.08888889e-01
 2.22129630e-01 2.35370370e-01 2.48611111e-01 2.61851852e-01
 2.75092593e-01 2.88333333e-01 3.01574074e-01 3.14814815e-01
 3.28055556e-01 3.41296296e-01 3.54537037e-01 3.67777778e-01
 3.81018519e-01 3.94259259e-01 4.07500000e-01 4.20740741e-01
 4.33981481e-01 4.47222222e-01 4.60462963e-01 4.73703704e-01
 4.86944444e-01 5.00185185e-01 5.13425926e-01 5.26666667e-01
 4.35000000e-01 1.85000000e-01 2.05000000e-01 3.34333333e-01
 2.40500000e-01 7.78333333e-02 5.70000000e-02 2.83333333e-02
 4.60000000e-02 3.55000000e-02 1.65000000e-02 5.40000000e-02
 4.86000000e-01 5.22000000e-01 4.98333333e-01 2.00000000e-01
 8.60000000e-02 1.27666667e-01 1.69333333e-01 2.11000000e-01
 2.52666667e-01 2.94333333e-01 3.36000000e-01 3.77666667e-01
 4.19333333e-01 4.61000000e-01 4.61000000e-01 4.41235294e-01
 4.21470588e-01 4.01705882e-01 3.81941176e-01 3.62176471e-01
 3.42411765e-01 3.22647059e-01 3.02882353e-01 2.83117647e-01
 2.63352941e-01 2.43588235e-01 2.23823529e-01 2.04058824e-01
 1.84294118e-01 1.64529412e-01 1.44764706e-01 1.25000000e-01
 8.75000000e-02 5.00000000e-02 3.18000000e-01 2.40666667e-01
 2.51500000e-01 4.99000000e-01 4.41666667e-01 4.90000000e-01
 4.78166667e-01 3.40000000e-02 8.80000000e-02 5.30000000e-02
 1.80000000e-02 2.66666667e-02 3.53333333e-02 4.40000000e-02
 3.70500000e-01 6.97000000e-01 7.44000000e-01 8.90000000e-01
 1.03600000e+00 2.66000000e-01 2.74500000e-01 2.83000000e-01]                        DOY   WD    HR
2021-11-26 15:00:00  330.0  5.0  15.0
2021-11-26 16:00:00  330.0  5.0  16.0
2021-11-26 17:00:00  330.0  5.0  17.0
2021-11-26 18:00:00  330.0  5.0  18.0
2021-11-26 19:00:00  330.0  5.0  19.0
...                    ...  ...   ...
2021-12-21 15:00:00  355.0  2.0  15.0
2021-12-21 16:00:00  355.0  2.0  16.0
2021-12-21 17:00:00  355.0  2.0  17.0
2021-12-21 18:00:00  355.0  2.0  18.0
2021-12-21 19:00:00  355.0  2.0  19.0

[605 rows x 3 columns] 2021-11-26 15:00:00    0.228000
2021-11-26 16:00:00    0.225333
2021-11-26 17:00:00    0.442000
2021-11-26 18:00:00    0.674500
2021-11-26 19:00:00    0.747000
                         ...   
2021-12-21 15:00:00    0.000000
2021-12-21 16:00:00    0.000000
2021-12-21 17:00:00    0.000000
2021-12-21 18:00:00    0.000000
2021-12-21 19:00:00    0.000000
Freq: 60T, Name: scadavalue, Length: 605, dtype: float64 [[351. 351. 351. 351. 351. 351. 351. 351. 351. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354.
  354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 355. 355. 355.
  355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355.
  355. 355. 355.]
 [  5.   5.   5.   5.   5.   5.   5.   5.   5.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.]
 [ 15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.   3.   4.
    5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.
   19.  20.  21.  22.  23.   0.   1.   2.   3.   4.   5.   6.   7.   8.
    9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.
   23.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
   13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.
    3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.
   17.  18.  19.]]
Run 0
Iteration 1, loss = 1947.76173134
Iteration 2, loss = 689.67148340
Iteration 3, loss = 178.93854388
Iteration 4, loss = 68.79015037
Iteration 5, loss = 66.57741736
Iteration 6, loss = 28.07698756
Iteration 7, loss = 15.79732950
Iteration 8, loss = 21.30425762
Iteration 9, loss = 1.35295541
Iteration 10, loss = 4.13086291
Iteration 11, loss = 1.55187491
Iteration 12, loss = 0.74315085
Iteration 13, loss = 0.25354214
Iteration 14, loss = 0.36409208
Iteration 15, loss = 2.15765167
Iteration 16, loss = 1.60338724
Iteration 17, loss = 0.44395206
Iteration 18, loss = 0.25608515
Iteration 19, loss = 0.30022723
Iteration 20, loss = 0.13184510
Iteration 21, loss = 0.09066581
Iteration 22, loss = 0.06164216
Iteration 23, loss = 0.05358255
Iteration 24, loss = 0.04814794
Iteration 25, loss = 0.05392072
Iteration 26, loss = 0.05782234
Iteration 27, loss = 0.05777840
Iteration 28, loss = 0.05545179
Iteration 29, loss = 0.05229434
Iteration 30, loss = 0.04980012
Iteration 31, loss = 0.05085084
Iteration 32, loss = 0.05007626
Iteration 33, loss = 0.04730436
Iteration 34, loss = 0.04865977
Iteration 35, loss = 0.04747635
Iteration 36, loss = 0.04713707
Iteration 37, loss = 0.04702499
Iteration 38, loss = 0.04830406
Iteration 39, loss = 0.04780210
Iteration 40, loss = 0.04602667
Iteration 41, loss = 0.04783412
Iteration 42, loss = 0.04718062
Iteration 43, loss = 0.04630916
Iteration 44, loss = 0.04621834
Iteration 45, loss = 0.04706328
Iteration 46, loss = 0.04621877
Iteration 47, loss = 0.04626582
Iteration 48, loss = 0.04592025
Iteration 49, loss = 0.04578187
Iteration 50, loss = 0.04577560
Iteration 51, loss = 0.04580600
Iteration 52, loss = 0.04587578
Iteration 53, loss = 0.04651167
Iteration 54, loss = 0.04561541
Iteration 55, loss = 0.04550358
Iteration 56, loss = 0.04544420
Iteration 57, loss = 0.04528565
Iteration 58, loss = 0.04604448
Iteration 59, loss = 0.04533014
Iteration 60, loss = 0.04536011
Iteration 61, loss = 0.04562650
Iteration 62, loss = 0.04551253
Iteration 63, loss = 0.04475089
Iteration 64, loss = 0.04457657
Iteration 65, loss = 0.04462896
Iteration 66, loss = 0.04481426
Iteration 67, loss = 0.04551194
Iteration 68, loss = 0.04413801
Iteration 69, loss = 0.04393996
Iteration 70, loss = 0.04465743
Iteration 71, loss = 0.04439061
Iteration 72, loss = 0.04727470
Iteration 73, loss = 0.04611934
Iteration 74, loss = 0.04329040
Iteration 75, loss = 0.04341223
Iteration 76, loss = 0.04289929
Iteration 77, loss = 0.04270456
Iteration 78, loss = 0.04305678
Iteration 79, loss = 0.04259761
Iteration 80, loss = 0.04247507
Iteration 81, loss = 0.04219570
Iteration 82, loss = 0.04192521
Iteration 83, loss = 0.04185104
Iteration 84, loss = 0.04196849
Iteration 85, loss = 0.04285142
Iteration 86, loss = 0.04135862
Iteration 87, loss = 0.04737660
Iteration 88, loss = 0.04178107
Iteration 89, loss = 0.04316658
Iteration 90, loss = 0.04401864
Iteration 91, loss = 0.04254186
Iteration 92, loss = 0.04177613
Iteration 93, loss = 0.04219290
Iteration 94, loss = 0.04282319
Iteration 95, loss = 0.04384890
Iteration 96, loss = 0.04301190
Iteration 97, loss = 0.04288741
Iteration 98, loss = 0.04167719
Iteration 99, loss = 0.04512282
Iteration 100, loss = 0.04308192
Iteration 101, loss = 0.04651409
Iteration 102, loss = 0.04423043
Iteration 103, loss = 0.04609235
Iteration 104, loss = 0.04359882
Iteration 105, loss = 0.04783699
Iteration 106, loss = 0.04411372
Iteration 107, loss = 0.04591087
Iteration 108, loss = 0.04340301
Iteration 109, loss = 0.04192326
Iteration 110, loss = 0.04340973
Iteration 111, loss = 0.04213946
Iteration 112, loss = 0.04206721
Iteration 113, loss = 0.04077306
Iteration 114, loss = 0.04325901
Iteration 115, loss = 0.04152162
Iteration 116, loss = 0.04982630
Iteration 117, loss = 0.04785553
Iteration 118, loss = 0.04452065
Iteration 119, loss = 0.05198717
Iteration 120, loss = 0.04269082
Iteration 121, loss = 0.04217462
Iteration 122, loss = 0.05683258
Iteration 123, loss = 0.05277155
Iteration 124, loss = 0.05008326
Iteration 125, loss = 0.05450022
Iteration 126, loss = 0.04573449
Iteration 127, loss = 0.04369785
Iteration 128, loss = 0.05100371
Iteration 129, loss = 0.04752336
Iteration 130, loss = 0.04837472
Iteration 131, loss = 0.04557406
Iteration 132, loss = 0.04587567
Iteration 133, loss = 0.04457934
Iteration 134, loss = 0.03911996
Iteration 135, loss = 0.04008118
Iteration 136, loss = 0.04218689
Iteration 137, loss = 0.04094288
Iteration 138, loss = 0.04167136
Iteration 139, loss = 0.04107040
Iteration 140, loss = 0.04218309
Iteration 141, loss = 0.04160637
Iteration 142, loss = 0.04117950
Iteration 143, loss = 0.04919831
Iteration 144, loss = 0.04659976
Iteration 145, loss = 0.04422973
Iteration 146, loss = 0.04200805
Iteration 147, loss = 0.04642600
Iteration 148, loss = 0.04295035
Iteration 149, loss = 0.04059000
Iteration 150, loss = 0.03997655
Iteration 151, loss = 0.04094193
Iteration 152, loss = 0.04114684
Iteration 153, loss = 0.04378481
Iteration 154, loss = 0.03921126
Iteration 155, loss = 0.03960034
Iteration 156, loss = 0.03927073
Iteration 157, loss = 0.04001799
Iteration 158, loss = 0.03858620
Iteration 159, loss = 0.04405224
Iteration 160, loss = 0.04035777
Iteration 161, loss = 0.04233759
Iteration 162, loss = 0.04927745
Iteration 163, loss = 0.04522942
Iteration 164, loss = 0.04684657
Iteration 165, loss = 0.04041523
Iteration 166, loss = 0.03987425
Iteration 167, loss = 0.04176421
Iteration 168, loss = 0.04184268
Iteration 169, loss = 0.03901775
Iteration 170, loss = 0.03868353
Iteration 171, loss = 0.03929734
Iteration 172, loss = 0.03836014
Iteration 173, loss = 0.04105265
Iteration 174, loss = 0.04177556
Iteration 175, loss = 0.04123335
Iteration 176, loss = 0.03876150
Iteration 177, loss = 0.03905321
Iteration 178, loss = 0.03947932
Iteration 179, loss = 0.03991323
Iteration 180, loss = 0.03950637
Iteration 181, loss = 0.03942239
Iteration 182, loss = 0.03982942
Iteration 183, loss = 0.03863407
Iteration 184, loss = 0.03992229
Iteration 185, loss = 0.03860184
Iteration 186, loss = 0.03839245
Iteration 187, loss = 0.03930965
Iteration 188, loss = 0.03854446
Iteration 189, loss = 0.03948011
Iteration 190, loss = 0.03931636
Iteration 191, loss = 0.03889858
Iteration 192, loss = 0.03879055
Iteration 193, loss = 0.04220960
Iteration 194, loss = 0.04060446
Iteration 195, loss = 0.04566795
Iteration 196, loss = 0.04526736
Iteration 197, loss = 0.04430739
Iteration 198, loss = 0.04056213
Iteration 199, loss = 0.04574790
Iteration 200, loss = 0.04049971
Iteration 201, loss = 0.04184747
Iteration 202, loss = 0.04725241
Iteration 203, loss = 0.04492304
Iteration 204, loss = 0.04101918
Iteration 205, loss = 0.04027501
Iteration 206, loss = 0.04212225
Iteration 207, loss = 0.04166973
Iteration 208, loss = 0.04235221
Iteration 209, loss = 0.04490204
Iteration 210, loss = 0.03934994
Iteration 211, loss = 0.04541447
Iteration 212, loss = 0.04371782
Iteration 213, loss = 0.03850376
Iteration 214, loss = 0.03938100
Iteration 215, loss = 0.03945708
Iteration 216, loss = 0.03947609
Iteration 217, loss = 0.03991281
Iteration 218, loss = 0.04071635
Iteration 219, loss = 0.04264362
Iteration 220, loss = 0.04229823
Iteration 221, loss = 0.04214497
Iteration 222, loss = 0.03965051
Iteration 223, loss = 0.04285333
Iteration 224, loss = 0.04200719
Iteration 225, loss = 0.04101401
Iteration 226, loss = 0.04068706
Iteration 227, loss = 0.04217459
Iteration 228, loss = 0.03849361
Iteration 229, loss = 0.03853972
Iteration 230, loss = 0.03934541
Iteration 231, loss = 0.03960276
Iteration 232, loss = 0.04044332
Iteration 233, loss = 0.03904930
Iteration 234, loss = 0.03970810
Iteration 235, loss = 0.03871611
Iteration 236, loss = 0.03814373
Iteration 237, loss = 0.03907253
Iteration 238, loss = 0.03936713
Iteration 239, loss = 0.03923546
Iteration 240, loss = 0.03902703
Iteration 241, loss = 0.04619164
Iteration 242, loss = 0.03900276
Iteration 243, loss = 0.03778949
Iteration 244, loss = 0.03981115
Iteration 245, loss = 0.04240585
Iteration 246, loss = 0.04254940
Iteration 247, loss = 0.04244968
Iteration 248, loss = 0.03875604
Iteration 249, loss = 0.03880580
Iteration 250, loss = 0.03918503
Iteration 251, loss = 0.03920299
Iteration 252, loss = 0.05288969
Iteration 253, loss = 0.04578713
Iteration 254, loss = 0.04773192
Iteration 255, loss = 0.04836271
Iteration 256, loss = 0.04382151
Iteration 257, loss = 0.04626122
Iteration 258, loss = 0.04055184
Iteration 259, loss = 0.04116812
Iteration 260, loss = 0.03819682
Iteration 261, loss = 0.03908037
Iteration 262, loss = 0.03788369
Iteration 263, loss = 0.03897506
Iteration 264, loss = 0.03845826
Iteration 265, loss = 0.04719359
Iteration 266, loss = 0.04262821
Iteration 267, loss = 0.04170813
Iteration 268, loss = 0.05310085
Iteration 269, loss = 0.05108154
Iteration 270, loss = 0.05601918
Iteration 271, loss = 0.05980503
Iteration 272, loss = 0.04776797
Iteration 273, loss = 0.04684713
Iteration 274, loss = 0.04049406
Iteration 275, loss = 0.04436054
Iteration 276, loss = 0.04679381
Iteration 277, loss = 0.05111519
Iteration 278, loss = 0.04643386
Iteration 279, loss = 0.04750875
Iteration 280, loss = 0.04605046
Iteration 281, loss = 0.03853356
Iteration 282, loss = 0.04017542
Iteration 283, loss = 0.04212573
Iteration 284, loss = 0.04176429
Iteration 285, loss = 0.04605340
Iteration 286, loss = 0.05575671
Iteration 287, loss = 0.05288594
Iteration 288, loss = 0.04440779
Iteration 289, loss = 0.04040384
Iteration 290, loss = 0.04004941
Iteration 291, loss = 0.03857233
Iteration 292, loss = 0.04624150
Iteration 293, loss = 0.04441302
Iteration 294, loss = 0.05526467
Iteration 295, loss = 0.04237197
Iteration 296, loss = 0.04042347
Iteration 297, loss = 0.03990112
Iteration 298, loss = 0.04015558
Iteration 299, loss = 0.04447423
Iteration 300, loss = 0.04921716
Iteration 301, loss = 0.06145319
Iteration 302, loss = 0.04933034
Iteration 303, loss = 0.04175586
Iteration 304, loss = 0.04728420
Iteration 305, loss = 0.03941872
Iteration 306, loss = 0.03744933
Iteration 307, loss = 0.04140103
Iteration 308, loss = 0.04123573
Iteration 309, loss = 0.04025465
Iteration 310, loss = 0.05368672
Iteration 311, loss = 0.05363536
Iteration 312, loss = 0.05006378
Iteration 313, loss = 0.04969171
Iteration 314, loss = 0.03981740
Iteration 315, loss = 0.03787515
Iteration 316, loss = 0.03960575
Iteration 317, loss = 0.04162173
Iteration 318, loss = 0.04150453
Iteration 319, loss = 0.03767269
Iteration 320, loss = 0.04443488
Iteration 321, loss = 0.04132178
Iteration 322, loss = 0.03823637
Iteration 323, loss = 0.03985356
Iteration 324, loss = 0.04303974
Iteration 325, loss = 0.05200654
Iteration 326, loss = 0.04820653
Iteration 327, loss = 0.04025048
Iteration 328, loss = 0.04619026
Iteration 329, loss = 0.03929314
Iteration 330, loss = 0.04012554
Iteration 331, loss = 0.03779756
Iteration 332, loss = 0.03849081
Iteration 333, loss = 0.03705189
Iteration 334, loss = 0.04038913
Iteration 335, loss = 0.03816554
Iteration 336, loss = 0.03834014
Iteration 337, loss = 0.03742317
Iteration 338, loss = 0.03738099
Iteration 339, loss = 0.03690520
Iteration 340, loss = 0.04072943
Iteration 341, loss = 0.03611770
Iteration 342, loss = 0.04054165
Iteration 343, loss = 0.03759425
Iteration 344, loss = 0.04327497
Iteration 345, loss = 0.03930468
Iteration 346, loss = 0.04482987
Iteration 347, loss = 0.04600198
Iteration 348, loss = 0.04599416
Iteration 349, loss = 0.04600329
Iteration 350, loss = 0.04253554
Iteration 351, loss = 0.04588161
Iteration 352, loss = 0.03993698
Iteration 353, loss = 0.03820643
Iteration 354, loss = 0.04352932
Iteration 355, loss = 0.04384587
Iteration 356, loss = 0.04574180
Iteration 357, loss = 0.04290191
Iteration 358, loss = 0.03972878
Iteration 359, loss = 0.03682360
Iteration 360, loss = 0.04112671
Iteration 361, loss = 0.03839451
Iteration 362, loss = 0.03917657
Iteration 363, loss = 0.03713375
Iteration 364, loss = 0.04099337
Iteration 365, loss = 0.03848737
Iteration 366, loss = 0.03753672
Iteration 367, loss = 0.03686006
Iteration 368, loss = 0.04257828
Iteration 369, loss = 0.04449201
Iteration 370, loss = 0.04591351
Iteration 371, loss = 0.04271499
Iteration 372, loss = 0.05939010
Iteration 373, loss = 0.05973505
Iteration 374, loss = 0.06115308
Iteration 375, loss = 0.08736987
Iteration 376, loss = 0.08411396
Iteration 377, loss = 0.05634112
Iteration 378, loss = 0.06473774
Iteration 379, loss = 0.05657513
Iteration 380, loss = 0.05560121
Iteration 381, loss = 0.04265303
Iteration 382, loss = 0.04542116
Iteration 383, loss = 0.04012701
Iteration 384, loss = 0.04660653
Iteration 385, loss = 0.04831086
Iteration 386, loss = 0.04352865
Iteration 387, loss = 0.04614675
Iteration 388, loss = 0.04717443
Iteration 389, loss = 0.04847465
Iteration 390, loss = 0.04298178
Iteration 391, loss = 0.04722344
Iteration 392, loss = 0.04946229
Iteration 393, loss = 0.03888597
Iteration 394, loss = 0.04402369
Iteration 395, loss = 0.04603944
Iteration 396, loss = 0.04044962
Iteration 397, loss = 0.03746563
Iteration 398, loss = 0.03858754
Iteration 399, loss = 0.03645372
Iteration 400, loss = 0.03675870
Iteration 401, loss = 0.03845691
Iteration 402, loss = 0.03731435
Iteration 403, loss = 0.04194771
Iteration 404, loss = 0.03639558
Iteration 405, loss = 0.03706015
Iteration 406, loss = 0.03645294
Iteration 407, loss = 0.04116049
Iteration 408, loss = 0.04089985
Iteration 409, loss = 0.03739435
Iteration 410, loss = 0.03845213
Iteration 411, loss = 0.03851052
Iteration 412, loss = 0.03933010
Iteration 413, loss = 0.03941236
Iteration 414, loss = 0.03659372
Iteration 415, loss = 0.04254582
Iteration 416, loss = 0.03715312
Iteration 417, loss = 0.04022166
Iteration 418, loss = 0.04708846
Iteration 419, loss = 0.04540277
Iteration 420, loss = 0.05131058
Iteration 421, loss = 0.04948618
Iteration 422, loss = 0.04269051
Iteration 423, loss = 0.07346246
Iteration 424, loss = 0.06630829
Iteration 425, loss = 0.05611011
Iteration 426, loss = 0.04237329
Iteration 427, loss = 0.03739847
Iteration 428, loss = 0.03855514
Iteration 429, loss = 0.03756787
Iteration 430, loss = 0.03963130
Iteration 431, loss = 0.03843612
Iteration 432, loss = 0.03762909
Iteration 433, loss = 0.03737611
Iteration 434, loss = 0.03854703
Iteration 435, loss = 0.04030712
Iteration 436, loss = 0.04666081
Iteration 437, loss = 0.04471878
Iteration 438, loss = 0.05049748
Iteration 439, loss = 0.05141504
Iteration 440, loss = 0.04521069
Iteration 441, loss = 0.03613987
Iteration 442, loss = 0.03812545
Iteration 443, loss = 0.03812720
Iteration 444, loss = 0.03816308
Iteration 445, loss = 0.04369384
Iteration 446, loss = 0.04243265
Iteration 447, loss = 0.04492604
Iteration 448, loss = 0.04345121
Iteration 449, loss = 0.04590240
Iteration 450, loss = 0.03634554
Iteration 451, loss = 0.03881360
Iteration 452, loss = 0.03906616
Iteration 453, loss = 0.03748879
Iteration 454, loss = 0.03610413
Iteration 455, loss = 0.03961384
Iteration 456, loss = 0.04907749
Iteration 457, loss = 0.04164501
Iteration 458, loss = 0.04506460
Iteration 459, loss = 0.03913337
Iteration 460, loss = 0.03853868
Iteration 461, loss = 0.04099959
Iteration 462, loss = 0.04953476
Iteration 463, loss = 0.05318195
Iteration 464, loss = 0.04398446
Iteration 465, loss = 0.04872917
Iteration 466, loss = 0.04573255
Iteration 467, loss = 0.04047441
Iteration 468, loss = 0.05274874
Iteration 469, loss = 0.03998179
Iteration 470, loss = 0.04156863
Iteration 471, loss = 0.04184667
Iteration 472, loss = 0.04222418
Iteration 473, loss = 0.04083413
Iteration 474, loss = 0.03604842
Iteration 475, loss = 0.03652272
Iteration 476, loss = 0.04622945
Iteration 477, loss = 0.04320172
Iteration 478, loss = 0.03878814
Iteration 479, loss = 0.04412841
Iteration 480, loss = 0.05948980
Iteration 481, loss = 0.06481066
Iteration 482, loss = 0.05288699
Iteration 483, loss = 0.04489455
Iteration 484, loss = 0.04255987
Iteration 485, loss = 0.05129283
Iteration 486, loss = 0.06261438
Iteration 487, loss = 0.06046113
Iteration 488, loss = 0.06462221
Iteration 489, loss = 0.07329935
Iteration 490, loss = 0.05728147
Iteration 491, loss = 0.05184817
Iteration 492, loss = 0.06257603
Iteration 493, loss = 0.06102817
Iteration 494, loss = 0.05135205
Iteration 495, loss = 0.05927466
Iteration 496, loss = 0.04387604
Iteration 497, loss = 0.03898356
Iteration 498, loss = 0.03770158
Iteration 499, loss = 0.04126078
Iteration 500, loss = 0.04073239
Iteration 501, loss = 0.04515090
Iteration 502, loss = 0.04677288
Iteration 503, loss = 0.04548657
Iteration 504, loss = 0.05695359
Iteration 505, loss = 0.04836541
Iteration 506, loss = 0.04642824
Iteration 507, loss = 0.04126764
Iteration 508, loss = 0.03633598
Iteration 509, loss = 0.03572664
Iteration 510, loss = 0.03823331
Iteration 511, loss = 0.03762175
Iteration 512, loss = 0.04265396
Iteration 513, loss = 0.04068214
Iteration 514, loss = 0.03905210
Iteration 515, loss = 0.04162774
Iteration 516, loss = 0.04098510
Iteration 517, loss = 0.04638218
Iteration 518, loss = 0.04987465
Iteration 519, loss = 0.03816575
Iteration 520, loss = 0.03947016
Iteration 521, loss = 0.04019131
Iteration 522, loss = 0.03769353
Iteration 523, loss = 0.03664585
Iteration 524, loss = 0.03865852
Iteration 525, loss = 0.03710486
Iteration 526, loss = 0.03641785
Iteration 527, loss = 0.04179609
Iteration 528, loss = 0.03972466
Iteration 529, loss = 0.03782643
Iteration 530, loss = 0.03684247
Iteration 531, loss = 0.03546478
Iteration 532, loss = 0.03576919
Iteration 533, loss = 0.03645215
Iteration 534, loss = 0.03725457
Iteration 535, loss = 0.03667492
Iteration 536, loss = 0.03577421
Iteration 537, loss = 0.04016347
Iteration 538, loss = 0.04322529
Iteration 539, loss = 0.04738535
Iteration 540, loss = 0.04471996
Iteration 541, loss = 0.04834029
Iteration 542, loss = 0.03995697
Iteration 543, loss = 0.03779482
Iteration 544, loss = 0.03708446
Iteration 545, loss = 0.03875560
Iteration 546, loss = 0.03698919
Iteration 547, loss = 0.04466486
Iteration 548, loss = 0.04856662
Iteration 549, loss = 0.03871753
Iteration 550, loss = 0.04533519
Iteration 551, loss = 0.04586430
Iteration 552, loss = 0.04340458
Iteration 553, loss = 0.04121382
Iteration 554, loss = 0.03647707
Iteration 555, loss = 0.04567697
Iteration 556, loss = 0.05696908
Iteration 557, loss = 0.06463783
Iteration 558, loss = 0.05911522
Iteration 559, loss = 0.04330659
Iteration 560, loss = 0.03729691
Iteration 561, loss = 0.03733990
Iteration 562, loss = 0.03830788
Iteration 563, loss = 0.03834580
Iteration 564, loss = 0.05054666
Iteration 565, loss = 0.04482943
Iteration 566, loss = 0.04816412
Iteration 567, loss = 0.04252299
Iteration 568, loss = 0.04218510
Iteration 569, loss = 0.04118879
Iteration 570, loss = 0.04116714
Iteration 571, loss = 0.04223697
Iteration 572, loss = 0.03918108
Iteration 573, loss = 0.03568009
Iteration 574, loss = 0.03610830
Iteration 575, loss = 0.03961981
Iteration 576, loss = 0.03894259
Iteration 577, loss = 0.03738256
Iteration 578, loss = 0.03741522
Iteration 579, loss = 0.03662849
Iteration 580, loss = 0.03562494
Iteration 581, loss = 0.03695565
Iteration 582, loss = 0.03688672
Iteration 583, loss = 0.04521485
Iteration 584, loss = 0.04730773
Iteration 585, loss = 0.04683258
Iteration 586, loss = 0.05120947
Iteration 587, loss = 0.06600041
Iteration 588, loss = 0.07190718
Iteration 589, loss = 0.06063109
Iteration 590, loss = 0.04572610
Iteration 591, loss = 0.04122204
Iteration 592, loss = 0.05093299
Iteration 593, loss = 0.05078453
Iteration 594, loss = 0.04900148
Iteration 595, loss = 0.04238245
Iteration 596, loss = 0.05835638
Iteration 597, loss = 0.04013910
Iteration 598, loss = 0.03745862
Iteration 599, loss = 0.03564974
Iteration 600, loss = 0.03589148
Iteration 601, loss = 0.04412651
Iteration 602, loss = 0.05459778
Iteration 603, loss = 0.06316000
Iteration 604, loss = 0.05097667
Iteration 605, loss = 0.04578735
Iteration 606, loss = 0.04129969
Iteration 607, loss = 0.04012896
Iteration 608, loss = 0.04108685
Iteration 609, loss = 0.04116628
Iteration 610, loss = 0.04766966
Iteration 611, loss = 0.04111550
Iteration 612, loss = 0.03622963
Iteration 613, loss = 0.03713434
Iteration 614, loss = 0.03720110
Iteration 615, loss = 0.03703246
Iteration 616, loss = 0.04418564
Iteration 617, loss = 0.03900353
Iteration 618, loss = 0.03927372
Iteration 619, loss = 0.03519125
Iteration 620, loss = 0.03516931
Iteration 621, loss = 0.03478794
Iteration 622, loss = 0.03461529
Iteration 623, loss = 0.04120014
Iteration 624, loss = 0.04432114
Iteration 625, loss = 0.03693055
Iteration 626, loss = 0.03587038
Iteration 627, loss = 0.03632598
Iteration 628, loss = 0.03887032
Iteration 629, loss = 0.04225870
Iteration 630, loss = 0.04464666
Iteration 631, loss = 0.03828440
Iteration 632, loss = 0.03676840
Iteration 633, loss = 0.03494735
Iteration 634, loss = 0.03600638
Iteration 635, loss = 0.04382234
Iteration 636, loss = 0.04872163
Iteration 637, loss = 0.03739284
Iteration 638, loss = 0.03455659
Iteration 639, loss = 0.03872701
Iteration 640, loss = 0.03930710
Iteration 641, loss = 0.04038709
Iteration 642, loss = 0.04433842
Iteration 643, loss = 0.03887553
Iteration 644, loss = 0.03594528
Iteration 645, loss = 0.03630410
Iteration 646, loss = 0.04121109
Iteration 647, loss = 0.03791358
Iteration 648, loss = 0.04229381
Iteration 649, loss = 0.05271294
Iteration 650, loss = 0.03909368
Iteration 651, loss = 0.03533409
Iteration 652, loss = 0.03501702
Iteration 653, loss = 0.04515481
Iteration 654, loss = 0.04688915
Iteration 655, loss = 0.04533560
Iteration 656, loss = 0.04147932
Iteration 657, loss = 0.04380185
Iteration 658, loss = 0.04821407
Iteration 659, loss = 0.04836939
Iteration 660, loss = 0.05917266
Iteration 661, loss = 0.04732706
Iteration 662, loss = 0.04605680
Iteration 663, loss = 0.04960296
Iteration 664, loss = 0.04487061
Iteration 665, loss = 0.04348950
Iteration 666, loss = 0.04851251
Iteration 667, loss = 0.04155794
Iteration 668, loss = 0.04179518
Iteration 669, loss = 0.04161468
Iteration 670, loss = 0.04376253
Iteration 671, loss = 0.04303034
Iteration 672, loss = 0.04717449
Iteration 673, loss = 0.03988794
Iteration 674, loss = 0.04338277
Iteration 675, loss = 0.04024337
Iteration 676, loss = 0.04228697
Iteration 677, loss = 0.04073849
Iteration 678, loss = 0.03985023
Iteration 679, loss = 0.03946487
Iteration 680, loss = 0.04223682
Iteration 681, loss = 0.04151139
Iteration 682, loss = 0.04062414
Iteration 683, loss = 0.04032107
Iteration 684, loss = 0.04128307
Iteration 685, loss = 0.04100585
Iteration 686, loss = 0.04260178
Iteration 687, loss = 0.04191231
Iteration 688, loss = 0.04039577
Iteration 689, loss = 0.03952835
Iteration 690, loss = 0.04779392
Iteration 691, loss = 0.04086830
Iteration 692, loss = 0.04266532
Iteration 693, loss = 0.05081169
Iteration 694, loss = 0.04884463
Iteration 695, loss = 0.04365044
Iteration 696, loss = 0.04573582
Iteration 697, loss = 0.04709656
Iteration 698, loss = 0.04062295
Iteration 699, loss = 0.03943783
Iteration 700, loss = 0.04108024
Iteration 701, loss = 0.04012606
Iteration 702, loss = 0.04324415
Iteration 703, loss = 0.05097669
Iteration 704, loss = 0.06100260
Iteration 705, loss = 0.05732298
Iteration 706, loss = 0.04731192
Iteration 707, loss = 0.07965070
Iteration 708, loss = 0.06715379
Iteration 709, loss = 0.05206231
Iteration 710, loss = 0.04726731
Iteration 711, loss = 0.04207651
Iteration 712, loss = 0.04520074
Iteration 713, loss = 0.04228026
Iteration 714, loss = 0.05339794
Iteration 715, loss = 0.05114974
Iteration 716, loss = 0.04596253
Iteration 717, loss = 0.04457191
Iteration 718, loss = 0.04858430
Iteration 719, loss = 0.04246429
Iteration 720, loss = 0.04149266
Iteration 721, loss = 0.04194207
Iteration 722, loss = 0.04254599
Iteration 723, loss = 0.04032313
Iteration 724, loss = 0.03975725
Iteration 725, loss = 0.04030308
Iteration 726, loss = 0.03982489
Iteration 727, loss = 0.04285566
Iteration 728, loss = 0.05516228
Iteration 729, loss = 0.04323815
Iteration 730, loss = 0.04198958
Iteration 731, loss = 0.04008974
Iteration 732, loss = 0.04056356
Iteration 733, loss = 0.04289072
Iteration 734, loss = 0.04392838
Iteration 735, loss = 0.04388399
Iteration 736, loss = 0.05276601
Iteration 737, loss = 0.06752098
Iteration 738, loss = 0.05455594
Iteration 739, loss = 0.06014134
Iteration 740, loss = 0.07556234
Iteration 741, loss = 0.05767404
Iteration 742, loss = 0.06947776
Iteration 743, loss = 0.06613959
Iteration 744, loss = 0.06892306
Iteration 745, loss = 0.05092557
Iteration 746, loss = 0.05190821
Iteration 747, loss = 0.04577935
Iteration 748, loss = 0.04975595
Iteration 749, loss = 0.04342329
Iteration 750, loss = 0.04267222
Iteration 751, loss = 0.03967434
Iteration 752, loss = 0.04022868
Iteration 753, loss = 0.04047556
Iteration 754, loss = 0.04775081
Iteration 755, loss = 0.05684600
Iteration 756, loss = 0.06040337
Iteration 757, loss = 0.04511480
Iteration 758, loss = 0.04535290
Iteration 759, loss = 0.04573184
Iteration 760, loss = 0.04540582
Iteration 761, loss = 0.04183015
Iteration 762, loss = 0.04639599
Iteration 763, loss = 0.04445031
Iteration 764, loss = 0.04085888
Iteration 765, loss = 0.03969364
Iteration 766, loss = 0.04052253
Iteration 767, loss = 0.04121154
Iteration 768, loss = 0.04226249
Iteration 769, loss = 0.04138696
Iteration 770, loss = 0.04082486
Iteration 771, loss = 0.03954318
Iteration 772, loss = 0.04018823
Iteration 773, loss = 0.03915561
Iteration 774, loss = 0.04102766
Iteration 775, loss = 0.04167440
Iteration 776, loss = 0.04367238
Iteration 777, loss = 0.04966815
Iteration 778, loss = 0.04886607
Iteration 779, loss = 0.04093157
Iteration 780, loss = 0.04102231
Iteration 781, loss = 0.04757443
Iteration 782, loss = 0.04203775
Iteration 783, loss = 0.04639713
Iteration 784, loss = 0.04414646
Iteration 785, loss = 0.04865193
Iteration 786, loss = 0.05188588
Iteration 787, loss = 0.05934444
Iteration 788, loss = 0.06032676
Iteration 789, loss = 0.05430521
Iteration 790, loss = 0.06386298
Iteration 791, loss = 0.05547487
Iteration 792, loss = 0.05402140
Iteration 793, loss = 0.04697027
Iteration 794, loss = 0.04541177
Iteration 795, loss = 0.04407885
Iteration 796, loss = 0.03991795
Iteration 797, loss = 0.04073765
Iteration 798, loss = 0.04119757
Iteration 799, loss = 0.04325106
Iteration 800, loss = 0.04046717
Iteration 801, loss = 0.04149364
Iteration 802, loss = 0.04282591
Iteration 803, loss = 0.03980099
Iteration 804, loss = 0.03990491
Iteration 805, loss = 0.04068840
Iteration 806, loss = 0.04132510
Iteration 807, loss = 0.03963272
Iteration 808, loss = 0.03965472
Iteration 809, loss = 0.04046576
Iteration 810, loss = 0.04166004
Iteration 811, loss = 0.04097402
Iteration 812, loss = 0.04578198
Iteration 813, loss = 0.04853708
Iteration 814, loss = 0.04994083
Iteration 815, loss = 0.04925696
Iteration 816, loss = 0.05301777
Iteration 817, loss = 0.04418597
Iteration 818, loss = 0.05176736
Iteration 819, loss = 0.05044973
Iteration 820, loss = 0.05197475
Iteration 821, loss = 0.05087848
Iteration 822, loss = 0.04450650
Iteration 823, loss = 0.04465168
Iteration 824, loss = 0.04014753
Iteration 825, loss = 0.04542961
Iteration 826, loss = 0.04077709
Iteration 827, loss = 0.04564551
Iteration 828, loss = 0.04589124
Iteration 829, loss = 0.04646995
Iteration 830, loss = 0.04195962
Iteration 831, loss = 0.04848054
Iteration 832, loss = 0.04582286
Iteration 833, loss = 0.04300206
Iteration 834, loss = 0.04441157
Iteration 835, loss = 0.04717659
Iteration 836, loss = 0.04266020
Iteration 837, loss = 0.04115818
Iteration 838, loss = 0.04007638
Iteration 839, loss = 0.03899766
Iteration 840, loss = 0.03920691
Iteration 841, loss = 0.03918092
Iteration 842, loss = 0.03922361
Iteration 843, loss = 0.03919363
Iteration 844, loss = 0.04649854
Iteration 845, loss = 0.04524290
Iteration 846, loss = 0.04213821
Iteration 847, loss = 0.04032684
Iteration 848, loss = 0.04108038
Iteration 849, loss = 0.04002100
Iteration 850, loss = 0.04184503
Iteration 851, loss = 0.04147234
Iteration 852, loss = 0.04031438
Iteration 853, loss = 0.04871190
Iteration 854, loss = 0.04814588
Iteration 855, loss = 0.04776307
Iteration 856, loss = 0.04279737
Iteration 857, loss = 0.04746063
Iteration 858, loss = 0.05010887
Iteration 859, loss = 0.05256457
Iteration 860, loss = 0.05064047
Iteration 861, loss = 0.05974595
Iteration 862, loss = 0.07209184
Iteration 863, loss = 0.06448531
Iteration 864, loss = 0.05971765
Iteration 865, loss = 0.06935536
Iteration 866, loss = 0.08679875
Iteration 867, loss = 0.06269729
Iteration 868, loss = 0.06825707
Iteration 869, loss = 0.07376665
Iteration 870, loss = 0.05581116
Iteration 871, loss = 0.05350206
Iteration 872, loss = 0.04840303
Iteration 873, loss = 0.05867003
Iteration 874, loss = 0.05718916
Iteration 875, loss = 0.04583324
Iteration 876, loss = 0.04397225
Iteration 877, loss = 0.04460893
Iteration 878, loss = 0.04252338
Iteration 879, loss = 0.04768524
Iteration 880, loss = 0.04691773
Iteration 881, loss = 0.04789187
Iteration 882, loss = 0.04182774
Iteration 883, loss = 0.04325757
Iteration 884, loss = 0.04308637
Iteration 885, loss = 0.04744551
Iteration 886, loss = 0.04353037
Iteration 887, loss = 0.04003772
Iteration 888, loss = 0.05047654
Iteration 889, loss = 0.04499591
Iteration 890, loss = 0.06484849
Iteration 891, loss = 0.05719140
Iteration 892, loss = 0.05606279
Iteration 893, loss = 0.04702137
Iteration 894, loss = 0.04261182
Iteration 895, loss = 0.05833143
Iteration 896, loss = 0.05007992
Iteration 897, loss = 0.04062272
Iteration 898, loss = 0.04242176
Iteration 899, loss = 0.04508587
Iteration 900, loss = 0.04391206
Iteration 901, loss = 0.04800387
Iteration 902, loss = 0.04376944
Iteration 903, loss = 0.04416154
Iteration 904, loss = 0.04794024
Iteration 905, loss = 0.04908119
Iteration 906, loss = 0.04642902
Iteration 907, loss = 0.03912857
Iteration 908, loss = 0.04246595
Iteration 909, loss = 0.04400745
Iteration 910, loss = 0.04316451
Iteration 911, loss = 0.04159896
Iteration 912, loss = 0.04887444
Iteration 913, loss = 0.04594292
Iteration 914, loss = 0.03975096
Iteration 915, loss = 0.04312071
Iteration 916, loss = 0.04327726
Iteration 917, loss = 0.03972401
Iteration 918, loss = 0.04227659
Iteration 919, loss = 0.03970473
Iteration 920, loss = 0.03945503
Iteration 921, loss = 0.04014196
Iteration 922, loss = 0.04157486
Iteration 923, loss = 0.03932706
Iteration 924, loss = 0.04341752
Iteration 925, loss = 0.05144656
Iteration 926, loss = 0.05128304
Iteration 927, loss = 0.04083205
Iteration 928, loss = 0.04160432
Iteration 929, loss = 0.03906889
Iteration 930, loss = 0.04045934
Iteration 931, loss = 0.03917415
Iteration 932, loss = 0.03892524
Iteration 933, loss = 0.03945064
Iteration 934, loss = 0.04104327
Iteration 935, loss = 0.04027455
Iteration 936, loss = 0.03936693
Iteration 937, loss = 0.04136737
Iteration 938, loss = 0.04353911
Iteration 939, loss = 0.04489461
Iteration 940, loss = 0.04432381
Iteration 941, loss = 0.03937361
Iteration 942, loss = 0.04128613
Iteration 943, loss = 0.04087649
Iteration 944, loss = 0.04240808
Iteration 945, loss = 0.04775239
Iteration 946, loss = 0.04878863
Iteration 947, loss = 0.04596633
Iteration 948, loss = 0.05201880
Iteration 949, loss = 0.05148915
Iteration 950, loss = 0.05100495
Iteration 951, loss = 0.04416453
Iteration 952, loss = 0.04213463
Iteration 953, loss = 0.04432670
Iteration 954, loss = 0.04298899
Iteration 955, loss = 0.04123938
Iteration 956, loss = 0.04529703
Iteration 957, loss = 0.04266351
Iteration 958, loss = 0.04159005
Iteration 959, loss = 0.03890322
Iteration 960, loss = 0.03876194
Iteration 961, loss = 0.03987021
Iteration 962, loss = 0.04261620
Iteration 963, loss = 0.04415156
Iteration 964, loss = 0.04062722
Iteration 965, loss = 0.05330338
Iteration 966, loss = 0.05006557
Iteration 967, loss = 0.05328762
Iteration 968, loss = 0.04900334
Iteration 969, loss = 0.04460915
Iteration 970, loss = 0.04760199
Iteration 971, loss = 0.05149094
Iteration 972, loss = 0.04478694
Iteration 973, loss = 0.04534022
Iteration 974, loss = 0.05017905
Iteration 975, loss = 0.04606482
Iteration 976, loss = 0.05294349
Iteration 977, loss = 0.05094856
Iteration 978, loss = 0.04124660
Iteration 979, loss = 0.03988039
Iteration 980, loss = 0.03938851
Iteration 981, loss = 0.04078350
Iteration 982, loss = 0.03906396
Iteration 983, loss = 0.03910869
Iteration 984, loss = 0.03889836
Iteration 985, loss = 0.03887917
Iteration 986, loss = 0.03984723
Iteration 987, loss = 0.04112753
Iteration 988, loss = 0.04849335
Iteration 989, loss = 0.05641427
Iteration 990, loss = 0.05076926
Iteration 991, loss = 0.06156728
Iteration 992, loss = 0.06296834
Iteration 993, loss = 0.04763033
Iteration 994, loss = 0.05793980
Iteration 995, loss = 0.05837411
Iteration 996, loss = 0.05504882
Iteration 997, loss = 0.05943267
Iteration 998, loss = 0.04507272
Iteration 999, loss = 0.04963775
Iteration 1000, loss = 0.04979552
Run 1
Iteration 1, loss = 1824.05169950
Iteration 2, loss = 451.50526169
Iteration 3, loss = 79.74066191
Iteration 4, loss = 81.72314267
Iteration 5, loss = 26.32117686
Iteration 6, loss = 6.53283753
Iteration 7, loss = 14.05341012
Iteration 8, loss = 2.03578654
Iteration 9, loss = 1.57530111
Iteration 10, loss = 0.97872402
Iteration 11, loss = 0.15930681
Iteration 12, loss = 0.19202386
Iteration 13, loss = 0.11190043
Iteration 14, loss = 0.07422091
Iteration 15, loss = 0.24461719
Iteration 16, loss = 0.11031754
Iteration 17, loss = 0.06333349
Iteration 18, loss = 0.09055667
Iteration 19, loss = 0.06324002
Iteration 20, loss = 0.06831033
Iteration 21, loss = 0.06231369
Iteration 22, loss = 0.05649456
Iteration 23, loss = 0.05825854
Iteration 24, loss = 0.05560597
Iteration 25, loss = 0.05517683
Iteration 26, loss = 0.05421850
Iteration 27, loss = 0.05363395
Iteration 28, loss = 0.05355310
Iteration 29, loss = 0.05283157
Iteration 30, loss = 0.05057530
Iteration 31, loss = 0.05014962
Iteration 32, loss = 0.04829592
Iteration 33, loss = 0.04747479
Iteration 34, loss = 0.04700108
Iteration 35, loss = 0.04581450
Iteration 36, loss = 0.05010899
Iteration 37, loss = 0.04538364
Iteration 38, loss = 0.04508322
Iteration 39, loss = 0.04257508
Iteration 40, loss = 0.04310850
Iteration 41, loss = 0.04206839
Iteration 42, loss = 0.04360149
Iteration 43, loss = 0.04272375
Iteration 44, loss = 0.04516071
Iteration 45, loss = 0.04282981
Iteration 46, loss = 0.04245146
Iteration 47, loss = 0.04329134
Iteration 48, loss = 0.04399387
Iteration 49, loss = 0.04620159
Iteration 50, loss = 0.04410888
Iteration 51, loss = 0.04374130
Iteration 52, loss = 0.04303597
Iteration 53, loss = 0.04096520
Iteration 54, loss = 0.04197868
Iteration 55, loss = 0.04141673
Iteration 56, loss = 0.04176384
Iteration 57, loss = 0.04192220
Iteration 58, loss = 0.04233393
Iteration 59, loss = 0.04112707
Iteration 60, loss = 0.03994873
Iteration 61, loss = 0.04041779
Iteration 62, loss = 0.04043686
Iteration 63, loss = 0.04039817
Iteration 64, loss = 0.04039853
Iteration 65, loss = 0.04040153
Iteration 66, loss = 0.04066583
Iteration 67, loss = 0.04185175
Iteration 68, loss = 0.03977292
Iteration 69, loss = 0.04049379
Iteration 70, loss = 0.03946754
Iteration 71, loss = 0.04046479
Iteration 72, loss = 0.04102404
Iteration 73, loss = 0.04088472
Iteration 74, loss = 0.04179013
Iteration 75, loss = 0.04271189
Iteration 76, loss = 0.04264528
Iteration 77, loss = 0.04406894
Iteration 78, loss = 0.04417061
Iteration 79, loss = 0.04163617
Iteration 80, loss = 0.03968483
Iteration 81, loss = 0.03932921
Iteration 82, loss = 0.03974932
Iteration 83, loss = 0.04038624
Iteration 84, loss = 0.04118084
Iteration 85, loss = 0.03889060
Iteration 86, loss = 0.04120320
Iteration 87, loss = 0.04705209
Iteration 88, loss = 0.04286399
Iteration 89, loss = 0.04130186
Iteration 90, loss = 0.03984172
Iteration 91, loss = 0.03992646
Iteration 92, loss = 0.03974637
Iteration 93, loss = 0.04097517
Iteration 94, loss = 0.04095254
Iteration 95, loss = 0.04153411
Iteration 96, loss = 0.04161918
Iteration 97, loss = 0.04138318
Iteration 98, loss = 0.04140298
Iteration 99, loss = 0.04389589
Iteration 100, loss = 0.04078526
Iteration 101, loss = 0.04262756
Iteration 102, loss = 0.04125919
Iteration 103, loss = 0.04295315
Iteration 104, loss = 0.04153627
Iteration 105, loss = 0.04134794
Iteration 106, loss = 0.03974609
Iteration 107, loss = 0.04156069
Iteration 108, loss = 0.04498670
Iteration 109, loss = 0.04097073
Iteration 110, loss = 0.04129699
Iteration 111, loss = 0.04081292
Iteration 112, loss = 0.04110164
Iteration 113, loss = 0.04072813
Iteration 114, loss = 0.04118969
Iteration 115, loss = 0.04196988
Iteration 116, loss = 0.04182072
Iteration 117, loss = 0.03902234
Iteration 118, loss = 0.04100001
Iteration 119, loss = 0.04167690
Iteration 120, loss = 0.04286749
Iteration 121, loss = 0.04121616
Iteration 122, loss = 0.04081515
Iteration 123, loss = 0.03994673
Iteration 124, loss = 0.03952481
Iteration 125, loss = 0.03950014
Iteration 126, loss = 0.03889822
Iteration 127, loss = 0.03942367
Iteration 128, loss = 0.03891385
Iteration 129, loss = 0.03934930
Iteration 130, loss = 0.04067581
Iteration 131, loss = 0.04086612
Iteration 132, loss = 0.03950851
Iteration 133, loss = 0.04065535
Iteration 134, loss = 0.04073783
Iteration 135, loss = 0.04211338
Iteration 136, loss = 0.03959360
Iteration 137, loss = 0.03949404
Iteration 138, loss = 0.03999489
Iteration 139, loss = 0.04133216
Iteration 140, loss = 0.03983463
Iteration 141, loss = 0.04065973
Iteration 142, loss = 0.04096622
Iteration 143, loss = 0.04110007
Iteration 144, loss = 0.04265961
Iteration 145, loss = 0.04022247
Iteration 146, loss = 0.04012496
Iteration 147, loss = 0.04047133
Iteration 148, loss = 0.03978193
Iteration 149, loss = 0.03890599
Iteration 150, loss = 0.03843329
Iteration 151, loss = 0.03973285
Iteration 152, loss = 0.03916282
Iteration 153, loss = 0.03903934
Iteration 154, loss = 0.03876262
Iteration 155, loss = 0.03885957
Iteration 156, loss = 0.03943592
Iteration 157, loss = 0.03913595
Iteration 158, loss = 0.03962962
Iteration 159, loss = 0.03982859
Iteration 160, loss = 0.03882662
Iteration 161, loss = 0.03918371
Iteration 162, loss = 0.03857812
Iteration 163, loss = 0.03910406
Iteration 164, loss = 0.03992708
Iteration 165, loss = 0.03974656
Iteration 166, loss = 0.03943817
Iteration 167, loss = 0.03966364
Iteration 168, loss = 0.04030972
Iteration 169, loss = 0.04170763
Iteration 170, loss = 0.04282710
Iteration 171, loss = 0.03893043
Iteration 172, loss = 0.04193994
Iteration 173, loss = 0.04059437
Iteration 174, loss = 0.03883025
Iteration 175, loss = 0.03938006
Iteration 176, loss = 0.03962451
Iteration 177, loss = 0.04000412
Iteration 178, loss = 0.03935962
Iteration 179, loss = 0.03949198
Iteration 180, loss = 0.03912787
Iteration 181, loss = 0.03847300
Iteration 182, loss = 0.03927064
Iteration 183, loss = 0.04121982
Iteration 184, loss = 0.04043063
Iteration 185, loss = 0.04067895
Iteration 186, loss = 0.04078861
Iteration 187, loss = 0.03861170
Iteration 188, loss = 0.03985081
Iteration 189, loss = 0.04048023
Iteration 190, loss = 0.03908860
Iteration 191, loss = 0.03937070
Iteration 192, loss = 0.03889245
Iteration 193, loss = 0.04021269
Iteration 194, loss = 0.04003851
Iteration 195, loss = 0.03924496
Iteration 196, loss = 0.03863328
Iteration 197, loss = 0.03921455
Iteration 198, loss = 0.03897973
Iteration 199, loss = 0.03903478
Iteration 200, loss = 0.03899493
Iteration 201, loss = 0.04084257
Iteration 202, loss = 0.03938411
Iteration 203, loss = 0.03900675
Iteration 204, loss = 0.03956355
Iteration 205, loss = 0.03944438
Iteration 206, loss = 0.03893649
Iteration 207, loss = 0.03909209
Iteration 208, loss = 0.03928508
Iteration 209, loss = 0.04096078
Iteration 210, loss = 0.03922913
Iteration 211, loss = 0.03906935
Iteration 212, loss = 0.03867774
Iteration 213, loss = 0.03855301
Iteration 214, loss = 0.03914669
Iteration 215, loss = 0.03936382
Iteration 216, loss = 0.03893872
Iteration 217, loss = 0.03886134
Iteration 218, loss = 0.03874394
Iteration 219, loss = 0.03941184
Iteration 220, loss = 0.03931104
Iteration 221, loss = 0.03854959
Iteration 222, loss = 0.03866548
Iteration 223, loss = 0.03900686
Iteration 224, loss = 0.03860944
Iteration 225, loss = 0.03911800
Iteration 226, loss = 0.03929532
Iteration 227, loss = 0.04069728
Iteration 228, loss = 0.03910740
Iteration 229, loss = 0.03953493
Iteration 230, loss = 0.03836513
Iteration 231, loss = 0.03909505
Iteration 232, loss = 0.03897421
Iteration 233, loss = 0.03945488
Iteration 234, loss = 0.03876204
Iteration 235, loss = 0.03835735
Iteration 236, loss = 0.03883608
Iteration 237, loss = 0.03844870
Iteration 238, loss = 0.03941515
Iteration 239, loss = 0.03823261
Iteration 240, loss = 0.03838087
Iteration 241, loss = 0.03798878
Iteration 242, loss = 0.03829576
Iteration 243, loss = 0.03868343
Iteration 244, loss = 0.04324041
Iteration 245, loss = 0.04179232
Iteration 246, loss = 0.04066606
Iteration 247, loss = 0.04070927
Iteration 248, loss = 0.03937728
Iteration 249, loss = 0.03833844
Iteration 250, loss = 0.03976572
Iteration 251, loss = 0.03927406
Iteration 252, loss = 0.03888134
Iteration 253, loss = 0.03931811
Iteration 254, loss = 0.03921378
Iteration 255, loss = 0.04037709
Iteration 256, loss = 0.03951829
Iteration 257, loss = 0.03900696
Iteration 258, loss = 0.03942769
Iteration 259, loss = 0.03835236
Iteration 260, loss = 0.03942363
Iteration 261, loss = 0.03995732
Iteration 262, loss = 0.03999089
Iteration 263, loss = 0.03939256
Iteration 264, loss = 0.03885666
Iteration 265, loss = 0.03828645
Iteration 266, loss = 0.03873862
Iteration 267, loss = 0.03914080
Iteration 268, loss = 0.03833033
Iteration 269, loss = 0.04048676
Iteration 270, loss = 0.03927979
Iteration 271, loss = 0.03938709
Iteration 272, loss = 0.03931949
Iteration 273, loss = 0.04043128
Iteration 274, loss = 0.03987118
Iteration 275, loss = 0.03945806
Iteration 276, loss = 0.03906062
Iteration 277, loss = 0.04012214
Iteration 278, loss = 0.03901462
Iteration 279, loss = 0.03936685
Iteration 280, loss = 0.03808549
Iteration 281, loss = 0.03815738
Iteration 282, loss = 0.04059292
Iteration 283, loss = 0.03995585
Iteration 284, loss = 0.03943972
Iteration 285, loss = 0.03868040
Iteration 286, loss = 0.03981858
Iteration 287, loss = 0.03832558
Iteration 288, loss = 0.03873043
Iteration 289, loss = 0.03885082
Iteration 290, loss = 0.03809850
Iteration 291, loss = 0.03848302
Iteration 292, loss = 0.03824860
Iteration 293, loss = 0.03888126
Iteration 294, loss = 0.04404649
Iteration 295, loss = 0.04285437
Iteration 296, loss = 0.04036639
Iteration 297, loss = 0.03812316
Iteration 298, loss = 0.03898463
Iteration 299, loss = 0.04150689
Iteration 300, loss = 0.04455433
Iteration 301, loss = 0.04204533
Iteration 302, loss = 0.03995156
Iteration 303, loss = 0.04202357
Iteration 304, loss = 0.03977956
Iteration 305, loss = 0.04027983
Iteration 306, loss = 0.04244566
Iteration 307, loss = 0.04009442
Iteration 308, loss = 0.03839786
Iteration 309, loss = 0.03824573
Iteration 310, loss = 0.03813818
Iteration 311, loss = 0.03831545
Iteration 312, loss = 0.03812773
Iteration 313, loss = 0.03809614
Iteration 314, loss = 0.03800273
Iteration 315, loss = 0.03878106
Iteration 316, loss = 0.03869643
Iteration 317, loss = 0.03796600
Iteration 318, loss = 0.03849180
Iteration 319, loss = 0.03865800
Iteration 320, loss = 0.03782570
Iteration 321, loss = 0.03792240
Iteration 322, loss = 0.03830321
Iteration 323, loss = 0.04152541
Iteration 324, loss = 0.04081978
Iteration 325, loss = 0.03883482
Iteration 326, loss = 0.03789089
Iteration 327, loss = 0.03791049
Iteration 328, loss = 0.03805644
Iteration 329, loss = 0.03789726
Iteration 330, loss = 0.03784826
Iteration 331, loss = 0.03850515
Iteration 332, loss = 0.03925502
Iteration 333, loss = 0.03794056
Iteration 334, loss = 0.03816559
Iteration 335, loss = 0.03790365
Iteration 336, loss = 0.04023754
Iteration 337, loss = 0.04282055
Iteration 338, loss = 0.04298326
Iteration 339, loss = 0.04065179
Iteration 340, loss = 0.04384743
Iteration 341, loss = 0.03747124
Iteration 342, loss = 0.04075548
Iteration 343, loss = 0.04276764
Iteration 344, loss = 0.04318587
Iteration 345, loss = 0.04124191
Iteration 346, loss = 0.04038524
Iteration 347, loss = 0.03942592
Iteration 348, loss = 0.03794107
Iteration 349, loss = 0.03913549
Iteration 350, loss = 0.03922094
Iteration 351, loss = 0.03868214
Iteration 352, loss = 0.03964611
Iteration 353, loss = 0.04126518
Iteration 354, loss = 0.04096155
Iteration 355, loss = 0.03807108
Iteration 356, loss = 0.03959577
Iteration 357, loss = 0.03966038
Iteration 358, loss = 0.04383943
Iteration 359, loss = 0.03998887
Iteration 360, loss = 0.03942557
Iteration 361, loss = 0.03900191
Iteration 362, loss = 0.03837292
Iteration 363, loss = 0.03765463
Iteration 364, loss = 0.03948841
Iteration 365, loss = 0.03766267
Iteration 366, loss = 0.03823374
Iteration 367, loss = 0.03788105
Iteration 368, loss = 0.03781672
Iteration 369, loss = 0.03809115
Iteration 370, loss = 0.03818143
Iteration 371, loss = 0.03894811
Iteration 372, loss = 0.04149952
Iteration 373, loss = 0.03952516
Iteration 374, loss = 0.04228721
Iteration 375, loss = 0.03984369
Iteration 376, loss = 0.03864942
Iteration 377, loss = 0.03950816
Iteration 378, loss = 0.03868033
Iteration 379, loss = 0.03844409
Iteration 380, loss = 0.03844334
Iteration 381, loss = 0.03803487
Iteration 382, loss = 0.03817279
Iteration 383, loss = 0.03773759
Iteration 384, loss = 0.04064724
Iteration 385, loss = 0.04020043
Iteration 386, loss = 0.03846518
Iteration 387, loss = 0.03838264
Iteration 388, loss = 0.04018989
Iteration 389, loss = 0.03920438
Iteration 390, loss = 0.03820952
Iteration 391, loss = 0.03752741
Iteration 392, loss = 0.03764369
Iteration 393, loss = 0.03851317
Iteration 394, loss = 0.03855849
Iteration 395, loss = 0.03943244
Iteration 396, loss = 0.03882273
Iteration 397, loss = 0.03837492
Iteration 398, loss = 0.03806471
Iteration 399, loss = 0.03864664
Iteration 400, loss = 0.03784799
Iteration 401, loss = 0.03735511
Iteration 402, loss = 0.03955365
Iteration 403, loss = 0.03798334
Iteration 404, loss = 0.03711751
Iteration 405, loss = 0.03973324
Iteration 406, loss = 0.03821433
Iteration 407, loss = 0.03889076
Iteration 408, loss = 0.03924813
Iteration 409, loss = 0.03971595
Iteration 410, loss = 0.03990770
Iteration 411, loss = 0.04012901
Iteration 412, loss = 0.04016239
Iteration 413, loss = 0.03776427
Iteration 414, loss = 0.03994132
Iteration 415, loss = 0.03781632
Iteration 416, loss = 0.03886493
Iteration 417, loss = 0.03818797
Iteration 418, loss = 0.03749456
Iteration 419, loss = 0.03793908
Iteration 420, loss = 0.04007140
Iteration 421, loss = 0.03964662
Iteration 422, loss = 0.03884207
Iteration 423, loss = 0.03928140
Iteration 424, loss = 0.03792944
Iteration 425, loss = 0.03727064
Iteration 426, loss = 0.03804996
Iteration 427, loss = 0.03736108
Iteration 428, loss = 0.03785035
Iteration 429, loss = 0.03755424
Iteration 430, loss = 0.03740279
Iteration 431, loss = 0.03731403
Iteration 432, loss = 0.03721297
Iteration 433, loss = 0.03736376
Iteration 434, loss = 0.03729462
Iteration 435, loss = 0.03940292
Iteration 436, loss = 0.04012732
Iteration 437, loss = 0.03853168
Iteration 438, loss = 0.03761554
Iteration 439, loss = 0.03819269
Iteration 440, loss = 0.03767593
Iteration 441, loss = 0.03729846
Iteration 442, loss = 0.03706815
Iteration 443, loss = 0.03803092
Iteration 444, loss = 0.03744658
Iteration 445, loss = 0.03855033
Iteration 446, loss = 0.03707781
Iteration 447, loss = 0.03726591
Iteration 448, loss = 0.03954064
Iteration 449, loss = 0.03840379
Iteration 450, loss = 0.03922434
Iteration 451, loss = 0.04036589
Iteration 452, loss = 0.03745896
Iteration 453, loss = 0.03735891
Iteration 454, loss = 0.03925197
Iteration 455, loss = 0.03688050
Iteration 456, loss = 0.03761155
Iteration 457, loss = 0.03769802
Iteration 458, loss = 0.03820418
Iteration 459, loss = 0.03723136
Iteration 460, loss = 0.03832918
Iteration 461, loss = 0.03792813
Iteration 462, loss = 0.03866418
Iteration 463, loss = 0.03819037
Iteration 464, loss = 0.04101845
Iteration 465, loss = 0.04127837
Iteration 466, loss = 0.04132494
Iteration 467, loss = 0.03845378
Iteration 468, loss = 0.03849502
Iteration 469, loss = 0.03951779
Iteration 470, loss = 0.04020395
Iteration 471, loss = 0.04108670
Iteration 472, loss = 0.04002184
Iteration 473, loss = 0.03929943
Iteration 474, loss = 0.03911010
Iteration 475, loss = 0.04003718
Iteration 476, loss = 0.04034766
Iteration 477, loss = 0.04080445
Iteration 478, loss = 0.03999418
Iteration 479, loss = 0.03752595
Iteration 480, loss = 0.03952783
Iteration 481, loss = 0.03932382
Iteration 482, loss = 0.03766535
Iteration 483, loss = 0.03652682
Iteration 484, loss = 0.03696708
Iteration 485, loss = 0.04165412
Iteration 486, loss = 0.04073353
Iteration 487, loss = 0.04307288
Iteration 488, loss = 0.04133690
Iteration 489, loss = 0.03997283
Iteration 490, loss = 0.03897526
Iteration 491, loss = 0.03907977
Iteration 492, loss = 0.03950664
Iteration 493, loss = 0.03798394
Iteration 494, loss = 0.03966992
Iteration 495, loss = 0.03766250
Iteration 496, loss = 0.03691443
Iteration 497, loss = 0.03754627
Iteration 498, loss = 0.03783091
Iteration 499, loss = 0.03760529
Iteration 500, loss = 0.03717569
Iteration 501, loss = 0.03794435
Iteration 502, loss = 0.03912728
Iteration 503, loss = 0.03923311
Iteration 504, loss = 0.04021003
Iteration 505, loss = 0.03701612
Iteration 506, loss = 0.03750175
Iteration 507, loss = 0.03711268
Iteration 508, loss = 0.03813205
Iteration 509, loss = 0.03733489
Iteration 510, loss = 0.03769674
Iteration 511, loss = 0.03875937
Iteration 512, loss = 0.03999175
Iteration 513, loss = 0.03869290
Iteration 514, loss = 0.03890089
Iteration 515, loss = 0.04036329
Iteration 516, loss = 0.03998651
Iteration 517, loss = 0.04046995
Iteration 518, loss = 0.04086201
Iteration 519, loss = 0.03981580
Iteration 520, loss = 0.04040981
Iteration 521, loss = 0.03674582
Iteration 522, loss = 0.03937931
Iteration 523, loss = 0.04053127
Iteration 524, loss = 0.03795457
Iteration 525, loss = 0.03731900
Iteration 526, loss = 0.03837180
Iteration 527, loss = 0.03873901
Iteration 528, loss = 0.03814071
Iteration 529, loss = 0.03962075
Iteration 530, loss = 0.03976417
Iteration 531, loss = 0.03958233
Iteration 532, loss = 0.03852313
Iteration 533, loss = 0.03948816
Iteration 534, loss = 0.03696069
Iteration 535, loss = 0.03955993
Iteration 536, loss = 0.03680420
Iteration 537, loss = 0.03767101
Iteration 538, loss = 0.03708224
Iteration 539, loss = 0.03671639
Iteration 540, loss = 0.03708793
Iteration 541, loss = 0.03945212
Iteration 542, loss = 0.03958337
Iteration 543, loss = 0.03865303
Iteration 544, loss = 0.03795229
Iteration 545, loss = 0.03913746
Iteration 546, loss = 0.04090392
Iteration 547, loss = 0.03704378
Iteration 548, loss = 0.03738708
Iteration 549, loss = 0.03859958
Iteration 550, loss = 0.03691573
Iteration 551, loss = 0.03796313
Iteration 552, loss = 0.04101897
Iteration 553, loss = 0.03982955
Iteration 554, loss = 0.03687471
Iteration 555, loss = 0.03713951
Iteration 556, loss = 0.03655290
Iteration 557, loss = 0.03700309
Iteration 558, loss = 0.03693299
Iteration 559, loss = 0.03796797
Iteration 560, loss = 0.03889522
Iteration 561, loss = 0.03696143
Iteration 562, loss = 0.03653443
Iteration 563, loss = 0.03685449
Iteration 564, loss = 0.03639950
Iteration 565, loss = 0.03761408
Iteration 566, loss = 0.04075673
Iteration 567, loss = 0.03776813
Iteration 568, loss = 0.03944063
Iteration 569, loss = 0.03829241
Iteration 570, loss = 0.03649077
Iteration 571, loss = 0.03810695
Iteration 572, loss = 0.03905152
Iteration 573, loss = 0.04123387
Iteration 574, loss = 0.03901512
Iteration 575, loss = 0.03978520
Iteration 576, loss = 0.03785757
Iteration 577, loss = 0.03684876
Iteration 578, loss = 0.04013437
Iteration 579, loss = 0.03770220
Iteration 580, loss = 0.03662803
Iteration 581, loss = 0.03678522
Iteration 582, loss = 0.03662059
Iteration 583, loss = 0.03774078
Iteration 584, loss = 0.03681875
Iteration 585, loss = 0.03744110
Iteration 586, loss = 0.03654500
Iteration 587, loss = 0.03646719
Iteration 588, loss = 0.03640740
Iteration 589, loss = 0.03694551
Iteration 590, loss = 0.03683418
Iteration 591, loss = 0.03612030
Iteration 592, loss = 0.03703838
Iteration 593, loss = 0.03745058
Iteration 594, loss = 0.03667458
Iteration 595, loss = 0.03778178
Iteration 596, loss = 0.03821962
Iteration 597, loss = 0.03809923
Iteration 598, loss = 0.03745489
Iteration 599, loss = 0.03659172
Iteration 600, loss = 0.03644760
Iteration 601, loss = 0.03609043
Iteration 602, loss = 0.03680409
Iteration 603, loss = 0.03618968
Iteration 604, loss = 0.03659926
Iteration 605, loss = 0.03628594
Iteration 606, loss = 0.03661472
Iteration 607, loss = 0.03842338
Iteration 608, loss = 0.04323067
Iteration 609, loss = 0.04416505
Iteration 610, loss = 0.04558202
Iteration 611, loss = 0.03874121
Iteration 612, loss = 0.03929583
Iteration 613, loss = 0.03694797
Iteration 614, loss = 0.04035829
Iteration 615, loss = 0.04353427
Iteration 616, loss = 0.04403905
Iteration 617, loss = 0.04210119
Iteration 618, loss = 0.04314851
Iteration 619, loss = 0.03905565
Iteration 620, loss = 0.04028415
Iteration 621, loss = 0.03930878
Iteration 622, loss = 0.03918308
Iteration 623, loss = 0.03795625
Iteration 624, loss = 0.03843348
Iteration 625, loss = 0.03918621
Iteration 626, loss = 0.04180572
Iteration 627, loss = 0.03728524
Iteration 628, loss = 0.03690528
Iteration 629, loss = 0.03684029
Iteration 630, loss = 0.03801482
Iteration 631, loss = 0.03847955
Iteration 632, loss = 0.03986611
Iteration 633, loss = 0.03742887
Iteration 634, loss = 0.03892760
Iteration 635, loss = 0.03778661
Iteration 636, loss = 0.03788887
Iteration 637, loss = 0.03935481
Iteration 638, loss = 0.03937168
Iteration 639, loss = 0.03686761
Iteration 640, loss = 0.03899987
Iteration 641, loss = 0.03813363
Iteration 642, loss = 0.03728118
Iteration 643, loss = 0.03693878
Iteration 644, loss = 0.03852933
Iteration 645, loss = 0.03830014
Iteration 646, loss = 0.03712688
Iteration 647, loss = 0.03660364
Iteration 648, loss = 0.03792230
Iteration 649, loss = 0.03668756
Iteration 650, loss = 0.03744753
Iteration 651, loss = 0.03760348
Iteration 652, loss = 0.03828016
Iteration 653, loss = 0.03970715
Iteration 654, loss = 0.04301394
Iteration 655, loss = 0.04007807
Iteration 656, loss = 0.04039839
Iteration 657, loss = 0.03914960
Iteration 658, loss = 0.03913050
Iteration 659, loss = 0.03927805
Iteration 660, loss = 0.03959957
Iteration 661, loss = 0.03784892
Iteration 662, loss = 0.03782537
Iteration 663, loss = 0.03992200
Iteration 664, loss = 0.04306254
Iteration 665, loss = 0.03982613
Iteration 666, loss = 0.03749252
Iteration 667, loss = 0.04037951
Iteration 668, loss = 0.03739473
Iteration 669, loss = 0.03912781
Iteration 670, loss = 0.04134403
Iteration 671, loss = 0.03784753
Iteration 672, loss = 0.03905257
Iteration 673, loss = 0.04182218
Iteration 674, loss = 0.03971614
Iteration 675, loss = 0.03823774
Iteration 676, loss = 0.03657617
Iteration 677, loss = 0.03709206
Iteration 678, loss = 0.03885681
Iteration 679, loss = 0.03799296
Iteration 680, loss = 0.04038754
Iteration 681, loss = 0.04029452
Iteration 682, loss = 0.03684288
Iteration 683, loss = 0.03994289
Iteration 684, loss = 0.03626046
Iteration 685, loss = 0.03820740
Iteration 686, loss = 0.03739342
Iteration 687, loss = 0.03718089
Iteration 688, loss = 0.03746382
Iteration 689, loss = 0.03668201
Iteration 690, loss = 0.03769996
Iteration 691, loss = 0.03618977
Iteration 692, loss = 0.03597164
Iteration 693, loss = 0.03760642
Iteration 694, loss = 0.04014173
Iteration 695, loss = 0.04258078
Iteration 696, loss = 0.04151315
Iteration 697, loss = 0.03975789
Iteration 698, loss = 0.03979841
Iteration 699, loss = 0.04067819
Iteration 700, loss = 0.03897233
Iteration 701, loss = 0.03723945
Iteration 702, loss = 0.03760847
Iteration 703, loss = 0.03824696
Iteration 704, loss = 0.03697291
Iteration 705, loss = 0.03849539
Iteration 706, loss = 0.03998176
Iteration 707, loss = 0.04095298
Iteration 708, loss = 0.04016760
Iteration 709, loss = 0.03921444
Iteration 710, loss = 0.03710209
Iteration 711, loss = 0.04021909
Iteration 712, loss = 0.03929004
Iteration 713, loss = 0.03695794
Iteration 714, loss = 0.03762572
Iteration 715, loss = 0.03769287
Iteration 716, loss = 0.03813971
Iteration 717, loss = 0.03794323
Iteration 718, loss = 0.03734909
Iteration 719, loss = 0.03650956
Iteration 720, loss = 0.03694375
Iteration 721, loss = 0.03770795
Iteration 722, loss = 0.03717316
Iteration 723, loss = 0.03744191
Iteration 724, loss = 0.03627249
Iteration 725, loss = 0.03607236
Iteration 726, loss = 0.03599154
Iteration 727, loss = 0.03656278
Iteration 728, loss = 0.03643446
Iteration 729, loss = 0.03660090
Iteration 730, loss = 0.03615008
Iteration 731, loss = 0.03598339
Iteration 732, loss = 0.03602013
Iteration 733, loss = 0.03603900
Iteration 734, loss = 0.03664922
Iteration 735, loss = 0.03661637
Iteration 736, loss = 0.03953375
Iteration 737, loss = 0.03815529
Iteration 738, loss = 0.03802368
Iteration 739, loss = 0.03694871
Iteration 740, loss = 0.03571614
Iteration 741, loss = 0.03730516
Iteration 742, loss = 0.03656054
Iteration 743, loss = 0.03662719
Iteration 744, loss = 0.03609664
Iteration 745, loss = 0.03579027
Iteration 746, loss = 0.03762396
Iteration 747, loss = 0.03697084
Iteration 748, loss = 0.03596278
Iteration 749, loss = 0.03577570
Iteration 750, loss = 0.03601820
Iteration 751, loss = 0.03633738
Iteration 752, loss = 0.03579018
Iteration 753, loss = 0.03601632
Iteration 754, loss = 0.03573654
Iteration 755, loss = 0.03586085
Iteration 756, loss = 0.03637936
Iteration 757, loss = 0.03625320
Iteration 758, loss = 0.03635963
Iteration 759, loss = 0.03606083
Iteration 760, loss = 0.03815105
Iteration 761, loss = 0.03646862
Iteration 762, loss = 0.03623267
Iteration 763, loss = 0.03695116
Iteration 764, loss = 0.03679061
Iteration 765, loss = 0.03665702
Iteration 766, loss = 0.03558373
Iteration 767, loss = 0.03636698
Iteration 768, loss = 0.03581703
Iteration 769, loss = 0.03698735
Iteration 770, loss = 0.03612912
Iteration 771, loss = 0.03600808
Iteration 772, loss = 0.03675252
Iteration 773, loss = 0.03614118
Iteration 774, loss = 0.03600735
Iteration 775, loss = 0.03633174
Iteration 776, loss = 0.03737061
Iteration 777, loss = 0.03656123
Iteration 778, loss = 0.03685370
Iteration 779, loss = 0.03738130
Iteration 780, loss = 0.03751225
Iteration 781, loss = 0.03800813
Iteration 782, loss = 0.04081843
Iteration 783, loss = 0.03811519
Iteration 784, loss = 0.03621199
Iteration 785, loss = 0.03653304
Iteration 786, loss = 0.03606725
Iteration 787, loss = 0.03706373
Iteration 788, loss = 0.04215983
Iteration 789, loss = 0.04015175
Iteration 790, loss = 0.03853548
Iteration 791, loss = 0.04120546
Iteration 792, loss = 0.04220833
Iteration 793, loss = 0.04239667
Iteration 794, loss = 0.03979808
Iteration 795, loss = 0.03951388
Iteration 796, loss = 0.03926276
Iteration 797, loss = 0.03722697
Iteration 798, loss = 0.03666297
Iteration 799, loss = 0.03609227
Iteration 800, loss = 0.03604584
Iteration 801, loss = 0.03640738
Iteration 802, loss = 0.03638211
Iteration 803, loss = 0.03665282
Iteration 804, loss = 0.03699041
Iteration 805, loss = 0.03599870
Iteration 806, loss = 0.03581845
Iteration 807, loss = 0.03588951
Iteration 808, loss = 0.03572244
Iteration 809, loss = 0.03558659
Iteration 810, loss = 0.03553843
Iteration 811, loss = 0.03580567
Iteration 812, loss = 0.03555985
Iteration 813, loss = 0.03631359
Iteration 814, loss = 0.03616904
Iteration 815, loss = 0.03781723
Iteration 816, loss = 0.03604335
Iteration 817, loss = 0.03580005
Iteration 818, loss = 0.03519422
Iteration 819, loss = 0.03565587
Iteration 820, loss = 0.03623310
Iteration 821, loss = 0.03723591
Iteration 822, loss = 0.03820989
Iteration 823, loss = 0.03613236
Iteration 824, loss = 0.03606676
Iteration 825, loss = 0.03653377
Iteration 826, loss = 0.03570891
Iteration 827, loss = 0.03504315
Iteration 828, loss = 0.03556527
Iteration 829, loss = 0.03618066
Iteration 830, loss = 0.03580223
Iteration 831, loss = 0.03598775
Iteration 832, loss = 0.03507422
Iteration 833, loss = 0.03504905
Iteration 834, loss = 0.03563922
Iteration 835, loss = 0.03536277
Iteration 836, loss = 0.03554532
Iteration 837, loss = 0.03601281
Iteration 838, loss = 0.03636983
Iteration 839, loss = 0.03680364
Iteration 840, loss = 0.03577603
Iteration 841, loss = 0.03578881
Iteration 842, loss = 0.03550902
Iteration 843, loss = 0.03572622
Iteration 844, loss = 0.03558175
Iteration 845, loss = 0.03796714
Iteration 846, loss = 0.04291021
Iteration 847, loss = 0.03862271
Iteration 848, loss = 0.03812296
Iteration 849, loss = 0.03785772
Iteration 850, loss = 0.03749193
Iteration 851, loss = 0.03791594
Iteration 852, loss = 0.03517836
Iteration 853, loss = 0.03617921
Iteration 854, loss = 0.03841649
Iteration 855, loss = 0.03859857
Iteration 856, loss = 0.03831840
Iteration 857, loss = 0.03740186
Iteration 858, loss = 0.03901307
Iteration 859, loss = 0.04279162
Iteration 860, loss = 0.04231098
Iteration 861, loss = 0.04260357
Iteration 862, loss = 0.04140520
Iteration 863, loss = 0.04045319
Iteration 864, loss = 0.04089307
Iteration 865, loss = 0.03945624
Iteration 866, loss = 0.03965159
Iteration 867, loss = 0.03944741
Iteration 868, loss = 0.03935785
Iteration 869, loss = 0.03894169
Iteration 870, loss = 0.03908315
Iteration 871, loss = 0.03963624
Iteration 872, loss = 0.03876037
Iteration 873, loss = 0.03874258
Iteration 874, loss = 0.03850384
Iteration 875, loss = 0.03850822
Iteration 876, loss = 0.03815608
Iteration 877, loss = 0.03804153
Iteration 878, loss = 0.03774049
Iteration 879, loss = 0.03660227
Iteration 880, loss = 0.03715484
Iteration 881, loss = 0.03595810
Iteration 882, loss = 0.03636390
Iteration 883, loss = 0.03555784
Iteration 884, loss = 0.03600480
Iteration 885, loss = 0.03567933
Iteration 886, loss = 0.03666297
Iteration 887, loss = 0.03735281
Iteration 888, loss = 0.03792544
Iteration 889, loss = 0.04040426
Iteration 890, loss = 0.03646607
Iteration 891, loss = 0.03847323
Iteration 892, loss = 0.03551028
Iteration 893, loss = 0.03541771
Iteration 894, loss = 0.03519876
Iteration 895, loss = 0.03523131
Iteration 896, loss = 0.03485316
Iteration 897, loss = 0.03594915
Iteration 898, loss = 0.03490556
Iteration 899, loss = 0.03478262
Iteration 900, loss = 0.03516896
Iteration 901, loss = 0.03857089
Iteration 902, loss = 0.03614730
Iteration 903, loss = 0.03521545
Iteration 904, loss = 0.03504364
Iteration 905, loss = 0.03481710
Iteration 906, loss = 0.03560921
Iteration 907, loss = 0.03734668
Iteration 908, loss = 0.03643710
Iteration 909, loss = 0.03598797
Iteration 910, loss = 0.03632797
Iteration 911, loss = 0.03674741
Iteration 912, loss = 0.03512599
Iteration 913, loss = 0.03619435
Iteration 914, loss = 0.03566219
Iteration 915, loss = 0.03683759
Iteration 916, loss = 0.03445193
Iteration 917, loss = 0.03600672
Iteration 918, loss = 0.03751324
Iteration 919, loss = 0.04033170
Iteration 920, loss = 0.04102344
Iteration 921, loss = 0.04233455
Iteration 922, loss = 0.03751909
Iteration 923, loss = 0.03914877
Iteration 924, loss = 0.03658109
Iteration 925, loss = 0.04120561
Iteration 926, loss = 0.03812944
Iteration 927, loss = 0.03732251
Iteration 928, loss = 0.03709766
Iteration 929, loss = 0.03711035
Iteration 930, loss = 0.03513547
Iteration 931, loss = 0.03778190
Iteration 932, loss = 0.03605310
Iteration 933, loss = 0.03850363
Iteration 934, loss = 0.03758158
Iteration 935, loss = 0.03639229
Iteration 936, loss = 0.03557176
Iteration 937, loss = 0.03766456
Iteration 938, loss = 0.03555276
Iteration 939, loss = 0.03508250
Iteration 940, loss = 0.03611787
Iteration 941, loss = 0.03748863
Iteration 942, loss = 0.03668034
Iteration 943, loss = 0.03542541
Iteration 944, loss = 0.03558870
Iteration 945, loss = 0.03484307
Iteration 946, loss = 0.03517822
Iteration 947, loss = 0.03517971
Iteration 948, loss = 0.03512029
Iteration 949, loss = 0.03527507
Iteration 950, loss = 0.03468397
Iteration 951, loss = 0.03556789
Iteration 952, loss = 0.03677511
Iteration 953, loss = 0.03584910
Iteration 954, loss = 0.03518521
Iteration 955, loss = 0.03683122
Iteration 956, loss = 0.03575816
Iteration 957, loss = 0.03525247
Iteration 958, loss = 0.03857792
Iteration 959, loss = 0.03857055
Iteration 960, loss = 0.04617584
Iteration 961, loss = 0.04143076
Iteration 962, loss = 0.04159332
Iteration 963, loss = 0.04293944
Iteration 964, loss = 0.03728154
Iteration 965, loss = 0.03973404
Iteration 966, loss = 0.03776872
Iteration 967, loss = 0.04015939
Iteration 968, loss = 0.03785830
Iteration 969, loss = 0.03822895
Iteration 970, loss = 0.03790772
Iteration 971, loss = 0.04121951
Iteration 972, loss = 0.04033894
Iteration 973, loss = 0.03549289
Iteration 974, loss = 0.03662729
Iteration 975, loss = 0.03601319
Iteration 976, loss = 0.03510638
Iteration 977, loss = 0.03527071
Iteration 978, loss = 0.03459114
Iteration 979, loss = 0.03467372
Iteration 980, loss = 0.03538166
Iteration 981, loss = 0.03699607
Iteration 982, loss = 0.03623275
Iteration 983, loss = 0.03444942
Iteration 984, loss = 0.03581311
Iteration 985, loss = 0.03546026
Iteration 986, loss = 0.03457580
Iteration 987, loss = 0.03512186
Iteration 988, loss = 0.03761566
Iteration 989, loss = 0.04247464
Iteration 990, loss = 0.04137863
Iteration 991, loss = 0.03678875
Iteration 992, loss = 0.03759982
Iteration 993, loss = 0.03564012
Iteration 994, loss = 0.03564711
Iteration 995, loss = 0.03693267
Iteration 996, loss = 0.03595638
Iteration 997, loss = 0.03573879
Iteration 998, loss = 0.03537923
Iteration 999, loss = 0.03495655
Iteration 1000, loss = 0.03408918
Run 2
Iteration 1, loss = 3328.46510415
Iteration 2, loss = 348.13782375
Iteration 3, loss = 93.53158000
Iteration 4, loss = 162.22779883
Iteration 5, loss = 11.15153267
Iteration 6, loss = 40.89133213
Iteration 7, loss = 9.26285491
Iteration 8, loss = 8.06790767
Iteration 9, loss = 14.41741172
Iteration 10, loss = 1.70626433
Iteration 11, loss = 3.77178462
Iteration 12, loss = 1.80311585
Iteration 13, loss = 0.70792423
Iteration 14, loss = 0.21228115
Iteration 15, loss = 0.34466205
Iteration 16, loss = 0.33945847
Iteration 17, loss = 0.47845023
Iteration 18, loss = 0.36432646
Iteration 19, loss = 0.16730362
Iteration 20, loss = 0.15859614
Iteration 21, loss = 0.09386264
Iteration 22, loss = 0.07953690
Iteration 23, loss = 0.07101139
Iteration 24, loss = 0.06482037
Iteration 25, loss = 0.06016871
Iteration 26, loss = 0.06136161
Iteration 27, loss = 0.06046651
Iteration 28, loss = 0.06033678
Iteration 29, loss = 0.05980232
Iteration 30, loss = 0.05982430
Iteration 31, loss = 0.05973330
Iteration 32, loss = 0.05949277
Iteration 33, loss = 0.05907110
Iteration 34, loss = 0.05874962
Iteration 35, loss = 0.05934157
Iteration 36, loss = 0.05857781
Iteration 37, loss = 0.05850365
Iteration 38, loss = 0.05890665
Iteration 39, loss = 0.05813193
Iteration 40, loss = 0.05814247
Iteration 41, loss = 0.05818777
Iteration 42, loss = 0.05821188
Iteration 43, loss = 0.05815197
Iteration 44, loss = 0.05790192
Iteration 45, loss = 0.05951361
Iteration 46, loss = 0.05736254
Iteration 47, loss = 0.05757365
Iteration 48, loss = 0.05808730
Iteration 49, loss = 0.05694884
Iteration 50, loss = 0.05678752
Iteration 51, loss = 0.05690220
Iteration 52, loss = 0.05665465
Iteration 53, loss = 0.05823463
Iteration 54, loss = 0.06031364
Iteration 55, loss = 0.05702192
Iteration 56, loss = 0.05839874
Iteration 57, loss = 0.05748291
Iteration 58, loss = 0.05674419
Iteration 59, loss = 0.05724947
Iteration 60, loss = 0.05709730
Iteration 61, loss = 0.05685903
Iteration 62, loss = 0.05771105
Iteration 63, loss = 0.06016871
Iteration 64, loss = 0.06043774
Iteration 65, loss = 0.05826669
Iteration 66, loss = 0.05885097
Iteration 67, loss = 0.05739058
Iteration 68, loss = 0.05509985
Iteration 69, loss = 0.05527544
Iteration 70, loss = 0.05585210
Iteration 71, loss = 0.05583690
Iteration 72, loss = 0.05513260
Iteration 73, loss = 0.05455693
Iteration 74, loss = 0.05663964
Iteration 75, loss = 0.05646264
Iteration 76, loss = 0.05468316
Iteration 77, loss = 0.05487924
Iteration 78, loss = 0.05495382
Iteration 79, loss = 0.05627111
Iteration 80, loss = 0.05441459
Iteration 81, loss = 0.05227779
Iteration 82, loss = 0.05409010
Iteration 83, loss = 0.05292711
Iteration 84, loss = 0.05405767
Iteration 85, loss = 0.05529634
Iteration 86, loss = 0.05397153
Iteration 87, loss = 0.05564751
Iteration 88, loss = 0.05347505
Iteration 89, loss = 0.05505040
Iteration 90, loss = 0.05818553
Iteration 91, loss = 0.05760334
Iteration 92, loss = 0.05906973
Iteration 93, loss = 0.06020009
Iteration 94, loss = 0.05863293
Iteration 95, loss = 0.05331274
Iteration 96, loss = 0.05491954
Iteration 97, loss = 0.05429184
Iteration 98, loss = 0.05220993
Iteration 99, loss = 0.05085284
Iteration 100, loss = 0.05034837
Iteration 101, loss = 0.05043698
Iteration 102, loss = 0.05310334
Iteration 103, loss = 0.06058095
Iteration 104, loss = 0.05878933
Iteration 105, loss = 0.05822219
Iteration 106, loss = 0.05534157
Iteration 107, loss = 0.05196715
Iteration 108, loss = 0.05243831
Iteration 109, loss = 0.04834553
Iteration 110, loss = 0.04891042
Iteration 111, loss = 0.05093456
Iteration 112, loss = 0.05371018
Iteration 113, loss = 0.05378798
Iteration 114, loss = 0.05032015
Iteration 115, loss = 0.04934787
Iteration 116, loss = 0.05110963
Iteration 117, loss = 0.05481731
Iteration 118, loss = 0.05212227
Iteration 119, loss = 0.05252855
Iteration 120, loss = 0.04999014
Iteration 121, loss = 0.05261378
Iteration 122, loss = 0.04784797
Iteration 123, loss = 0.04704784
Iteration 124, loss = 0.04788294
Iteration 125, loss = 0.04647087
Iteration 126, loss = 0.04536321
Iteration 127, loss = 0.04544655
Iteration 128, loss = 0.04670447
Iteration 129, loss = 0.04710119
Iteration 130, loss = 0.04606186
Iteration 131, loss = 0.04619126
Iteration 132, loss = 0.04632123
Iteration 133, loss = 0.04629945
Iteration 134, loss = 0.04578948
Iteration 135, loss = 0.04711490
Iteration 136, loss = 0.04571743
Iteration 137, loss = 0.04539579
Iteration 138, loss = 0.04519498
Iteration 139, loss = 0.04606085
Iteration 140, loss = 0.04476478
Iteration 141, loss = 0.04366520
Iteration 142, loss = 0.04529800
Iteration 143, loss = 0.04343643
Iteration 144, loss = 0.04307437
Iteration 145, loss = 0.04292794
Iteration 146, loss = 0.04398926
Iteration 147, loss = 0.04498683
Iteration 148, loss = 0.04943685
Iteration 149, loss = 0.04744874
Iteration 150, loss = 0.04912692
Iteration 151, loss = 0.04375106
Iteration 152, loss = 0.04275714
Iteration 153, loss = 0.04226226
Iteration 154, loss = 0.04459946
Iteration 155, loss = 0.04302849
Iteration 156, loss = 0.04234791
Iteration 157, loss = 0.04282889
Iteration 158, loss = 0.04334525
Iteration 159, loss = 0.04252816
Iteration 160, loss = 0.04282067
Iteration 161, loss = 0.04667889
Iteration 162, loss = 0.04385266
Iteration 163, loss = 0.04322203
Iteration 164, loss = 0.04429360
Iteration 165, loss = 0.04183765
Iteration 166, loss = 0.04311744
Iteration 167, loss = 0.04771933
Iteration 168, loss = 0.04160123
Iteration 169, loss = 0.04207104
Iteration 170, loss = 0.04533069
Iteration 171, loss = 0.04290727
Iteration 172, loss = 0.04177417
Iteration 173, loss = 0.04174576
Iteration 174, loss = 0.04212966
Iteration 175, loss = 0.04173794
Iteration 176, loss = 0.04315497
Iteration 177, loss = 0.04217635
Iteration 178, loss = 0.04178929
Iteration 179, loss = 0.04514327
Iteration 180, loss = 0.04820982
Iteration 181, loss = 0.04259870
Iteration 182, loss = 0.04389417
Iteration 183, loss = 0.04444141
Iteration 184, loss = 0.04583756
Iteration 185, loss = 0.04623660
Iteration 186, loss = 0.04574524
Iteration 187, loss = 0.04497288
Iteration 188, loss = 0.04259984
Iteration 189, loss = 0.04154555
Iteration 190, loss = 0.04149053
Iteration 191, loss = 0.04239802
Iteration 192, loss = 0.04163320
Iteration 193, loss = 0.04177066
Iteration 194, loss = 0.04159417
Iteration 195, loss = 0.04200225
Iteration 196, loss = 0.04476511
Iteration 197, loss = 0.04738983
Iteration 198, loss = 0.04386334
Iteration 199, loss = 0.04237957
Iteration 200, loss = 0.04523042
Iteration 201, loss = 0.04464796
Iteration 202, loss = 0.04523938
Iteration 203, loss = 0.04928729
Iteration 204, loss = 0.04786547
Iteration 205, loss = 0.04747378
Iteration 206, loss = 0.05370499
Iteration 207, loss = 0.04355576
Iteration 208, loss = 0.04475968
Iteration 209, loss = 0.04781600
Iteration 210, loss = 0.04300000
Iteration 211, loss = 0.04673261
Iteration 212, loss = 0.04367860
Iteration 213, loss = 0.04552278
Iteration 214, loss = 0.04335256
Iteration 215, loss = 0.04819447
Iteration 216, loss = 0.04659374
Iteration 217, loss = 0.04524768
Iteration 218, loss = 0.04517170
Iteration 219, loss = 0.04337872
Iteration 220, loss = 0.04351429
Iteration 221, loss = 0.04389120
Iteration 222, loss = 0.04160999
Iteration 223, loss = 0.04381367
Iteration 224, loss = 0.04395331
Iteration 225, loss = 0.04281254
Iteration 226, loss = 0.04204193
Iteration 227, loss = 0.04309967
Iteration 228, loss = 0.04229821
Iteration 229, loss = 0.04206116
Iteration 230, loss = 0.04129125
Iteration 231, loss = 0.04136316
Iteration 232, loss = 0.04198572
Iteration 233, loss = 0.04342388
Iteration 234, loss = 0.04892345
Iteration 235, loss = 0.04674683
Iteration 236, loss = 0.04832456
Iteration 237, loss = 0.04618828
Iteration 238, loss = 0.04754206
Iteration 239, loss = 0.04281207
Iteration 240, loss = 0.04329138
Iteration 241, loss = 0.04156151
Iteration 242, loss = 0.04121601
Iteration 243, loss = 0.04111302
Iteration 244, loss = 0.04156150
Iteration 245, loss = 0.04245619
Iteration 246, loss = 0.04169280
Iteration 247, loss = 0.04186209
Iteration 248, loss = 0.04386264
Iteration 249, loss = 0.04295829
Iteration 250, loss = 0.04225229
Iteration 251, loss = 0.04133373
Iteration 252, loss = 0.04131486
Iteration 253, loss = 0.04242117
Iteration 254, loss = 0.04116514
Iteration 255, loss = 0.04242476
Iteration 256, loss = 0.04295176
Iteration 257, loss = 0.04470573
Iteration 258, loss = 0.04546001
Iteration 259, loss = 0.04182177
Iteration 260, loss = 0.04235623
Iteration 261, loss = 0.04478752
Iteration 262, loss = 0.04344036
Iteration 263, loss = 0.04481184
Iteration 264, loss = 0.04363060
Iteration 265, loss = 0.04438647
Iteration 266, loss = 0.04797350
Iteration 267, loss = 0.04338740
Iteration 268, loss = 0.04636083
Iteration 269, loss = 0.04417381
Iteration 270, loss = 0.04179841
Iteration 271, loss = 0.04130556
Iteration 272, loss = 0.04387132
Iteration 273, loss = 0.04427335
Iteration 274, loss = 0.04822917
Iteration 275, loss = 0.04498840
Iteration 276, loss = 0.04521869
Iteration 277, loss = 0.04775076
Iteration 278, loss = 0.04429574
Iteration 279, loss = 0.04431421
Iteration 280, loss = 0.05377120
Iteration 281, loss = 0.04651298
Iteration 282, loss = 0.05318813
Iteration 283, loss = 0.04877328
Iteration 284, loss = 0.04702415
Iteration 285, loss = 0.04491151
Iteration 286, loss = 0.04553422
Iteration 287, loss = 0.04307753
Iteration 288, loss = 0.04362936
Iteration 289, loss = 0.04222539
Iteration 290, loss = 0.04302559
Iteration 291, loss = 0.04307671
Iteration 292, loss = 0.04119883
Iteration 293, loss = 0.04120143
Iteration 294, loss = 0.04282423
Iteration 295, loss = 0.04451798
Iteration 296, loss = 0.04300232
Iteration 297, loss = 0.04182749
Iteration 298, loss = 0.04145328
Iteration 299, loss = 0.04162862
Iteration 300, loss = 0.04104737
Iteration 301, loss = 0.04268173
Iteration 302, loss = 0.04388725
Iteration 303, loss = 0.05649862
Iteration 304, loss = 0.04694850
Iteration 305, loss = 0.04488072
Iteration 306, loss = 0.04152092
Iteration 307, loss = 0.04237230
Iteration 308, loss = 0.04119227
Iteration 309, loss = 0.04159715
Iteration 310, loss = 0.04145121
Iteration 311, loss = 0.04236217
Iteration 312, loss = 0.04359251
Iteration 313, loss = 0.04161421
Iteration 314, loss = 0.04158757
Iteration 315, loss = 0.04423872
Iteration 316, loss = 0.04503052
Iteration 317, loss = 0.04566231
Iteration 318, loss = 0.04374771
Iteration 319, loss = 0.04219714
Iteration 320, loss = 0.04203756
Iteration 321, loss = 0.04418464
Iteration 322, loss = 0.04161167
Iteration 323, loss = 0.04372428
Iteration 324, loss = 0.04260210
Iteration 325, loss = 0.04181506
Iteration 326, loss = 0.04495826
Iteration 327, loss = 0.04325451
Iteration 328, loss = 0.04447396
Iteration 329, loss = 0.04207424
Iteration 330, loss = 0.04199821
Iteration 331, loss = 0.04257770
Iteration 332, loss = 0.04192669
Iteration 333, loss = 0.04071908
Iteration 334, loss = 0.04075875
Iteration 335, loss = 0.04110795
Iteration 336, loss = 0.04225074
Iteration 337, loss = 0.04237131
Iteration 338, loss = 0.04244729
Iteration 339, loss = 0.04345320
Iteration 340, loss = 0.04100084
Iteration 341, loss = 0.04282752
Iteration 342, loss = 0.04212162
Iteration 343, loss = 0.04222911
Iteration 344, loss = 0.04273440
Iteration 345, loss = 0.04084143
Iteration 346, loss = 0.04091357
Iteration 347, loss = 0.04321390
Iteration 348, loss = 0.04141792
Iteration 349, loss = 0.04176241
Iteration 350, loss = 0.04326455
Iteration 351, loss = 0.04351446
Iteration 352, loss = 0.04466434
Iteration 353, loss = 0.04266766
Iteration 354, loss = 0.05006309
Iteration 355, loss = 0.05353530
Iteration 356, loss = 0.06754282
Iteration 357, loss = 0.04886430
Iteration 358, loss = 0.04302635
Iteration 359, loss = 0.04570540
Iteration 360, loss = 0.04730921
Iteration 361, loss = 0.04331020
Iteration 362, loss = 0.04363811
Iteration 363, loss = 0.04455423
Iteration 364, loss = 0.04242043
Iteration 365, loss = 0.04175361
Iteration 366, loss = 0.04278827
Iteration 367, loss = 0.04054008
Iteration 368, loss = 0.04137793
Iteration 369, loss = 0.04099112
Iteration 370, loss = 0.04386930
Iteration 371, loss = 0.04187989
Iteration 372, loss = 0.04275701
Iteration 373, loss = 0.04448225
Iteration 374, loss = 0.04186190
Iteration 375, loss = 0.04324620
Iteration 376, loss = 0.04113640
Iteration 377, loss = 0.04175971
Iteration 378, loss = 0.04268745
Iteration 379, loss = 0.04215674
Iteration 380, loss = 0.04188885
Iteration 381, loss = 0.04079491
Iteration 382, loss = 0.04131660
Iteration 383, loss = 0.04542150
Iteration 384, loss = 0.04493755
Iteration 385, loss = 0.04150630
Iteration 386, loss = 0.04352697
Iteration 387, loss = 0.04216758
Iteration 388, loss = 0.04237957
Iteration 389, loss = 0.04263025
Iteration 390, loss = 0.04527549
Iteration 391, loss = 0.04410242
Iteration 392, loss = 0.04180618
Iteration 393, loss = 0.04522995
Iteration 394, loss = 0.04158684
Iteration 395, loss = 0.04117779
Iteration 396, loss = 0.04532015
Iteration 397, loss = 0.04432214
Iteration 398, loss = 0.04231322
Iteration 399, loss = 0.04099328
Iteration 400, loss = 0.04263502
Iteration 401, loss = 0.04194010
Iteration 402, loss = 0.04365073
Iteration 403, loss = 0.04468345
Iteration 404, loss = 0.04254871
Iteration 405, loss = 0.04073757
Iteration 406, loss = 0.04088757
Iteration 407, loss = 0.04673047
Iteration 408, loss = 0.04377236
Iteration 409, loss = 0.04392001
Iteration 410, loss = 0.04256395
Iteration 411, loss = 0.04394862
Iteration 412, loss = 0.04327898
Iteration 413, loss = 0.04584838
Iteration 414, loss = 0.04281948
Iteration 415, loss = 0.04156716
Iteration 416, loss = 0.04425255
Iteration 417, loss = 0.04120422
Iteration 418, loss = 0.04123078
Iteration 419, loss = 0.04336910
Iteration 420, loss = 0.04183585
Iteration 421, loss = 0.04132656
Iteration 422, loss = 0.04161417
Iteration 423, loss = 0.04207464
Iteration 424, loss = 0.04228995
Iteration 425, loss = 0.04279633
Iteration 426, loss = 0.04427867
Iteration 427, loss = 0.04186523
Iteration 428, loss = 0.04542507
Iteration 429, loss = 0.04275074
Iteration 430, loss = 0.04258804
Iteration 431, loss = 0.04139651
Iteration 432, loss = 0.04257551
Iteration 433, loss = 0.04148426
Iteration 434, loss = 0.04108660
Iteration 435, loss = 0.04366098
Iteration 436, loss = 0.04301418
Iteration 437, loss = 0.04178191
Iteration 438, loss = 0.04119360
Iteration 439, loss = 0.04082383
Iteration 440, loss = 0.04323105
Iteration 441, loss = 0.04169599
Iteration 442, loss = 0.04350648
Iteration 443, loss = 0.04520398
Iteration 444, loss = 0.04796367
Iteration 445, loss = 0.04560643
Iteration 446, loss = 0.05010853
Iteration 447, loss = 0.05082529
Iteration 448, loss = 0.05057734
Iteration 449, loss = 0.05798858
Iteration 450, loss = 0.05313836
Iteration 451, loss = 0.04942257
Iteration 452, loss = 0.04856583
Iteration 453, loss = 0.04577926
Iteration 454, loss = 0.04266425
Iteration 455, loss = 0.04196798
Iteration 456, loss = 0.04406588
Iteration 457, loss = 0.04179738
Iteration 458, loss = 0.04427856
Iteration 459, loss = 0.04095560
Iteration 460, loss = 0.04257029
Iteration 461, loss = 0.04312359
Iteration 462, loss = 0.04022128
Iteration 463, loss = 0.04223729
Iteration 464, loss = 0.04531588
Iteration 465, loss = 0.04591858
Iteration 466, loss = 0.04143288
Iteration 467, loss = 0.04097766
Iteration 468, loss = 0.04174691
Iteration 469, loss = 0.04157827
Iteration 470, loss = 0.04196506
Iteration 471, loss = 0.04045129
Iteration 472, loss = 0.04078538
Iteration 473, loss = 0.04033512
Iteration 474, loss = 0.04055117
Iteration 475, loss = 0.04024894
Iteration 476, loss = 0.04226537
Iteration 477, loss = 0.04314843
Iteration 478, loss = 0.04336442
Iteration 479, loss = 0.04246202
Iteration 480, loss = 0.04201991
Iteration 481, loss = 0.04175557
Iteration 482, loss = 0.04327781
Iteration 483, loss = 0.04858434
Iteration 484, loss = 0.04516443
Iteration 485, loss = 0.04292280
Iteration 486, loss = 0.04146417
Iteration 487, loss = 0.04351856
Iteration 488, loss = 0.04173331
Iteration 489, loss = 0.04524512
Iteration 490, loss = 0.04503277
Iteration 491, loss = 0.04204918
Iteration 492, loss = 0.04085001
Iteration 493, loss = 0.04043416
Iteration 494, loss = 0.04074283
Iteration 495, loss = 0.04173631
Iteration 496, loss = 0.04415518
Iteration 497, loss = 0.04236498
Iteration 498, loss = 0.04231817
Iteration 499, loss = 0.04448329
Iteration 500, loss = 0.04311539
Iteration 501, loss = 0.04405156
Iteration 502, loss = 0.04743624
Iteration 503, loss = 0.05892612
Iteration 504, loss = 0.05311955
Iteration 505, loss = 0.04664836
Iteration 506, loss = 0.04349849
Iteration 507, loss = 0.04252697
Iteration 508, loss = 0.04263435
Iteration 509, loss = 0.04126834
Iteration 510, loss = 0.04251821
Iteration 511, loss = 0.03986080
Iteration 512, loss = 0.04218655
Iteration 513, loss = 0.04034477
Iteration 514, loss = 0.04145255
Iteration 515, loss = 0.04112647
Iteration 516, loss = 0.04027744
Iteration 517, loss = 0.04177087
Iteration 518, loss = 0.04047496
Iteration 519, loss = 0.04100609
Iteration 520, loss = 0.04081698
Iteration 521, loss = 0.04146123
Iteration 522, loss = 0.04230585
Iteration 523, loss = 0.04101378
Iteration 524, loss = 0.04180170
Iteration 525, loss = 0.04398250
Iteration 526, loss = 0.04478912
Iteration 527, loss = 0.04879705
Iteration 528, loss = 0.05678205
Iteration 529, loss = 0.04944904
Iteration 530, loss = 0.04756920
Iteration 531, loss = 0.04916832
Iteration 532, loss = 0.04661081
Iteration 533, loss = 0.04543326
Iteration 534, loss = 0.04461960
Iteration 535, loss = 0.04412436
Iteration 536, loss = 0.04228364
Iteration 537, loss = 0.04134585
Iteration 538, loss = 0.04480994
Iteration 539, loss = 0.04412132
Iteration 540, loss = 0.04405292
Iteration 541, loss = 0.04325982
Iteration 542, loss = 0.04301776
Iteration 543, loss = 0.04459439
Iteration 544, loss = 0.04174095
Iteration 545, loss = 0.04099492
Iteration 546, loss = 0.04185311
Iteration 547, loss = 0.04252795
Iteration 548, loss = 0.04638014
Iteration 549, loss = 0.04386657
Iteration 550, loss = 0.04884981
Iteration 551, loss = 0.04370367
Iteration 552, loss = 0.04780164
Iteration 553, loss = 0.04578031
Iteration 554, loss = 0.04523880
Iteration 555, loss = 0.04622412
Iteration 556, loss = 0.04868314
Iteration 557, loss = 0.04616408
Iteration 558, loss = 0.04300192
Iteration 559, loss = 0.04112769
Iteration 560, loss = 0.04112379
Iteration 561, loss = 0.04048656
Iteration 562, loss = 0.04032083
Iteration 563, loss = 0.04074132
Iteration 564, loss = 0.04141084
Iteration 565, loss = 0.04101123
Iteration 566, loss = 0.04079051
Iteration 567, loss = 0.04146607
Iteration 568, loss = 0.04494480
Iteration 569, loss = 0.04969804
Iteration 570, loss = 0.04858213
Iteration 571, loss = 0.04603935
Iteration 572, loss = 0.04397457
Iteration 573, loss = 0.04371355
Iteration 574, loss = 0.04124140
Iteration 575, loss = 0.04280242
Iteration 576, loss = 0.04158751
Iteration 577, loss = 0.04558647
Iteration 578, loss = 0.04778135
Iteration 579, loss = 0.05145691
Iteration 580, loss = 0.04343638
Iteration 581, loss = 0.04922143
Iteration 582, loss = 0.05083952
Iteration 583, loss = 0.04357278
Iteration 584, loss = 0.04202124
Iteration 585, loss = 0.04297727
Iteration 586, loss = 0.04299964
Iteration 587, loss = 0.04049984
Iteration 588, loss = 0.04009726
Iteration 589, loss = 0.04042422
Iteration 590, loss = 0.04268203
Iteration 591, loss = 0.04157923
Iteration 592, loss = 0.04048344
Iteration 593, loss = 0.04121139
Iteration 594, loss = 0.04094591
Iteration 595, loss = 0.04009448
Iteration 596, loss = 0.04058772
Iteration 597, loss = 0.04061046
Iteration 598, loss = 0.04062133
Iteration 599, loss = 0.04183997
Iteration 600, loss = 0.04122009
Iteration 601, loss = 0.04519569
Iteration 602, loss = 0.04658092
Iteration 603, loss = 0.04587226
Iteration 604, loss = 0.05107371
Iteration 605, loss = 0.04843287
Iteration 606, loss = 0.05033111
Iteration 607, loss = 0.05546097
Iteration 608, loss = 0.04383562
Iteration 609, loss = 0.04477727
Iteration 610, loss = 0.04407198
Iteration 611, loss = 0.04337107
Iteration 612, loss = 0.04153710
Iteration 613, loss = 0.04483397
Iteration 614, loss = 0.04893332
Iteration 615, loss = 0.04602219
Iteration 616, loss = 0.04327349
Iteration 617, loss = 0.04086293
Iteration 618, loss = 0.04070061
Iteration 619, loss = 0.04008082
Iteration 620, loss = 0.04030766
Iteration 621, loss = 0.03999212
Iteration 622, loss = 0.04006229
Iteration 623, loss = 0.04042844
Iteration 624, loss = 0.04023620
Iteration 625, loss = 0.04020158
Iteration 626, loss = 0.04124405
Iteration 627, loss = 0.04095729
Iteration 628, loss = 0.04380124
Iteration 629, loss = 0.04488161
Iteration 630, loss = 0.04138309
Iteration 631, loss = 0.04400181
Iteration 632, loss = 0.04225811
Iteration 633, loss = 0.04336345
Iteration 634, loss = 0.04614975
Iteration 635, loss = 0.04271733
Iteration 636, loss = 0.04400983
Iteration 637, loss = 0.04127692
Iteration 638, loss = 0.04001721
Iteration 639, loss = 0.04020092
Iteration 640, loss = 0.04079467
Iteration 641, loss = 0.03994465
Iteration 642, loss = 0.04065108
Iteration 643, loss = 0.04016013
Iteration 644, loss = 0.04269293
Iteration 645, loss = 0.04290419
Iteration 646, loss = 0.04201689
Iteration 647, loss = 0.04248164
Iteration 648, loss = 0.04376294
Iteration 649, loss = 0.04229230
Iteration 650, loss = 0.04401134
Iteration 651, loss = 0.04194007
Iteration 652, loss = 0.04323093
Iteration 653, loss = 0.04017390
Iteration 654, loss = 0.04025385
Iteration 655, loss = 0.04005107
Iteration 656, loss = 0.03994276
Iteration 657, loss = 0.04064463
Iteration 658, loss = 0.04056362
Iteration 659, loss = 0.04130434
Iteration 660, loss = 0.03980212
Iteration 661, loss = 0.04236924
Iteration 662, loss = 0.04089243
Iteration 663, loss = 0.04158715
Iteration 664, loss = 0.04204881
Iteration 665, loss = 0.04050610
Iteration 666, loss = 0.04090996
Iteration 667, loss = 0.04023522
Iteration 668, loss = 0.04025381
Iteration 669, loss = 0.04065251
Iteration 670, loss = 0.04023982
Iteration 671, loss = 0.03989330
Iteration 672, loss = 0.03984599
Iteration 673, loss = 0.04057103
Iteration 674, loss = 0.04079244
Iteration 675, loss = 0.04032425
Iteration 676, loss = 0.03989730
Iteration 677, loss = 0.04016016
Iteration 678, loss = 0.04019147
Iteration 679, loss = 0.04072356
Iteration 680, loss = 0.04065704
Iteration 681, loss = 0.04199184
Iteration 682, loss = 0.04453636
Iteration 683, loss = 0.04616434
Iteration 684, loss = 0.04785614
Iteration 685, loss = 0.04208569
Iteration 686, loss = 0.04362408
Iteration 687, loss = 0.04752496
Iteration 688, loss = 0.05637325
Iteration 689, loss = 0.04700945
Iteration 690, loss = 0.04717599
Iteration 691, loss = 0.04691881
Iteration 692, loss = 0.04620151
Iteration 693, loss = 0.04727681
Iteration 694, loss = 0.04723735
Iteration 695, loss = 0.04107736
Iteration 696, loss = 0.04486229
Iteration 697, loss = 0.04154839
Iteration 698, loss = 0.04351279
Iteration 699, loss = 0.04681695
Iteration 700, loss = 0.04290467
Iteration 701, loss = 0.04329409
Iteration 702, loss = 0.04178775
Iteration 703, loss = 0.04134019
Iteration 704, loss = 0.04052135
Iteration 705, loss = 0.04061848
Iteration 706, loss = 0.04003461
Iteration 707, loss = 0.04024132
Iteration 708, loss = 0.04199037
Iteration 709, loss = 0.04135707
Iteration 710, loss = 0.04070180
Iteration 711, loss = 0.04263069
Iteration 712, loss = 0.03991149
Iteration 713, loss = 0.04070384
Iteration 714, loss = 0.04137169
Iteration 715, loss = 0.04057438
Iteration 716, loss = 0.04068571
Iteration 717, loss = 0.04023940
Iteration 718, loss = 0.03970123
Iteration 719, loss = 0.04111145
Iteration 720, loss = 0.04009594
Iteration 721, loss = 0.04384736
Iteration 722, loss = 0.04133236
Iteration 723, loss = 0.03993146
Iteration 724, loss = 0.04040122
Iteration 725, loss = 0.04224101
Iteration 726, loss = 0.04044654
Iteration 727, loss = 0.04035259
Iteration 728, loss = 0.04222918
Iteration 729, loss = 0.04169013
Iteration 730, loss = 0.04697633
Iteration 731, loss = 0.04083329
Iteration 732, loss = 0.03997900
Iteration 733, loss = 0.04036170
Iteration 734, loss = 0.03990475
Iteration 735, loss = 0.03997153
Iteration 736, loss = 0.04321986
Iteration 737, loss = 0.04757080
Iteration 738, loss = 0.05316595
Iteration 739, loss = 0.05087910
Iteration 740, loss = 0.05035569
Iteration 741, loss = 0.04697810
Iteration 742, loss = 0.04225467
Iteration 743, loss = 0.04651951
Iteration 744, loss = 0.05379830
Iteration 745, loss = 0.05197936
Iteration 746, loss = 0.06186108
Iteration 747, loss = 0.04767125
Iteration 748, loss = 0.05452376
Iteration 749, loss = 0.04910658
Iteration 750, loss = 0.04540809
Iteration 751, loss = 0.04491070
Iteration 752, loss = 0.04181361
Iteration 753, loss = 0.04033320
Iteration 754, loss = 0.04134800
Iteration 755, loss = 0.04071723
Iteration 756, loss = 0.04223549
Iteration 757, loss = 0.04147228
Iteration 758, loss = 0.04210114
Iteration 759, loss = 0.04082287
Iteration 760, loss = 0.04056074
Iteration 761, loss = 0.04083155
Iteration 762, loss = 0.04040049
Iteration 763, loss = 0.04277610
Iteration 764, loss = 0.04023672
Iteration 765, loss = 0.04001460
Iteration 766, loss = 0.04474598
Iteration 767, loss = 0.04165396
Iteration 768, loss = 0.04004487
Iteration 769, loss = 0.04090853
Iteration 770, loss = 0.04065744
Iteration 771, loss = 0.04022209
Iteration 772, loss = 0.04282870
Iteration 773, loss = 0.04206643
Iteration 774, loss = 0.04418175
Iteration 775, loss = 0.04067733
Iteration 776, loss = 0.03984883
Iteration 777, loss = 0.04013215
Iteration 778, loss = 0.04157788
Iteration 779, loss = 0.04190293
Iteration 780, loss = 0.04584506
Iteration 781, loss = 0.04277067
Iteration 782, loss = 0.04614935
Iteration 783, loss = 0.04740381
Iteration 784, loss = 0.05298440
Iteration 785, loss = 0.04831510
Iteration 786, loss = 0.04549200
Iteration 787, loss = 0.04870110
Iteration 788, loss = 0.04795126
Iteration 789, loss = 0.05297894
Iteration 790, loss = 0.04642494
Iteration 791, loss = 0.04379987
Iteration 792, loss = 0.04136496
Iteration 793, loss = 0.04194848
Iteration 794, loss = 0.04248306
Iteration 795, loss = 0.04174417
Iteration 796, loss = 0.04180104
Iteration 797, loss = 0.03955675
Iteration 798, loss = 0.04169875
Iteration 799, loss = 0.04283461
Iteration 800, loss = 0.04158435
Iteration 801, loss = 0.04361379
Iteration 802, loss = 0.04484814
Iteration 803, loss = 0.04271184
Iteration 804, loss = 0.04628483
Iteration 805, loss = 0.04582238
Iteration 806, loss = 0.04083443
Iteration 807, loss = 0.04047498
Iteration 808, loss = 0.04226960
Iteration 809, loss = 0.04314849
Iteration 810, loss = 0.04290470
Iteration 811, loss = 0.04001220
Iteration 812, loss = 0.03982239
Iteration 813, loss = 0.03881750
Iteration 814, loss = 0.03950654
Iteration 815, loss = 0.04204846
Iteration 816, loss = 0.04020607
Iteration 817, loss = 0.03889480
Iteration 818, loss = 0.04063828
Iteration 819, loss = 0.03915616
Iteration 820, loss = 0.03985769
Iteration 821, loss = 0.03908405
Iteration 822, loss = 0.03878001
Iteration 823, loss = 0.03915646
Iteration 824, loss = 0.04099549
Iteration 825, loss = 0.04113886
Iteration 826, loss = 0.04350340
Iteration 827, loss = 0.04493103
Iteration 828, loss = 0.05155200
Iteration 829, loss = 0.04567622
Iteration 830, loss = 0.04925979
Iteration 831, loss = 0.04489647
Iteration 832, loss = 0.04752312
Iteration 833, loss = 0.04234548
Iteration 834, loss = 0.03969266
Iteration 835, loss = 0.03942394
Iteration 836, loss = 0.04493343
Iteration 837, loss = 0.04338775
Iteration 838, loss = 0.04139422
Iteration 839, loss = 0.03986161
Iteration 840, loss = 0.03883271
Iteration 841, loss = 0.04089078
Iteration 842, loss = 0.04005261
Iteration 843, loss = 0.03892944
Iteration 844, loss = 0.03945184
Iteration 845, loss = 0.04071249
Iteration 846, loss = 0.04552908
Iteration 847, loss = 0.04140990
Iteration 848, loss = 0.04337929
Iteration 849, loss = 0.03963733
Iteration 850, loss = 0.03978072
Iteration 851, loss = 0.04102386
Iteration 852, loss = 0.04119148
Iteration 853, loss = 0.04135364
Iteration 854, loss = 0.04294051
Iteration 855, loss = 0.04024439
Iteration 856, loss = 0.04409224
Iteration 857, loss = 0.04524677
Iteration 858, loss = 0.04639658
Iteration 859, loss = 0.05471313
Iteration 860, loss = 0.05030475
Iteration 861, loss = 0.06593753
Iteration 862, loss = 0.05207040
Iteration 863, loss = 0.04775038
Iteration 864, loss = 0.04771046
Iteration 865, loss = 0.04715261
Iteration 866, loss = 0.05055911
Iteration 867, loss = 0.05040748
Iteration 868, loss = 0.04842208
Iteration 869, loss = 0.04921946
Iteration 870, loss = 0.04946494
Iteration 871, loss = 0.04912315
Iteration 872, loss = 0.05985978
Iteration 873, loss = 0.05632311
Iteration 874, loss = 0.05320262
Iteration 875, loss = 0.04837812
Iteration 876, loss = 0.04207477
Iteration 877, loss = 0.04380306
Iteration 878, loss = 0.04536216
Iteration 879, loss = 0.04000589
Iteration 880, loss = 0.04244715
Iteration 881, loss = 0.04165594
Iteration 882, loss = 0.05155793
Iteration 883, loss = 0.04246106
Iteration 884, loss = 0.04195280
Iteration 885, loss = 0.04345001
Iteration 886, loss = 0.04645302
Iteration 887, loss = 0.04147857
Iteration 888, loss = 0.04870248
Iteration 889, loss = 0.04628152
Iteration 890, loss = 0.04723738
Iteration 891, loss = 0.03986329
Iteration 892, loss = 0.03856374
Iteration 893, loss = 0.03859211
Iteration 894, loss = 0.03910993
Iteration 895, loss = 0.03968393
Iteration 896, loss = 0.04045883
Iteration 897, loss = 0.03877849
Iteration 898, loss = 0.04255730
Iteration 899, loss = 0.04101059
Iteration 900, loss = 0.04298355
Iteration 901, loss = 0.03957224
Iteration 902, loss = 0.03877486
Iteration 903, loss = 0.04139374
Iteration 904, loss = 0.04070925
Iteration 905, loss = 0.04024314
Iteration 906, loss = 0.03935006
Iteration 907, loss = 0.04429041
Iteration 908, loss = 0.04479305
Iteration 909, loss = 0.04552836
Iteration 910, loss = 0.04509801
Iteration 911, loss = 0.05099116
Iteration 912, loss = 0.04796643
Iteration 913, loss = 0.04268810
Iteration 914, loss = 0.04116932
Iteration 915, loss = 0.03910495
Iteration 916, loss = 0.04125622
Iteration 917, loss = 0.04219892
Iteration 918, loss = 0.04415313
Iteration 919, loss = 0.03841649
Iteration 920, loss = 0.05189186
Iteration 921, loss = 0.04484209
Iteration 922, loss = 0.04020039
Iteration 923, loss = 0.03912047
Iteration 924, loss = 0.03933129
Iteration 925, loss = 0.03901603
Iteration 926, loss = 0.04105883
Iteration 927, loss = 0.03853402
Iteration 928, loss = 0.03967627
Iteration 929, loss = 0.04186273
Iteration 930, loss = 0.03943613
Iteration 931, loss = 0.04765547
Iteration 932, loss = 0.04757673
Iteration 933, loss = 0.04317511
Iteration 934, loss = 0.04061147
Iteration 935, loss = 0.04340811
Iteration 936, loss = 0.04062399
Iteration 937, loss = 0.04231707
Iteration 938, loss = 0.05726616
Iteration 939, loss = 0.04809732
Iteration 940, loss = 0.04860118
Iteration 941, loss = 0.04112081
Iteration 942, loss = 0.04471251
Iteration 943, loss = 0.04788045
Iteration 944, loss = 0.04219822
Iteration 945, loss = 0.04920575
Iteration 946, loss = 0.05051631
Iteration 947, loss = 0.04164602
Iteration 948, loss = 0.04377439
Iteration 949, loss = 0.04135752
Iteration 950, loss = 0.03874009
Iteration 951, loss = 0.04135365
Iteration 952, loss = 0.04627513
Iteration 953, loss = 0.04031116
Iteration 954, loss = 0.03895134
Iteration 955, loss = 0.03859394
Iteration 956, loss = 0.04050365
Iteration 957, loss = 0.04233429
Iteration 958, loss = 0.04226498
Iteration 959, loss = 0.03889936
Iteration 960, loss = 0.03935276
Iteration 961, loss = 0.03975756
Iteration 962, loss = 0.04699296
Iteration 963, loss = 0.04381447
Iteration 964, loss = 0.04274844
Iteration 965, loss = 0.05106811
Iteration 966, loss = 0.04538602
Iteration 967, loss = 0.05416902
Iteration 968, loss = 0.05214858
Iteration 969, loss = 0.05172118
Iteration 970, loss = 0.04494569
Iteration 971, loss = 0.04346633
Iteration 972, loss = 0.04021977
Iteration 973, loss = 0.03891404
Iteration 974, loss = 0.04029116
Iteration 975, loss = 0.04433151
Iteration 976, loss = 0.04161895
Iteration 977, loss = 0.03971329
Iteration 978, loss = 0.04201910
Iteration 979, loss = 0.04309189
Iteration 980, loss = 0.04607017
Iteration 981, loss = 0.04382373
Iteration 982, loss = 0.04480570
Iteration 983, loss = 0.04876713
Iteration 984, loss = 0.04891191
Iteration 985, loss = 0.04379649
Iteration 986, loss = 0.04628336
Iteration 987, loss = 0.04380432
Iteration 988, loss = 0.03973838
Iteration 989, loss = 0.03863749
Iteration 990, loss = 0.03834371
Iteration 991, loss = 0.03906976
Iteration 992, loss = 0.04141524
Iteration 993, loss = 0.04380437
Iteration 994, loss = 0.05190208
Iteration 995, loss = 0.05643950
Iteration 996, loss = 0.05645833
Iteration 997, loss = 0.05283288
Iteration 998, loss = 0.04876043
Iteration 999, loss = 0.05212454
Iteration 1000, loss = 0.04388266
*** fcst ***
[[ 0.506901    0.4371648   0.45025448]
 [ 0.50012847  0.45019982  0.46413936]
 [ 0.49335594  0.46323484  0.47802423]
 [ 0.49520271  0.47608574  0.49190911]
 [ 0.49937376  0.48048246  0.50579398]
 [ 0.50354481  0.48497546  0.51967886]
 [ 0.50771587  0.4897719   0.53356373]
 [ 0.51188692  0.49456834  0.54744861]
 [ 0.51605798  0.49936478  0.56133348]
 [ 0.30850315  0.22793717  0.3230713 ]
 [ 0.33689089  0.25729983  0.34181728]
 [ 0.36527863  0.28666248  0.36056326]
 [ 0.39366637  0.31602513  0.37930924]
 [ 0.42205411  0.34538778  0.39805522]
 [ 0.45044185  0.37475043  0.4168012 ]
 [ 0.47882959  0.39732041  0.43554718]
 [ 0.50721733  0.41912325  0.45429316]
 [ 0.53560507  0.44092609  0.47303914]
 [ 0.56399281  0.46272893  0.49178512]
 [ 0.59238055  0.48453177  0.50869341]
 [ 0.60216997  0.49814637  0.52337541]
 [ 0.59539743  0.50874172  0.53726029]
 [ 0.5886249   0.51933708  0.55114516]
 [ 0.58185236  0.52993243  0.56503004]
 [ 0.57507983  0.54014807  0.57891491]
 [ 0.5683073   0.5437525   0.59279979]
 [ 0.56153476  0.54302882  0.60668466]
 [ 0.55476223  0.54230513  0.62056954]
 [ 0.55456141  0.54223491  0.63104251]
 [ 0.55873246  0.54393925  0.62527276]
 [ 0.56290352  0.54597705  0.619503  ]
 [ 0.56707457  0.5507735   0.61373324]
 [ 0.57124563  0.55556994  0.60796348]
 [ 0.37350415  0.33828867  0.45458205]
 [ 0.40189189  0.38681762  0.47332802]
 [ 0.43027963  0.43534657  0.492074  ]
 [ 0.45866737  0.48387552  0.51081998]
 [ 0.48705511  0.53240448  0.52956596]
 [ 0.51544285  0.58093343  0.54831194]
 [ 0.54383059  0.62946238  0.56705792]
 [ 0.57221833  0.67799133  0.5858039 ]
 [ 0.60060607  0.72652028  0.60454988]
 [ 0.62899381  0.77447364  0.62329586]
 [ 0.65738155  0.71409     0.63815097]
 [ 0.66787233  0.64508409  0.65203584]
 [ 0.66357626  0.61193294  0.66250404]
 [ 0.65680372  0.61717311  0.65673428]
 [ 0.65003119  0.61644942  0.65096452]
 [ 0.64325866  0.61572574  0.64519476]
 [ 0.63648612  0.61500205  0.639425  ]
 [ 0.62971359  0.61427837  0.63365524]
 [ 0.62294105  0.61355468  0.62788548]
 [ 0.61616852  0.612831    0.62211573]
 [ 0.61392011  0.61210731  0.61634597]
 [ 0.61809117  0.61138363  0.61057621]
 [ 0.62226222  0.61065994  0.60480645]
 [ 0.62643328  0.61177509  0.59903669]
 [-0.01315351  0.09671151  0.03667779]
 [ 0.01523423  0.11397429  0.04987442]
 [ 0.04362197  0.13123707  0.06307105]
 [ 0.07200971  0.14895868  0.07626769]
 [ 0.10039745  0.16876309  0.08946432]
 [ 0.12878519  0.18856749  0.10266095]
 [ 0.15717293  0.2083719   0.11585758]
 [ 0.18556067  0.2281763   0.12905421]
 [ 0.20532022  0.24798071  0.14225085]
 [ 0.22283353  0.26372206  0.15544748]
 [ 0.24034684  0.27620611  0.16864411]
 [ 0.25786014  0.28869016  0.18184074]
 [ 0.26465676  0.29691506  0.19503737]
 [ 0.26143419  0.29817772  0.20823401]
 [ 0.26560524  0.29944038  0.22085206]
 [ 0.2697763   0.30249641  0.22990229]
 [ 0.27394735  0.30700429  0.23895253]
 [ 0.27811841  0.31151216  0.24800276]
 [ 0.28228946  0.31602003  0.25636835]
 [ 0.28646051  0.32052791  0.26457596]
 [ 0.29063157  0.32503578  0.27278356]
 [ 0.29480262  0.32954366  0.28099116]
 [ 0.29897368  0.33405153  0.28919876]
 [ 0.3017899   0.33855941  0.29740637]
 [ 0.05184749  0.11790237  0.08793791]
 [ 0.08023523  0.13516515  0.10113454]
 [ 0.10862297  0.15242793  0.11433117]
 [ 0.13701071  0.16969071  0.1275278 ]
 [ 0.16539845  0.18695349  0.14072443]
 [ 0.19378619  0.20421627  0.15392107]
 [ 0.22217393  0.22225034  0.1671177 ]
 [ 0.25056167  0.24205475  0.18031433]
 [ 0.2778222   0.26185915  0.19351096]
 [ 0.2953355   0.28166356  0.20670759]
 [ 0.31284881  0.30146796  0.21990423]
 [ 0.33036212  0.32127237  0.23310086]
 [ 0.33283559  0.3343529   0.24629749]
 [ 0.32606306  0.33993583  0.25935293]
 [ 0.32079289  0.34119849  0.26840316]
 [ 0.32496395  0.34246115  0.27745339]
 [ 0.329135    0.34524441  0.28650362]
 [ 0.33330606  0.34975229  0.2947206 ]
 [ 0.33747711  0.35426016  0.30292821]
 [ 0.34164816  0.35876804  0.31113581]]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.464773 2021-12-17 15:00:00          200
1    MALMEDY_CM_FT005.F.PV   0.471489 2021-12-17 16:00:00          200
2    MALMEDY_CM_FT005.F.PV   0.478205 2021-12-17 17:00:00          200
3    MALMEDY_CM_FT005.F.PV   0.487733 2021-12-17 18:00:00          200
4    MALMEDY_CM_FT005.F.PV   0.495217 2021-12-17 19:00:00          200
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.314959 2021-12-21 15:00:00          200
97   MALMEDY_CM_FT005.F.PV   0.320294 2021-12-21 16:00:00          200
98   MALMEDY_CM_FT005.F.PV   0.325926 2021-12-21 17:00:00          200
99   MALMEDY_CM_FT005.F.PV   0.331555 2021-12-21 18:00:00          200
100  MALMEDY_CM_FT005.F.PV   0.337184 2021-12-21 19:00:00          200

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT005.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.464773 2021-12-17 15:00:00          200
1    MALMEDY_CM_FT005.F.PV   0.471489 2021-12-17 16:00:00          200
2    MALMEDY_CM_FT005.F.PV   0.478205 2021-12-17 17:00:00          200
3    MALMEDY_CM_FT005.F.PV   0.487733 2021-12-17 18:00:00          200
4    MALMEDY_CM_FT005.F.PV   0.495217 2021-12-17 19:00:00          200
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.314959 2021-12-21 15:00:00          200
97   MALMEDY_CM_FT005.F.PV   0.320294 2021-12-21 16:00:00          200
98   MALMEDY_CM_FT005.F.PV   0.325926 2021-12-21 17:00:00          200
99   MALMEDY_CM_FT005.F.PV   0.331555 2021-12-21 18:00:00          200
100  MALMEDY_CM_FT005.F.PV   0.337184 2021-12-21 19:00:00          200

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.506901 2021-12-17 15:00:00          201
1    MALMEDY_CM_FT005.F.PV   0.500128 2021-12-17 16:00:00          201
2    MALMEDY_CM_FT005.F.PV   0.493356 2021-12-17 17:00:00          201
3    MALMEDY_CM_FT005.F.PV   0.495203 2021-12-17 18:00:00          201
4    MALMEDY_CM_FT005.F.PV   0.499374 2021-12-17 19:00:00          201
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.324964 2021-12-21 15:00:00          201
97   MALMEDY_CM_FT005.F.PV   0.329135 2021-12-21 16:00:00          201
98   MALMEDY_CM_FT005.F.PV   0.333306 2021-12-21 17:00:00          201
99   MALMEDY_CM_FT005.F.PV   0.337477 2021-12-21 18:00:00          201
100  MALMEDY_CM_FT005.F.PV   0.341648 2021-12-21 19:00:00          201

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT005.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.506901 2021-12-17 15:00:00          201
1    MALMEDY_CM_FT005.F.PV   0.500128 2021-12-17 16:00:00          201
2    MALMEDY_CM_FT005.F.PV   0.493356 2021-12-17 17:00:00          201
3    MALMEDY_CM_FT005.F.PV   0.495203 2021-12-17 18:00:00          201
4    MALMEDY_CM_FT005.F.PV   0.499374 2021-12-17 19:00:00          201
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.324964 2021-12-21 15:00:00          201
97   MALMEDY_CM_FT005.F.PV   0.329135 2021-12-21 16:00:00          201
98   MALMEDY_CM_FT005.F.PV   0.333306 2021-12-21 17:00:00          201
99   MALMEDY_CM_FT005.F.PV   0.337477 2021-12-21 18:00:00          201
100  MALMEDY_CM_FT005.F.PV   0.341648 2021-12-21 19:00:00          201

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.437165 2021-12-17 15:00:00          202
1    MALMEDY_CM_FT005.F.PV     0.4502 2021-12-17 16:00:00          202
2    MALMEDY_CM_FT005.F.PV   0.463235 2021-12-17 17:00:00          202
3    MALMEDY_CM_FT005.F.PV   0.476086 2021-12-17 18:00:00          202
4    MALMEDY_CM_FT005.F.PV   0.480482 2021-12-17 19:00:00          202
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.342461 2021-12-21 15:00:00          202
97   MALMEDY_CM_FT005.F.PV   0.345244 2021-12-21 16:00:00          202
98   MALMEDY_CM_FT005.F.PV   0.349752 2021-12-21 17:00:00          202
99   MALMEDY_CM_FT005.F.PV    0.35426 2021-12-21 18:00:00          202
100  MALMEDY_CM_FT005.F.PV   0.358768 2021-12-21 19:00:00          202

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT005.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.437165 2021-12-17 15:00:00          202
1    MALMEDY_CM_FT005.F.PV     0.4502 2021-12-17 16:00:00          202
2    MALMEDY_CM_FT005.F.PV   0.463235 2021-12-17 17:00:00          202
3    MALMEDY_CM_FT005.F.PV   0.476086 2021-12-17 18:00:00          202
4    MALMEDY_CM_FT005.F.PV   0.480482 2021-12-17 19:00:00          202
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.342461 2021-12-21 15:00:00          202
97   MALMEDY_CM_FT005.F.PV   0.345244 2021-12-21 16:00:00          202
98   MALMEDY_CM_FT005.F.PV   0.349752 2021-12-21 17:00:00          202
99   MALMEDY_CM_FT005.F.PV    0.35426 2021-12-21 18:00:00          202
100  MALMEDY_CM_FT005.F.PV   0.358768 2021-12-21 19:00:00          202

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.450254 2021-12-17 15:00:00          203
1    MALMEDY_CM_FT005.F.PV   0.464139 2021-12-17 16:00:00          203
2    MALMEDY_CM_FT005.F.PV   0.478024 2021-12-17 17:00:00          203
3    MALMEDY_CM_FT005.F.PV   0.491909 2021-12-17 18:00:00          203
4    MALMEDY_CM_FT005.F.PV   0.505794 2021-12-17 19:00:00          203
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.277453 2021-12-21 15:00:00          203
97   MALMEDY_CM_FT005.F.PV   0.286504 2021-12-21 16:00:00          203
98   MALMEDY_CM_FT005.F.PV   0.294721 2021-12-21 17:00:00          203
99   MALMEDY_CM_FT005.F.PV   0.302928 2021-12-21 18:00:00          203
100  MALMEDY_CM_FT005.F.PV   0.311136 2021-12-21 19:00:00          203

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT005.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT005.F.PV   0.450254 2021-12-17 15:00:00          203
1    MALMEDY_CM_FT005.F.PV   0.464139 2021-12-17 16:00:00          203
2    MALMEDY_CM_FT005.F.PV   0.478024 2021-12-17 17:00:00          203
3    MALMEDY_CM_FT005.F.PV   0.491909 2021-12-17 18:00:00          203
4    MALMEDY_CM_FT005.F.PV   0.505794 2021-12-17 19:00:00          203
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT005.F.PV   0.277453 2021-12-21 15:00:00          203
97   MALMEDY_CM_FT005.F.PV   0.286504 2021-12-21 16:00:00          203
98   MALMEDY_CM_FT005.F.PV   0.294721 2021-12-21 17:00:00          203
99   MALMEDY_CM_FT005.F.PV   0.302928 2021-12-21 18:00:00          203
100  MALMEDY_CM_FT005.F.PV   0.311136 2021-12-21 19:00:00          203

[101 rows x 4 columns]


7 cpt.inputdata MALMEDY_CM_FT003.F.PV cpt.ao 60.0 1440.0 30240.0 MLPDYNAMIC
DELETE FROM cpt.ao WHERE scadaid='MALMEDY_CM_FT003.F.PV'... done
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT003.F.PV' ORDER BY scadadatetime DESC... done
*** ldate, fdate ***
2021-12-17 14:15:00 2021-11-18 19:30:00
>>> Water demand prediction : TOF 2021-12-20 18:00:00
*** start_train, stop_train, start_pred, stop_pred, toftime ***
2021-11-26 15:00:00 2021-12-17 15:00:00 2021-12-17 15:00:00 2021-12-21 19:00:00 2021-12-20 18:00:00
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT003.F.PV' ORDER BY scadadatetime ASC, rnum ASC... done
*** df_in ***
        rnum       scadadatetime                scadaid  scadavalue scadaquality
0     333854 2021-11-18 19:30:00  MALMEDY_CM_FT003.F.PV       4.675         None
1     333885 2021-11-18 20:00:00  MALMEDY_CM_FT003.F.PV       4.580         None
2     333923 2021-11-18 20:00:00  MALMEDY_CM_FT003.F.PV       4.580         None
3     333925 2021-11-18 20:15:00  MALMEDY_CM_FT003.F.PV       4.580         None
4     333963 2021-11-18 20:15:00  MALMEDY_CM_FT003.F.PV       4.580         None
...      ...                 ...                    ...         ...          ...
6358  498790 2021-12-17 14:00:00  MALMEDY_CM_FT003.F.PV       3.838         None
6359  498798 2021-12-17 14:00:00  MALMEDY_CM_FT003.F.PV       3.838         None
6360  498831 2021-12-17 14:00:00  MALMEDY_CM_FT003.F.PV       3.838         None
6361  498855 2021-12-17 14:15:00  MALMEDY_CM_FT003.F.PV       3.838         None
6362  498872 2021-12-17 14:15:00  MALMEDY_CM_FT003.F.PV       3.838         None

[6363 rows x 5 columns]
*** X_train, y_train, df_X, df_y, X_pred ***
[[330. 330. 330. ... 351. 351. 351.]
 [  5.   5.   5. ...   5.   5.   5.]
 [ 15.  16.  17. ...  12.  13.  14.]] [3.491      3.30766667 3.8885     4.63133333 4.983      4.71366667
 3.6735     2.768      2.762      2.184      2.1085     2.033
 1.75766667 1.55866667 1.252      1.439      1.76966667 2.47983333
 5.204      6.398      6.2165     6.1935     6.986      5.46666667
 4.4175     4.128      4.26005556 4.39211111 4.52416667 4.65622222
 4.78827778 4.92033333 5.05238889 5.18444444 5.3165     5.44855556
 5.58061111 5.71266667 5.84472222 5.97677778 6.10883333 6.24088889
 6.37294444 6.505      7.05566667 7.38433333 7.651      3.608
 4.13       4.66083333 4.705      4.804      4.2505     3.87333333
 4.755      3.573      2.637      2.0305     1.769      1.498
 1.33666667 1.378      1.5        1.903      3.657      4.54483333
 4.599      4.737      4.366      4.019      4.139      3.621
 2.803      2.244      2.614      4.01266667 4.9715     5.231
 4.104      4.18733333 3.413      2.597      1.45       1.41466667
 1.30633333 1.2385     1.216      1.843      3.75566667 4.77416667
 5.085      5.112      4.1415     3.171      2.886      2.784
 2.682      2.248      2.9865     3.725      4.545      5.83233333
 5.99583333 3.595      2.853      2.243      2.004      1.876
 1.81       1.77183333 1.746      2.37866667 2.99383333 4.488
 4.96466667 5.0405     4.878      4.009      4.4835     4.958
 2.799      2.7955     2.792      3.946      4.846      5.746
 4.064      2.888      2.6525     3.005      2.118      1.65333333
 1.40966667 1.359      1.314      1.769      3.16033333 4.319
 4.782      4.884      3.664      3.06066667 3.094      3.091
 2.773      2.68183333 3.021      3.921      3.9815     4.212
 5.062      3.232      2.68       2.15816667 2.309      1.541
 1.40566667 1.3705     1.403      1.659      3.171      4.0215
 4.494      4.47       3.73       3.03816667 3.279      2.87033333
 2.6735     2.733      2.993      3.873      3.8015     3.29
 3.557      3.08366667 2.9285     2.884      2.254      1.986
 1.602      1.417      1.42366667 1.757      2.15166667 3.563
 4.985      6.025      5.817      5.3695     5.08083333 5.355
 5.069      4.874      4.8115     4.759      4.775      4.644
 4.38383333 3.738      2.529      2.724      2.6645     2.605
 1.90633333 1.4245     1.30166667 1.35       1.59333333 2.5965
 4.13766667 7.436      6.583      6.269      5.84916667 5.32
 4.868      4.5695     4.3655     4.838      4.428      4.949
 5.14866667 3.542      2.282      1.95       1.69066667 1.561
 1.361      1.2835     1.35716667 1.613      3.273      4.477
 4.76733333 4.349      3.73366667 3.683      3.94       3.014
 3.09066667 3.202      3.567      4.688      4.46066667 4.37216667
 4.498      3.626      2.39866667 1.60233333 1.49383333 1.408
 1.356      1.376      1.422      1.588      3.55733333 4.316
 4.09       4.981      3.86633333 3.33766667 3.481      2.865
 2.67766667 3.281      3.719      3.606      3.493      5.926
 4.99       3.79983333 2.529      1.877      1.7175     1.558
 1.36       1.2555     1.228      1.55666667 3.884      6.047
 4.136      4.2895     4.443      3.978      4.4455     4.913
 2.648      2.752      2.986      3.896      4.65066667 4.977
 4.722      3.74666667 2.705      2.151      1.519      1.461
 1.46166667 1.4615     1.314      1.72866667 3.2905     4.645
 4.058      3.6985     3.339      3.343      3.30983333 3.144
 2.70666667 2.8395     3.191      3.704      3.72087153 3.73774306
 3.75461458 3.77148611 3.78835764 3.80522917 3.82210069 3.83897222
 3.85584375 3.87271528 3.88958681 3.90645833 3.92332986 3.94020139
 3.95707292 3.97394444 3.99081597 4.0076875  4.02455903 4.04143056
 4.05830208 4.07517361 4.09204514 4.10891667 4.12578819 4.14265972
 4.15953125 4.17640278 4.19327431 4.21014583 4.22701736 4.24388889
 4.26076042 4.27763194 4.29450347 4.311375   4.32824653 4.34511806
 4.36198958 4.37886111 4.39573264 4.41260417 4.42947569 4.44634722
 4.46321875 4.48009028 4.49696181 4.51383333 4.53070486 4.54757639
 4.56444792 4.58131944 4.59819097 4.6150625  4.63193403 4.64880556
 4.66567708 4.68254861 4.69942014 4.71629167 4.73316319 4.75003472
 4.76690625 4.78377778 4.80064931 4.81752083 4.83439236 4.85126389
 4.86813542 4.88500694 4.90187847 4.91875    6.028      4.36
 4.133      3.77966667 3.0105     2.323      1.82366667 1.418
 1.27266667 1.354      1.41616667 1.347      1.40666667 1.46633333
 1.526      1.58566667 1.64533333 1.705      1.76466667 1.82433333
 1.884      1.94366667 2.00333333 2.063      2.12266667 2.18233333
 2.242      2.30166667 2.36133333 2.421      2.48066667 2.54033333
 2.6        2.65966667 2.71933333 2.779      2.83866667 2.89833333
 2.958      3.01766667 3.07733333 3.137      3.19666667 3.25633333
 3.316      3.37566667 3.43533333 3.495      4.527      5.212
 5.21       3.682      2.839      2.56333333 1.58       1.56533333
 1.51       1.492      1.49166667 1.46       3.435      3.5905
 3.73733333 3.694      2.696      2.96344444 3.23088889 3.49833333
 3.76577778 4.03322222 4.30066667 4.56811111 4.83555556 5.103
 5.103      4.98476471 4.86652941 4.74829412 4.63005882 4.51182353
 4.39358824 4.27535294 4.15711765 4.03888235 3.92064706 3.80241176
 3.68417647 3.56594118 3.44770588 3.32947059 3.21123529 3.093
 3.2165     3.34       3.761      4.09033333 4.24516667 4.196
 4.30666667 3.7635     3.0815     2.664      1.599      1.615
 1.613      1.56666667 1.38       2.15733333 4.447      6.348
 4.921      4.812      4.703      3.611      3.7245     3.838     ]                        DOY   WD    HR
2021-11-26 15:00:00  330.0  5.0  15.0
2021-11-26 16:00:00  330.0  5.0  16.0
2021-11-26 17:00:00  330.0  5.0  17.0
2021-11-26 18:00:00  330.0  5.0  18.0
2021-11-26 19:00:00  330.0  5.0  19.0
...                    ...  ...   ...
2021-12-21 15:00:00  355.0  2.0  15.0
2021-12-21 16:00:00  355.0  2.0  16.0
2021-12-21 17:00:00  355.0  2.0  17.0
2021-12-21 18:00:00  355.0  2.0  18.0
2021-12-21 19:00:00  355.0  2.0  19.0

[605 rows x 3 columns] 2021-11-26 15:00:00    3.491000
2021-11-26 16:00:00    3.307667
2021-11-26 17:00:00    3.888500
2021-11-26 18:00:00    4.631333
2021-11-26 19:00:00    4.983000
                         ...   
2021-12-21 15:00:00    0.000000
2021-12-21 16:00:00    0.000000
2021-12-21 17:00:00    0.000000
2021-12-21 18:00:00    0.000000
2021-12-21 19:00:00    0.000000
Freq: 60T, Name: scadavalue, Length: 605, dtype: float64 [[351. 351. 351. 351. 351. 351. 351. 351. 351. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354.
  354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 355. 355. 355.
  355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355.
  355. 355. 355.]
 [  5.   5.   5.   5.   5.   5.   5.   5.   5.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.]
 [ 15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.   3.   4.
    5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.
   19.  20.  21.  22.  23.   0.   1.   2.   3.   4.   5.   6.   7.   8.
    9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.
   23.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
   13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.
    3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.
   17.  18.  19.]]
Run 0
Iteration 1, loss = 1952.21615345
Iteration 2, loss = 541.91101839
Iteration 3, loss = 110.98778455
Iteration 4, loss = 19.04719298
Iteration 5, loss = 19.64413476
Iteration 6, loss = 4.41063703
Iteration 7, loss = 6.64297233
Iteration 8, loss = 1.30762549
Iteration 9, loss = 2.33875955
Iteration 10, loss = 1.30637960
Iteration 11, loss = 1.41951675
Iteration 12, loss = 1.04120629
Iteration 13, loss = 1.11898743
Iteration 14, loss = 1.05623310
Iteration 15, loss = 0.97998092
Iteration 16, loss = 1.05403529
Iteration 17, loss = 0.98063836
Iteration 18, loss = 0.98653677
Iteration 19, loss = 0.97841931
Iteration 20, loss = 0.94241926
Iteration 21, loss = 0.96435718
Iteration 22, loss = 0.93623904
Iteration 23, loss = 0.92742611
Iteration 24, loss = 0.90285290
Iteration 25, loss = 0.91148207
Iteration 26, loss = 0.85786653
Iteration 27, loss = 0.84457015
Iteration 28, loss = 0.83635904
Iteration 29, loss = 0.89670071
Iteration 30, loss = 0.82533951
Iteration 31, loss = 0.79744685
Iteration 32, loss = 0.75751238
Iteration 33, loss = 0.76951899
Iteration 34, loss = 0.73078686
Iteration 35, loss = 0.73972184
Iteration 36, loss = 0.75139460
Iteration 37, loss = 0.74689291
Iteration 38, loss = 0.72885826
Iteration 39, loss = 0.70177049
Iteration 40, loss = 0.69893899
Iteration 41, loss = 0.70655960
Iteration 42, loss = 0.68700432
Iteration 43, loss = 0.66501294
Iteration 44, loss = 0.66182552
Iteration 45, loss = 0.66185930
Iteration 46, loss = 0.65478047
Iteration 47, loss = 0.64898057
Iteration 48, loss = 0.64526742
Iteration 49, loss = 0.65240959
Iteration 50, loss = 0.66975575
Iteration 51, loss = 0.66849837
Iteration 52, loss = 0.67674980
Iteration 53, loss = 0.66245953
Iteration 54, loss = 0.64386431
Iteration 55, loss = 0.63508503
Iteration 56, loss = 0.63866024
Iteration 57, loss = 0.63060059
Iteration 58, loss = 0.62555665
Iteration 59, loss = 0.62003803
Iteration 60, loss = 0.61824057
Iteration 61, loss = 0.61425030
Iteration 62, loss = 0.61254114
Iteration 63, loss = 0.61538177
Iteration 64, loss = 0.60728687
Iteration 65, loss = 0.61152861
Iteration 66, loss = 0.60629942
Iteration 67, loss = 0.61851111
Iteration 68, loss = 0.60346182
Iteration 69, loss = 0.59941362
Iteration 70, loss = 0.59709107
Iteration 71, loss = 0.59815033
Iteration 72, loss = 0.59995725
Iteration 73, loss = 0.60799155
Iteration 74, loss = 0.61795832
Iteration 75, loss = 0.60209196
Iteration 76, loss = 0.60803841
Iteration 77, loss = 0.64237747
Iteration 78, loss = 0.62259978
Iteration 79, loss = 0.61453176
Iteration 80, loss = 0.60668014
Iteration 81, loss = 0.60467671
Iteration 82, loss = 0.60179943
Iteration 83, loss = 0.61897079
Iteration 84, loss = 0.62499473
Iteration 85, loss = 0.59168333
Iteration 86, loss = 0.60561953
Iteration 87, loss = 0.61323142
Iteration 88, loss = 0.66681070
Iteration 89, loss = 0.65884022
Iteration 90, loss = 0.67096919
Iteration 91, loss = 0.62867910
Iteration 92, loss = 0.63289091
Iteration 93, loss = 0.63027392
Iteration 94, loss = 0.63071762
Iteration 95, loss = 0.62710135
Iteration 96, loss = 0.62786326
Iteration 97, loss = 0.62101939
Iteration 98, loss = 0.61201081
Iteration 99, loss = 0.60741397
Iteration 100, loss = 0.61714512
Iteration 101, loss = 0.62341346
Iteration 102, loss = 0.61225272
Iteration 103, loss = 0.60689261
Iteration 104, loss = 0.61817006
Iteration 105, loss = 0.61037536
Iteration 106, loss = 0.61088236
Iteration 107, loss = 0.60487900
Iteration 108, loss = 0.60736558
Iteration 109, loss = 0.60773859
Iteration 110, loss = 0.60278396
Iteration 111, loss = 0.60472603
Iteration 112, loss = 0.60696095
Iteration 113, loss = 0.60414402
Iteration 114, loss = 0.60467569
Iteration 115, loss = 0.60649385
Iteration 116, loss = 0.60931701
Iteration 117, loss = 0.60118460
Iteration 118, loss = 0.60170648
Iteration 119, loss = 0.60144022
Iteration 120, loss = 0.60018506
Iteration 121, loss = 0.60523754
Iteration 122, loss = 0.61605563
Iteration 123, loss = 0.61399913
Iteration 124, loss = 0.60504630
Iteration 125, loss = 0.61303524
Iteration 126, loss = 0.60864431
Iteration 127, loss = 0.59979700
Iteration 128, loss = 0.59765156
Iteration 129, loss = 0.59466534
Iteration 130, loss = 0.60023747
Iteration 131, loss = 0.59695103
Iteration 132, loss = 0.59520867
Iteration 133, loss = 0.59402065
Iteration 134, loss = 0.59857603
Iteration 135, loss = 0.60815295
Iteration 136, loss = 0.59770277
Iteration 137, loss = 0.60044767
Iteration 138, loss = 0.59541869
Iteration 139, loss = 0.59517549
Iteration 140, loss = 0.58799323
Iteration 141, loss = 0.58493915
Iteration 142, loss = 0.58627592
Iteration 143, loss = 0.61127426
Iteration 144, loss = 0.59523206
Iteration 145, loss = 0.61233998
Iteration 146, loss = 0.59536567
Iteration 147, loss = 0.59818091
Iteration 148, loss = 0.60539351
Iteration 149, loss = 0.59342577
Iteration 150, loss = 0.58745392
Iteration 151, loss = 0.57875494
Iteration 152, loss = 0.59364526
Iteration 153, loss = 0.59411014
Iteration 154, loss = 0.60893308
Iteration 155, loss = 0.61780391
Iteration 156, loss = 0.61181681
Iteration 157, loss = 0.57920673
Iteration 158, loss = 0.57835911
Iteration 159, loss = 0.58163968
Iteration 160, loss = 0.58911630
Iteration 161, loss = 0.59451083
Iteration 162, loss = 0.58533984
Iteration 163, loss = 0.59042013
Iteration 164, loss = 0.59617424
Iteration 165, loss = 0.59501787
Iteration 166, loss = 0.57081007
Iteration 167, loss = 0.57580849
Iteration 168, loss = 0.57581337
Iteration 169, loss = 0.56954537
Iteration 170, loss = 0.56351257
Iteration 171, loss = 0.57263341
Iteration 172, loss = 0.56417576
Iteration 173, loss = 0.57019825
Iteration 174, loss = 0.60322617
Iteration 175, loss = 0.58916289
Iteration 176, loss = 0.60588172
Iteration 177, loss = 0.56754412
Iteration 178, loss = 0.56157418
Iteration 179, loss = 0.59548525
Iteration 180, loss = 0.59254133
Iteration 181, loss = 0.57122761
Iteration 182, loss = 0.57445218
Iteration 183, loss = 0.56472533
Iteration 184, loss = 0.57152539
Iteration 185, loss = 0.55936904
Iteration 186, loss = 0.58275181
Iteration 187, loss = 0.56633213
Iteration 188, loss = 0.56456098
Iteration 189, loss = 0.57017438
Iteration 190, loss = 0.56255122
Iteration 191, loss = 0.56216316
Iteration 192, loss = 0.56538345
Iteration 193, loss = 0.56475743
Iteration 194, loss = 0.56127642
Iteration 195, loss = 0.55726093
Iteration 196, loss = 0.55673314
Iteration 197, loss = 0.56719119
Iteration 198, loss = 0.56598766
Iteration 199, loss = 0.56688043
Iteration 200, loss = 0.56463247
Iteration 201, loss = 0.57175059
Iteration 202, loss = 0.55505900
Iteration 203, loss = 0.56881956
Iteration 204, loss = 0.56189403
Iteration 205, loss = 0.57470303
Iteration 206, loss = 0.57930752
Iteration 207, loss = 0.55923658
Iteration 208, loss = 0.56135170
Iteration 209, loss = 0.56154870
Iteration 210, loss = 0.59357570
Iteration 211, loss = 0.58773807
Iteration 212, loss = 0.59971807
Iteration 213, loss = 0.56421567
Iteration 214, loss = 0.56787708
Iteration 215, loss = 0.57646176
Iteration 216, loss = 0.56912012
Iteration 217, loss = 0.59938532
Iteration 218, loss = 0.62152348
Iteration 219, loss = 0.62155333
Iteration 220, loss = 0.61117439
Iteration 221, loss = 0.58822514
Iteration 222, loss = 0.58309040
Iteration 223, loss = 0.56730307
Iteration 224, loss = 0.56519443
Iteration 225, loss = 0.56445643
Iteration 226, loss = 0.56107001
Iteration 227, loss = 0.57275874
Iteration 228, loss = 0.60907048
Iteration 229, loss = 0.57647504
Iteration 230, loss = 0.58808413
Iteration 231, loss = 0.57727336
Iteration 232, loss = 0.56347017
Iteration 233, loss = 0.55929798
Iteration 234, loss = 0.55881075
Iteration 235, loss = 0.55496864
Iteration 236, loss = 0.57255240
Iteration 237, loss = 0.54962069
Iteration 238, loss = 0.55649054
Iteration 239, loss = 0.56368019
Iteration 240, loss = 0.55592922
Iteration 241, loss = 0.55921073
Iteration 242, loss = 0.56048608
Iteration 243, loss = 0.56366025
Iteration 244, loss = 0.54597721
Iteration 245, loss = 0.56932310
Iteration 246, loss = 0.55577513
Iteration 247, loss = 0.56349977
Iteration 248, loss = 0.54649917
Iteration 249, loss = 0.56423215
Iteration 250, loss = 0.55122683
Iteration 251, loss = 0.56428874
Iteration 252, loss = 0.54709269
Iteration 253, loss = 0.55622462
Iteration 254, loss = 0.56124801
Iteration 255, loss = 0.56047345
Iteration 256, loss = 0.55550839
Iteration 257, loss = 0.56250027
Iteration 258, loss = 0.55063921
Iteration 259, loss = 0.57058729
Iteration 260, loss = 0.55572482
Iteration 261, loss = 0.55447837
Iteration 262, loss = 0.55079747
Iteration 263, loss = 0.55329544
Iteration 264, loss = 0.56321793
Iteration 265, loss = 0.55472309
Iteration 266, loss = 0.55117096
Iteration 267, loss = 0.55434482
Iteration 268, loss = 0.54796949
Iteration 269, loss = 0.54963744
Iteration 270, loss = 0.57041704
Iteration 271, loss = 0.56964418
Iteration 272, loss = 0.56825795
Iteration 273, loss = 0.55051127
Iteration 274, loss = 0.55700758
Iteration 275, loss = 0.55754842
Iteration 276, loss = 0.54706746
Iteration 277, loss = 0.54791572
Iteration 278, loss = 0.55577141
Iteration 279, loss = 0.55360077
Iteration 280, loss = 0.55397887
Iteration 281, loss = 0.55649881
Iteration 282, loss = 0.54744246
Iteration 283, loss = 0.55870304
Iteration 284, loss = 0.55507068
Iteration 285, loss = 0.56546401
Iteration 286, loss = 0.55032861
Iteration 287, loss = 0.54913833
Iteration 288, loss = 0.55117354
Iteration 289, loss = 0.56295526
Iteration 290, loss = 0.56405988
Iteration 291, loss = 0.56054362
Iteration 292, loss = 0.56687505
Iteration 293, loss = 0.58798464
Iteration 294, loss = 0.56053361
Iteration 295, loss = 0.55869577
Iteration 296, loss = 0.56347140
Iteration 297, loss = 0.56231336
Iteration 298, loss = 0.58870287
Iteration 299, loss = 0.54600537
Iteration 300, loss = 0.58336714
Iteration 301, loss = 0.55281334
Iteration 302, loss = 0.54700347
Iteration 303, loss = 0.56823657
Iteration 304, loss = 0.55063698
Iteration 305, loss = 0.56025129
Iteration 306, loss = 0.55472128
Iteration 307, loss = 0.56305259
Iteration 308, loss = 0.54543111
Iteration 309, loss = 0.56325139
Iteration 310, loss = 0.55759917
Iteration 311, loss = 0.55222626
Iteration 312, loss = 0.55193055
Iteration 313, loss = 0.55439000
Iteration 314, loss = 0.54744206
Iteration 315, loss = 0.56037833
Iteration 316, loss = 0.59774006
Iteration 317, loss = 0.60952427
Iteration 318, loss = 0.59686761
Iteration 319, loss = 0.58870415
Iteration 320, loss = 0.58914143
Iteration 321, loss = 0.56310259
Iteration 322, loss = 0.56033848
Iteration 323, loss = 0.56860712
Iteration 324, loss = 0.56314130
Iteration 325, loss = 0.55550052
Iteration 326, loss = 0.55828345
Iteration 327, loss = 0.55364080
Iteration 328, loss = 0.58070345
Iteration 329, loss = 0.60668862
Iteration 330, loss = 0.56282744
Iteration 331, loss = 0.57099855
Iteration 332, loss = 0.56780976
Iteration 333, loss = 0.54814595
Iteration 334, loss = 0.55236592
Iteration 335, loss = 0.54735807
Iteration 336, loss = 0.55142154
Iteration 337, loss = 0.55581732
Iteration 338, loss = 0.56557576
Iteration 339, loss = 0.55051774
Iteration 340, loss = 0.54530379
Iteration 341, loss = 0.55613077
Iteration 342, loss = 0.59395293
Iteration 343, loss = 0.58386616
Iteration 344, loss = 0.56688351
Iteration 345, loss = 0.59570489
Iteration 346, loss = 0.60307098
Iteration 347, loss = 0.61939155
Iteration 348, loss = 0.59825764
Iteration 349, loss = 0.60839193
Iteration 350, loss = 0.59831226
Iteration 351, loss = 0.55348190
Iteration 352, loss = 0.57994195
Iteration 353, loss = 0.55846153
Iteration 354, loss = 0.56106907
Iteration 355, loss = 0.56757479
Iteration 356, loss = 0.58161734
Iteration 357, loss = 0.58435105
Iteration 358, loss = 0.62362749
Iteration 359, loss = 0.59945419
Iteration 360, loss = 0.57127480
Iteration 361, loss = 0.59721142
Iteration 362, loss = 0.59718357
Iteration 363, loss = 0.57506187
Iteration 364, loss = 0.57737519
Iteration 365, loss = 0.55342640
Iteration 366, loss = 0.54922000
Iteration 367, loss = 0.56172714
Iteration 368, loss = 0.56170219
Iteration 369, loss = 0.55513077
Iteration 370, loss = 0.55765700
Iteration 371, loss = 0.55129441
Iteration 372, loss = 0.54707760
Iteration 373, loss = 0.54778309
Iteration 374, loss = 0.54984102
Iteration 375, loss = 0.55672123
Iteration 376, loss = 0.56398161
Iteration 377, loss = 0.57041457
Iteration 378, loss = 0.57083988
Iteration 379, loss = 0.55085631
Iteration 380, loss = 0.54692495
Iteration 381, loss = 0.54736746
Iteration 382, loss = 0.54477641
Iteration 383, loss = 0.54779730
Iteration 384, loss = 0.55324404
Iteration 385, loss = 0.55056993
Iteration 386, loss = 0.55492360
Iteration 387, loss = 0.54844827
Iteration 388, loss = 0.54566025
Iteration 389, loss = 0.54590476
Iteration 390, loss = 0.55303277
Iteration 391, loss = 0.54569419
Iteration 392, loss = 0.56228875
Iteration 393, loss = 0.55512647
Iteration 394, loss = 0.55079864
Iteration 395, loss = 0.54603379
Iteration 396, loss = 0.55682650
Iteration 397, loss = 0.55342846
Iteration 398, loss = 0.56179607
Iteration 399, loss = 0.54379799
Iteration 400, loss = 0.54794701
Iteration 401, loss = 0.55567643
Iteration 402, loss = 0.54794825
Iteration 403, loss = 0.56533798
Iteration 404, loss = 0.55239248
Iteration 405, loss = 0.56336481
Iteration 406, loss = 0.57449205
Iteration 407, loss = 0.61432901
Iteration 408, loss = 0.57183601
Iteration 409, loss = 0.58229756
Iteration 410, loss = 0.55392259
Iteration 411, loss = 0.57510897
Iteration 412, loss = 0.58435900
Iteration 413, loss = 0.57170561
Iteration 414, loss = 0.57460365
Iteration 415, loss = 0.56535921
Iteration 416, loss = 0.56470367
Iteration 417, loss = 0.57021150
Iteration 418, loss = 0.55519405
Iteration 419, loss = 0.55228560
Iteration 420, loss = 0.55207259
Iteration 421, loss = 0.55973038
Iteration 422, loss = 0.56243857
Iteration 423, loss = 0.55126139
Iteration 424, loss = 0.56972414
Iteration 425, loss = 0.55320251
Iteration 426, loss = 0.55490758
Iteration 427, loss = 0.57139465
Iteration 428, loss = 0.59248189
Iteration 429, loss = 0.61206937
Iteration 430, loss = 0.65937619
Iteration 431, loss = 0.59680627
Iteration 432, loss = 0.63555537
Iteration 433, loss = 0.62845283
Iteration 434, loss = 0.62457054
Iteration 435, loss = 0.60266561
Iteration 436, loss = 0.58675963
Iteration 437, loss = 0.57560000
Iteration 438, loss = 0.56207720
Iteration 439, loss = 0.56872353
Iteration 440, loss = 0.56045510
Iteration 441, loss = 0.56378202
Iteration 442, loss = 0.55884229
Iteration 443, loss = 0.56206013
Iteration 444, loss = 0.54792604
Iteration 445, loss = 0.55614596
Iteration 446, loss = 0.55475748
Iteration 447, loss = 0.56786910
Iteration 448, loss = 0.55386600
Iteration 449, loss = 0.55873613
Iteration 450, loss = 0.55908376
Iteration 451, loss = 0.55070461
Iteration 452, loss = 0.56225460
Iteration 453, loss = 0.54643227
Iteration 454, loss = 0.55372095
Iteration 455, loss = 0.54528298
Iteration 456, loss = 0.54769297
Iteration 457, loss = 0.55128000
Iteration 458, loss = 0.54913328
Iteration 459, loss = 0.54570344
Iteration 460, loss = 0.55165843
Iteration 461, loss = 0.55670897
Iteration 462, loss = 0.54888699
Iteration 463, loss = 0.54674873
Iteration 464, loss = 0.55804863
Iteration 465, loss = 0.56062999
Iteration 466, loss = 0.55287992
Iteration 467, loss = 0.55248165
Iteration 468, loss = 0.55322644
Iteration 469, loss = 0.54984761
Iteration 470, loss = 0.54853057
Iteration 471, loss = 0.54891028
Iteration 472, loss = 0.54834069
Iteration 473, loss = 0.54672297
Iteration 474, loss = 0.54502602
Iteration 475, loss = 0.54517275
Iteration 476, loss = 0.55730259
Iteration 477, loss = 0.56303620
Iteration 478, loss = 0.55912490
Iteration 479, loss = 0.54976892
Iteration 480, loss = 0.54997412
Iteration 481, loss = 0.54983735
Iteration 482, loss = 0.54674230
Iteration 483, loss = 0.55172396
Iteration 484, loss = 0.54524892
Iteration 485, loss = 0.56452319
Iteration 486, loss = 0.55143137
Iteration 487, loss = 0.55198103
Iteration 488, loss = 0.54588358
Iteration 489, loss = 0.55218216
Iteration 490, loss = 0.54488813
Iteration 491, loss = 0.54913598
Iteration 492, loss = 0.55500630
Iteration 493, loss = 0.54781011
Iteration 494, loss = 0.55577687
Iteration 495, loss = 0.56275739
Iteration 496, loss = 0.56108242
Iteration 497, loss = 0.56736156
Iteration 498, loss = 0.57503014
Iteration 499, loss = 0.56423181
Iteration 500, loss = 0.54463011
Iteration 501, loss = 0.54482020
Iteration 502, loss = 0.55895351
Iteration 503, loss = 0.54540982
Iteration 504, loss = 0.56387985
Iteration 505, loss = 0.54685636
Iteration 506, loss = 0.54603560
Iteration 507, loss = 0.55000376
Iteration 508, loss = 0.56319029
Iteration 509, loss = 0.55835567
Iteration 510, loss = 0.57788377
Iteration 511, loss = 0.54897116
Iteration 512, loss = 0.56128224
Iteration 513, loss = 0.56966177
Iteration 514, loss = 0.54516950
Iteration 515, loss = 0.55953422
Iteration 516, loss = 0.54870864
Iteration 517, loss = 0.54525780
Iteration 518, loss = 0.54686711
Iteration 519, loss = 0.55097152
Iteration 520, loss = 0.55647358
Iteration 521, loss = 0.55902993
Iteration 522, loss = 0.56318406
Iteration 523, loss = 0.54753614
Iteration 524, loss = 0.55694565
Iteration 525, loss = 0.55066338
Iteration 526, loss = 0.54472348
Iteration 527, loss = 0.54369625
Iteration 528, loss = 0.55397764
Iteration 529, loss = 0.57403377
Iteration 530, loss = 0.55299683
Iteration 531, loss = 0.54727528
Iteration 532, loss = 0.56295361
Iteration 533, loss = 0.55379317
Iteration 534, loss = 0.54484376
Iteration 535, loss = 0.56296804
Iteration 536, loss = 0.55139152
Iteration 537, loss = 0.54838314
Iteration 538, loss = 0.56655651
Iteration 539, loss = 0.56889283
Iteration 540, loss = 0.55798868
Iteration 541, loss = 0.54631190
Iteration 542, loss = 0.54394101
Iteration 543, loss = 0.55107518
Iteration 544, loss = 0.54796463
Iteration 545, loss = 0.54905083
Iteration 546, loss = 0.54586489
Iteration 547, loss = 0.54893549
Iteration 548, loss = 0.55769126
Iteration 549, loss = 0.55601906
Iteration 550, loss = 0.57279398
Iteration 551, loss = 0.58121249
Iteration 552, loss = 0.55357744
Iteration 553, loss = 0.55305400
Iteration 554, loss = 0.55168026
Iteration 555, loss = 0.55253618
Iteration 556, loss = 0.54746295
Iteration 557, loss = 0.55261478
Iteration 558, loss = 0.54871939
Iteration 559, loss = 0.57303925
Iteration 560, loss = 0.58226815
Iteration 561, loss = 0.56546016
Iteration 562, loss = 0.56163897
Iteration 563, loss = 0.62814438
Iteration 564, loss = 0.58035751
Iteration 565, loss = 0.57126745
Iteration 566, loss = 0.56765097
Iteration 567, loss = 0.56365380
Iteration 568, loss = 0.56120873
Iteration 569, loss = 0.56580685
Iteration 570, loss = 0.54602360
Iteration 571, loss = 0.54622401
Iteration 572, loss = 0.54695637
Iteration 573, loss = 0.55095787
Iteration 574, loss = 0.54806189
Iteration 575, loss = 0.54592253
Iteration 576, loss = 0.54949723
Iteration 577, loss = 0.54507184
Iteration 578, loss = 0.54578108
Iteration 579, loss = 0.55070519
Iteration 580, loss = 0.55067548
Iteration 581, loss = 0.55153801
Iteration 582, loss = 0.55409125
Iteration 583, loss = 0.56259790
Iteration 584, loss = 0.55237259
Iteration 585, loss = 0.58397619
Iteration 586, loss = 0.57210037
Iteration 587, loss = 0.56461688
Iteration 588, loss = 0.55800967
Iteration 589, loss = 0.57407416
Iteration 590, loss = 0.54738223
Iteration 591, loss = 0.54829031
Iteration 592, loss = 0.55351555
Iteration 593, loss = 0.55275514
Iteration 594, loss = 0.57670818
Iteration 595, loss = 0.55043388
Iteration 596, loss = 0.56329139
Iteration 597, loss = 0.56646911
Iteration 598, loss = 0.56000516
Iteration 599, loss = 0.55459413
Iteration 600, loss = 0.55095232
Iteration 601, loss = 0.54958530
Iteration 602, loss = 0.56299129
Iteration 603, loss = 0.55005027
Iteration 604, loss = 0.56584950
Iteration 605, loss = 0.58049420
Iteration 606, loss = 0.56629366
Iteration 607, loss = 0.56989951
Iteration 608, loss = 0.59789731
Iteration 609, loss = 0.60679515
Iteration 610, loss = 0.61032618
Iteration 611, loss = 0.59180623
Iteration 612, loss = 0.56199763
Iteration 613, loss = 0.55170272
Iteration 614, loss = 0.56773350
Iteration 615, loss = 0.54735230
Iteration 616, loss = 0.56414427
Iteration 617, loss = 0.55577181
Iteration 618, loss = 0.55433819
Iteration 619, loss = 0.55209962
Iteration 620, loss = 0.55410252
Iteration 621, loss = 0.57111888
Iteration 622, loss = 0.62325910
Iteration 623, loss = 0.71214260
Iteration 624, loss = 0.63876079
Iteration 625, loss = 0.61257982
Iteration 626, loss = 0.61932915
Iteration 627, loss = 0.59928135
Iteration 628, loss = 0.60901005
Iteration 629, loss = 0.57879653
Iteration 630, loss = 0.59482345
Iteration 631, loss = 0.57208873
Iteration 632, loss = 0.58505983
Iteration 633, loss = 0.57002919
Iteration 634, loss = 0.55629450
Iteration 635, loss = 0.55343532
Iteration 636, loss = 0.56810075
Iteration 637, loss = 0.55715196
Iteration 638, loss = 0.55139930
Iteration 639, loss = 0.55070547
Iteration 640, loss = 0.54920679
Iteration 641, loss = 0.54976307
Iteration 642, loss = 0.62575293
Iteration 643, loss = 0.59341427
Iteration 644, loss = 0.56755270
Iteration 645, loss = 0.56463838
Iteration 646, loss = 0.55436115
Iteration 647, loss = 0.54429131
Iteration 648, loss = 0.56286762
Iteration 649, loss = 0.54599027
Iteration 650, loss = 0.55117310
Iteration 651, loss = 0.55656075
Iteration 652, loss = 0.55129816
Iteration 653, loss = 0.54927621
Iteration 654, loss = 0.54541190
Iteration 655, loss = 0.54632484
Iteration 656, loss = 0.55838915
Iteration 657, loss = 0.54718284
Iteration 658, loss = 0.56495075
Iteration 659, loss = 0.57594311
Iteration 660, loss = 0.57551260
Iteration 661, loss = 0.55960630
Iteration 662, loss = 0.54879703
Iteration 663, loss = 0.54488585
Iteration 664, loss = 0.55009404
Iteration 665, loss = 0.54648376
Iteration 666, loss = 0.54801328
Iteration 667, loss = 0.54319163
Iteration 668, loss = 0.55463710
Iteration 669, loss = 0.55962554
Iteration 670, loss = 0.54983689
Iteration 671, loss = 0.54719234
Iteration 672, loss = 0.55057136
Iteration 673, loss = 0.54624639
Iteration 674, loss = 0.54750068
Iteration 675, loss = 0.54871306
Iteration 676, loss = 0.55757676
Iteration 677, loss = 0.58139197
Iteration 678, loss = 0.56916208
Iteration 679, loss = 0.56588732
Iteration 680, loss = 0.55678191
Iteration 681, loss = 0.56877896
Iteration 682, loss = 0.57015216
Iteration 683, loss = 0.60392596
Iteration 684, loss = 0.60665361
Iteration 685, loss = 0.57771772
Iteration 686, loss = 0.61140716
Iteration 687, loss = 0.60031481
Iteration 688, loss = 0.57524709
Iteration 689, loss = 0.54693902
Iteration 690, loss = 0.55809325
Iteration 691, loss = 0.55678373
Iteration 692, loss = 0.57528533
Iteration 693, loss = 0.55740148
Iteration 694, loss = 0.54461995
Iteration 695, loss = 0.54601509
Iteration 696, loss = 0.54638361
Iteration 697, loss = 0.54839740
Iteration 698, loss = 0.54187681
Iteration 699, loss = 0.55435711
Iteration 700, loss = 0.55248145
Iteration 701, loss = 0.54396052
Iteration 702, loss = 0.54587518
Iteration 703, loss = 0.54714014
Iteration 704, loss = 0.55981297
Iteration 705, loss = 0.56068091
Iteration 706, loss = 0.56455076
Iteration 707, loss = 0.55180229
Iteration 708, loss = 0.56120834
Iteration 709, loss = 0.54990461
Iteration 710, loss = 0.55173278
Iteration 711, loss = 0.54232698
Iteration 712, loss = 0.55172167
Iteration 713, loss = 0.55413968
Iteration 714, loss = 0.54843758
Iteration 715, loss = 0.55626190
Iteration 716, loss = 0.55369017
Iteration 717, loss = 0.54918719
Iteration 718, loss = 0.54300395
Iteration 719, loss = 0.55417237
Iteration 720, loss = 0.55164362
Iteration 721, loss = 0.54673640
Iteration 722, loss = 0.54914580
Iteration 723, loss = 0.54829166
Iteration 724, loss = 0.54620527
Iteration 725, loss = 0.54675161
Iteration 726, loss = 0.54943415
Iteration 727, loss = 0.55422763
Iteration 728, loss = 0.54608423
Iteration 729, loss = 0.55028508
Iteration 730, loss = 0.56825257
Iteration 731, loss = 0.58017899
Iteration 732, loss = 0.57482231
Iteration 733, loss = 0.59233600
Iteration 734, loss = 0.57187702
Iteration 735, loss = 0.55887462
Iteration 736, loss = 0.55902442
Iteration 737, loss = 0.56718481
Iteration 738, loss = 0.58785839
Iteration 739, loss = 0.58838986
Iteration 740, loss = 0.58379472
Iteration 741, loss = 0.56159940
Iteration 742, loss = 0.54510994
Iteration 743, loss = 0.55098042
Iteration 744, loss = 0.55913755
Iteration 745, loss = 0.54940984
Iteration 746, loss = 0.54520386
Iteration 747, loss = 0.54516651
Iteration 748, loss = 0.54673950
Iteration 749, loss = 0.54428455
Iteration 750, loss = 0.54466644
Iteration 751, loss = 0.55071852
Iteration 752, loss = 0.55078529
Iteration 753, loss = 0.54274587
Iteration 754, loss = 0.54567725
Iteration 755, loss = 0.54212424
Iteration 756, loss = 0.54800763
Iteration 757, loss = 0.54390888
Iteration 758, loss = 0.55076557
Iteration 759, loss = 0.54748507
Iteration 760, loss = 0.54546985
Iteration 761, loss = 0.54347555
Iteration 762, loss = 0.54622946
Iteration 763, loss = 0.54248606
Iteration 764, loss = 0.54823024
Iteration 765, loss = 0.56515721
Iteration 766, loss = 0.55050465
Iteration 767, loss = 0.54765228
Iteration 768, loss = 0.56369083
Iteration 769, loss = 0.59868785
Iteration 770, loss = 0.58212174
Iteration 771, loss = 0.56364207
Iteration 772, loss = 0.55332773
Iteration 773, loss = 0.54320430
Iteration 774, loss = 0.55483950
Iteration 775, loss = 0.56014847
Iteration 776, loss = 0.56339660
Iteration 777, loss = 0.54628423
Iteration 778, loss = 0.55856020
Iteration 779, loss = 0.56470750
Iteration 780, loss = 0.54713881
Iteration 781, loss = 0.55216720
Iteration 782, loss = 0.55722428
Iteration 783, loss = 0.58458502
Iteration 784, loss = 0.56311681
Iteration 785, loss = 0.55610433
Iteration 786, loss = 0.55864389
Iteration 787, loss = 0.55786983
Iteration 788, loss = 0.57803794
Iteration 789, loss = 0.55403436
Iteration 790, loss = 0.56263448
Iteration 791, loss = 0.55996261
Iteration 792, loss = 0.55108108
Iteration 793, loss = 0.54669497
Iteration 794, loss = 0.54479246
Iteration 795, loss = 0.54755467
Iteration 796, loss = 0.54338982
Iteration 797, loss = 0.54768576
Iteration 798, loss = 0.54502805
Iteration 799, loss = 0.54652892
Iteration 800, loss = 0.54723198
Iteration 801, loss = 0.54335395
Iteration 802, loss = 0.54683770
Iteration 803, loss = 0.54484113
Iteration 804, loss = 0.54383660
Iteration 805, loss = 0.54885455
Iteration 806, loss = 0.54911915
Iteration 807, loss = 0.55168833
Iteration 808, loss = 0.58458806
Iteration 809, loss = 0.55075896
Iteration 810, loss = 0.55662683
Iteration 811, loss = 0.56529253
Iteration 812, loss = 0.54643906
Iteration 813, loss = 0.56682263
Iteration 814, loss = 0.58661210
Iteration 815, loss = 0.57355590
Iteration 816, loss = 0.55759322
Iteration 817, loss = 0.55709706
Iteration 818, loss = 0.55270204
Iteration 819, loss = 0.57557420
Iteration 820, loss = 0.55389585
Iteration 821, loss = 0.54673089
Iteration 822, loss = 0.54941830
Iteration 823, loss = 0.55559581
Iteration 824, loss = 0.55096973
Iteration 825, loss = 0.54565694
Iteration 826, loss = 0.58255931
Iteration 827, loss = 0.54927647
Iteration 828, loss = 0.54515969
Iteration 829, loss = 0.55275171
Iteration 830, loss = 0.55746939
Iteration 831, loss = 0.54409843
Iteration 832, loss = 0.54171352
Iteration 833, loss = 0.54698572
Iteration 834, loss = 0.55635023
Iteration 835, loss = 0.55529837
Iteration 836, loss = 0.55611953
Iteration 837, loss = 0.55829889
Iteration 838, loss = 0.54170141
Iteration 839, loss = 0.54796487
Iteration 840, loss = 0.55590724
Iteration 841, loss = 0.56811323
Iteration 842, loss = 0.56171694
Iteration 843, loss = 0.54922055
Iteration 844, loss = 0.54186647
Iteration 845, loss = 0.54406200
Iteration 846, loss = 0.55077754
Iteration 847, loss = 0.55531066
Iteration 848, loss = 0.55587748
Iteration 849, loss = 0.57505651
Iteration 850, loss = 0.57525475
Iteration 851, loss = 0.57494334
Iteration 852, loss = 0.55211065
Iteration 853, loss = 0.56408990
Iteration 854, loss = 0.55135198
Iteration 855, loss = 0.54725160
Iteration 856, loss = 0.54652732
Iteration 857, loss = 0.55250818
Iteration 858, loss = 0.55762771
Iteration 859, loss = 0.56781381
Iteration 860, loss = 0.55989632
Iteration 861, loss = 0.55426903
Iteration 862, loss = 0.55991199
Iteration 863, loss = 0.55720529
Iteration 864, loss = 0.59045026
Iteration 865, loss = 0.62322633
Iteration 866, loss = 0.59049258
Iteration 867, loss = 0.55561985
Iteration 868, loss = 0.57630003
Iteration 869, loss = 0.55068087
Iteration 870, loss = 0.54096102
Iteration 871, loss = 0.55072889
Iteration 872, loss = 0.54407163
Iteration 873, loss = 0.55006129
Iteration 874, loss = 0.59617040
Iteration 875, loss = 0.54060774
Iteration 876, loss = 0.54323175
Iteration 877, loss = 0.58147377
Iteration 878, loss = 0.57011982
Iteration 879, loss = 0.58671816
Iteration 880, loss = 0.55430578
Iteration 881, loss = 0.54589513
Iteration 882, loss = 0.54813262
Iteration 883, loss = 0.55229449
Iteration 884, loss = 0.55825641
Iteration 885, loss = 0.55074572
Iteration 886, loss = 0.54919470
Iteration 887, loss = 0.57291050
Iteration 888, loss = 0.54918254
Iteration 889, loss = 0.56290082
Iteration 890, loss = 0.55118406
Iteration 891, loss = 0.54863746
Iteration 892, loss = 0.54487392
Iteration 893, loss = 0.54886805
Iteration 894, loss = 0.56754677
Iteration 895, loss = 0.56758720
Iteration 896, loss = 0.58715724
Iteration 897, loss = 0.57950585
Iteration 898, loss = 0.56074327
Iteration 899, loss = 0.54469231
Iteration 900, loss = 0.55273709
Iteration 901, loss = 0.55178813
Iteration 902, loss = 0.54757061
Iteration 903, loss = 0.54460467
Iteration 904, loss = 0.54590386
Iteration 905, loss = 0.54412805
Iteration 906, loss = 0.54560161
Iteration 907, loss = 0.54294372
Iteration 908, loss = 0.55108194
Iteration 909, loss = 0.56263173
Iteration 910, loss = 0.56296500
Iteration 911, loss = 0.56052370
Iteration 912, loss = 0.58791814
Iteration 913, loss = 0.56631610
Iteration 914, loss = 0.55677904
Iteration 915, loss = 0.55063353
Iteration 916, loss = 0.57426007
Iteration 917, loss = 0.58696753
Iteration 918, loss = 0.57039945
Iteration 919, loss = 0.55839525
Iteration 920, loss = 0.55103069
Iteration 921, loss = 0.54827297
Iteration 922, loss = 0.54006525
Iteration 923, loss = 0.55394503
Iteration 924, loss = 0.58277423
Iteration 925, loss = 0.54496565
Iteration 926, loss = 0.55745098
Iteration 927, loss = 0.55378486
Iteration 928, loss = 0.56097718
Iteration 929, loss = 0.57942353
Iteration 930, loss = 0.60367913
Iteration 931, loss = 0.55087600
Iteration 932, loss = 0.54367199
Iteration 933, loss = 0.54993564
Iteration 934, loss = 0.54285751
Iteration 935, loss = 0.54624038
Iteration 936, loss = 0.54553463
Iteration 937, loss = 0.55587027
Iteration 938, loss = 0.54836362
Iteration 939, loss = 0.54657529
Iteration 940, loss = 0.55127403
Iteration 941, loss = 0.56311845
Iteration 942, loss = 0.56570719
Iteration 943, loss = 0.56221651
Iteration 944, loss = 0.54003536
Iteration 945, loss = 0.54282004
Iteration 946, loss = 0.56381306
Iteration 947, loss = 0.55704267
Iteration 948, loss = 0.56006927
Iteration 949, loss = 0.54688355
Iteration 950, loss = 0.55671029
Iteration 951, loss = 0.53973544
Iteration 952, loss = 0.55495812
Iteration 953, loss = 0.55679710
Iteration 954, loss = 0.54379966
Iteration 955, loss = 0.54318366
Iteration 956, loss = 0.54777368
Iteration 957, loss = 0.54869205
Iteration 958, loss = 0.54669703
Iteration 959, loss = 0.55277077
Iteration 960, loss = 0.54220401
Iteration 961, loss = 0.55568810
Iteration 962, loss = 0.55773380
Iteration 963, loss = 0.57415195
Iteration 964, loss = 0.56003536
Iteration 965, loss = 0.54812594
Iteration 966, loss = 0.54904114
Iteration 967, loss = 0.55190542
Iteration 968, loss = 0.55139723
Iteration 969, loss = 0.55667153
Iteration 970, loss = 0.55329614
Iteration 971, loss = 0.54746144
Iteration 972, loss = 0.58117960
Iteration 973, loss = 0.59002898
Iteration 974, loss = 0.62571442
Iteration 975, loss = 0.65632608
Iteration 976, loss = 0.63988485
Iteration 977, loss = 0.64296163
Iteration 978, loss = 0.61713455
Iteration 979, loss = 0.67939243
Iteration 980, loss = 0.63993256
Iteration 981, loss = 0.61842536
Iteration 982, loss = 0.60562191
Iteration 983, loss = 0.57365009
Iteration 984, loss = 0.55927449
Iteration 985, loss = 0.57196354
Iteration 986, loss = 0.56823900
Iteration 987, loss = 0.57645855
Iteration 988, loss = 0.55352588
Iteration 989, loss = 0.54537907
Iteration 990, loss = 0.54415162
Iteration 991, loss = 0.55117079
Iteration 992, loss = 0.54928598
Iteration 993, loss = 0.54132690
Iteration 994, loss = 0.54990788
Iteration 995, loss = 0.57792363
Iteration 996, loss = 0.58749181
Iteration 997, loss = 0.55047472
Iteration 998, loss = 0.57254948
Iteration 999, loss = 0.54848952
Iteration 1000, loss = 0.54142219
Run 1
Iteration 1, loss = 4086.39144039
Iteration 2, loss = 700.31780772
Iteration 3, loss = 272.72935709
Iteration 4, loss = 15.53538950
Iteration 5, loss = 38.72642576
Iteration 6, loss = 14.74019642
Iteration 7, loss = 1.80639915
Iteration 8, loss = 1.37809130
Iteration 9, loss = 2.66714390
Iteration 10, loss = 3.28941891
Iteration 11, loss = 2.53701912
Iteration 12, loss = 1.34445248
Iteration 13, loss = 1.11161047
Iteration 14, loss = 0.95813402
Iteration 15, loss = 0.93381467
Iteration 16, loss = 1.04694738
Iteration 17, loss = 0.97909236
Iteration 18, loss = 0.92441203
Iteration 19, loss = 0.91503067
Iteration 20, loss = 0.93607088
Iteration 21, loss = 0.93602384
Iteration 22, loss = 0.89674534
Iteration 23, loss = 0.88552724
Iteration 24, loss = 0.89651671
Iteration 25, loss = 0.86863533
Iteration 26, loss = 0.86056724
Iteration 27, loss = 0.83699559
Iteration 28, loss = 0.80483871
Iteration 29, loss = 0.78020050
Iteration 30, loss = 0.77054592
Iteration 31, loss = 0.74857086
Iteration 32, loss = 0.70886089
Iteration 33, loss = 0.70056932
Iteration 34, loss = 0.75762447
Iteration 35, loss = 0.70219997
Iteration 36, loss = 0.72744508
Iteration 37, loss = 0.74657224
Iteration 38, loss = 0.73835739
Iteration 39, loss = 0.67902992
Iteration 40, loss = 0.67921111
Iteration 41, loss = 0.66619430
Iteration 42, loss = 0.65947259
Iteration 43, loss = 0.64557706
Iteration 44, loss = 0.64506265
Iteration 45, loss = 0.65331501
Iteration 46, loss = 0.63727013
Iteration 47, loss = 0.65005073
Iteration 48, loss = 0.66023487
Iteration 49, loss = 0.64498410
Iteration 50, loss = 0.64496001
Iteration 51, loss = 0.61850769
Iteration 52, loss = 0.61712768
Iteration 53, loss = 0.63504537
Iteration 54, loss = 0.63293508
Iteration 55, loss = 0.61938659
Iteration 56, loss = 0.63065656
Iteration 57, loss = 0.63236488
Iteration 58, loss = 0.60999344
Iteration 59, loss = 0.62444964
Iteration 60, loss = 0.60725124
Iteration 61, loss = 0.63024763
Iteration 62, loss = 0.62751938
Iteration 63, loss = 0.61869032
Iteration 64, loss = 0.62007902
Iteration 65, loss = 0.62890998
Iteration 66, loss = 0.60367975
Iteration 67, loss = 0.61853205
Iteration 68, loss = 0.60423690
Iteration 69, loss = 0.60002094
Iteration 70, loss = 0.61574234
Iteration 71, loss = 0.60007081
Iteration 72, loss = 0.61073686
Iteration 73, loss = 0.60562190
Iteration 74, loss = 0.62238668
Iteration 75, loss = 0.60651413
Iteration 76, loss = 0.59891652
Iteration 77, loss = 0.58421033
Iteration 78, loss = 0.59232830
Iteration 79, loss = 0.60829089
Iteration 80, loss = 0.58913114
Iteration 81, loss = 0.59217653
Iteration 82, loss = 0.59030951
Iteration 83, loss = 0.58897763
Iteration 84, loss = 0.58184689
Iteration 85, loss = 0.60460325
Iteration 86, loss = 0.66125415
Iteration 87, loss = 0.60583420
Iteration 88, loss = 0.59252607
Iteration 89, loss = 0.60634794
Iteration 90, loss = 0.58181057
Iteration 91, loss = 0.58479310
Iteration 92, loss = 0.58078181
Iteration 93, loss = 0.59160311
Iteration 94, loss = 0.58148660
Iteration 95, loss = 0.59362239
Iteration 96, loss = 0.57496431
Iteration 97, loss = 0.57914733
Iteration 98, loss = 0.62511496
Iteration 99, loss = 0.60235441
Iteration 100, loss = 0.57968580
Iteration 101, loss = 0.58825545
Iteration 102, loss = 0.58235433
Iteration 103, loss = 0.57410277
Iteration 104, loss = 0.60499290
Iteration 105, loss = 0.64393759
Iteration 106, loss = 0.63558591
Iteration 107, loss = 0.67324852
Iteration 108, loss = 0.71236246
Iteration 109, loss = 0.62412881
Iteration 110, loss = 0.58301873
Iteration 111, loss = 0.57334969
Iteration 112, loss = 0.56745395
Iteration 113, loss = 0.57770001
Iteration 114, loss = 0.56447091
Iteration 115, loss = 0.56923368
Iteration 116, loss = 0.58613142
Iteration 117, loss = 0.57620906
Iteration 118, loss = 0.56530513
Iteration 119, loss = 0.56577000
Iteration 120, loss = 0.59150177
Iteration 121, loss = 0.57961189
Iteration 122, loss = 0.56137046
Iteration 123, loss = 0.57428029
Iteration 124, loss = 0.58726500
Iteration 125, loss = 0.58942463
Iteration 126, loss = 0.56384882
Iteration 127, loss = 0.55885848
Iteration 128, loss = 0.55456109
Iteration 129, loss = 0.55602749
Iteration 130, loss = 0.55610151
Iteration 131, loss = 0.55655914
Iteration 132, loss = 0.56775887
Iteration 133, loss = 0.55820469
Iteration 134, loss = 0.55423216
Iteration 135, loss = 0.55242871
Iteration 136, loss = 0.55557720
Iteration 137, loss = 0.54949796
Iteration 138, loss = 0.55064941
Iteration 139, loss = 0.55012514
Iteration 140, loss = 0.55065316
Iteration 141, loss = 0.56666755
Iteration 142, loss = 0.54685398
Iteration 143, loss = 0.55819558
Iteration 144, loss = 0.56239430
Iteration 145, loss = 0.58311465
Iteration 146, loss = 0.54540249
Iteration 147, loss = 0.54736277
Iteration 148, loss = 0.54402479
Iteration 149, loss = 0.54623270
Iteration 150, loss = 0.54561740
Iteration 151, loss = 0.53925340
Iteration 152, loss = 0.54437739
Iteration 153, loss = 0.54889494
Iteration 154, loss = 0.54889309
Iteration 155, loss = 0.54112945
Iteration 156, loss = 0.55529227
Iteration 157, loss = 0.54549533
Iteration 158, loss = 0.53591773
Iteration 159, loss = 0.56314285
Iteration 160, loss = 0.56444831
Iteration 161, loss = 0.55097958
Iteration 162, loss = 0.56435118
Iteration 163, loss = 0.53580419
Iteration 164, loss = 0.53474462
Iteration 165, loss = 0.53779493
Iteration 166, loss = 0.53664108
Iteration 167, loss = 0.53608097
Iteration 168, loss = 0.54972851
Iteration 169, loss = 0.54548957
Iteration 170, loss = 0.56679409
Iteration 171, loss = 0.58642350
Iteration 172, loss = 0.52294182
Iteration 173, loss = 0.54112786
Iteration 174, loss = 0.53686063
Iteration 175, loss = 0.52125360
Iteration 176, loss = 0.53140191
Iteration 177, loss = 0.53552733
Iteration 178, loss = 0.52332655
Iteration 179, loss = 0.53476604
Iteration 180, loss = 0.51730743
Iteration 181, loss = 0.51264739
Iteration 182, loss = 0.52297194
Iteration 183, loss = 0.50918207
Iteration 184, loss = 0.51521742
Iteration 185, loss = 0.51359910
Iteration 186, loss = 0.50829187
Iteration 187, loss = 0.50648559
Iteration 188, loss = 0.50158666
Iteration 189, loss = 0.50868118
Iteration 190, loss = 0.50975971
Iteration 191, loss = 0.50223228
Iteration 192, loss = 0.50194619
Iteration 193, loss = 0.50362505
Iteration 194, loss = 0.50952406
Iteration 195, loss = 0.50898052
Iteration 196, loss = 0.51089457
Iteration 197, loss = 0.50927197
Iteration 198, loss = 0.52193325
Iteration 199, loss = 0.51228456
Iteration 200, loss = 0.48632267
Iteration 201, loss = 0.51620229
Iteration 202, loss = 0.49836959
Iteration 203, loss = 0.48709587
Iteration 204, loss = 0.48285842
Iteration 205, loss = 0.48174866
Iteration 206, loss = 0.47694664
Iteration 207, loss = 0.50142707
Iteration 208, loss = 0.53235790
Iteration 209, loss = 0.50979411
Iteration 210, loss = 0.54924648
Iteration 211, loss = 0.54133464
Iteration 212, loss = 0.59722430
Iteration 213, loss = 0.54489333
Iteration 214, loss = 0.52230624
Iteration 215, loss = 0.52406390
Iteration 216, loss = 0.50542236
Iteration 217, loss = 0.52053125
Iteration 218, loss = 0.51342829
Iteration 219, loss = 0.50950466
Iteration 220, loss = 0.54840268
Iteration 221, loss = 0.57524690
Iteration 222, loss = 0.61723035
Iteration 223, loss = 0.55241630
Iteration 224, loss = 0.54937447
Iteration 225, loss = 0.50557873
Iteration 226, loss = 0.50899832
Iteration 227, loss = 0.49754224
Iteration 228, loss = 0.49994364
Iteration 229, loss = 0.49027760
Iteration 230, loss = 0.51242621
Iteration 231, loss = 0.50206661
Iteration 232, loss = 0.47699510
Iteration 233, loss = 0.47820564
Iteration 234, loss = 0.48370618
Iteration 235, loss = 0.48565055
Iteration 236, loss = 0.48883768
Iteration 237, loss = 0.49490708
Iteration 238, loss = 0.48132030
Iteration 239, loss = 0.47442458
Iteration 240, loss = 0.48898598
Iteration 241, loss = 0.53479841
Iteration 242, loss = 0.53537360
Iteration 243, loss = 0.54204047
Iteration 244, loss = 0.53685887
Iteration 245, loss = 0.59378739
Iteration 246, loss = 0.54854850
Iteration 247, loss = 0.51020827
Iteration 248, loss = 0.50227238
Iteration 249, loss = 0.48396097
Iteration 250, loss = 0.47338839
Iteration 251, loss = 0.51332359
Iteration 252, loss = 0.51292742
Iteration 253, loss = 0.48081048
Iteration 254, loss = 0.47931303
Iteration 255, loss = 0.48180837
Iteration 256, loss = 0.47673368
Iteration 257, loss = 0.47610635
Iteration 258, loss = 0.47734615
Iteration 259, loss = 0.47839240
Iteration 260, loss = 0.47179295
Iteration 261, loss = 0.47026507
Iteration 262, loss = 0.46804482
Iteration 263, loss = 0.47773230
Iteration 264, loss = 0.49439864
Iteration 265, loss = 0.49240073
Iteration 266, loss = 0.47958906
Iteration 267, loss = 0.48264046
Iteration 268, loss = 0.48568842
Iteration 269, loss = 0.51883502
Iteration 270, loss = 0.50746415
Iteration 271, loss = 0.49703170
Iteration 272, loss = 0.49118758
Iteration 273, loss = 0.46530036
Iteration 274, loss = 0.46915285
Iteration 275, loss = 0.47127028
Iteration 276, loss = 0.47561838
Iteration 277, loss = 0.46715243
Iteration 278, loss = 0.46429178
Iteration 279, loss = 0.46633288
Iteration 280, loss = 0.47196486
Iteration 281, loss = 0.47993034
Iteration 282, loss = 0.47769785
Iteration 283, loss = 0.46200289
Iteration 284, loss = 0.46683476
Iteration 285, loss = 0.46214729
Iteration 286, loss = 0.46330850
Iteration 287, loss = 0.47123580
Iteration 288, loss = 0.47952326
Iteration 289, loss = 0.46615854
Iteration 290, loss = 0.47610611
Iteration 291, loss = 0.46629731
Iteration 292, loss = 0.48634265
Iteration 293, loss = 0.47515732
Iteration 294, loss = 0.52321605
Iteration 295, loss = 0.55760540
Iteration 296, loss = 0.53528081
Iteration 297, loss = 0.52448882
Iteration 298, loss = 0.47923443
Iteration 299, loss = 0.48380746
Iteration 300, loss = 0.46754444
Iteration 301, loss = 0.46782685
Iteration 302, loss = 0.46109541
Iteration 303, loss = 0.47503545
Iteration 304, loss = 0.46989749
Iteration 305, loss = 0.47730765
Iteration 306, loss = 0.46691973
Iteration 307, loss = 0.46490362
Iteration 308, loss = 0.48361677
Iteration 309, loss = 0.47436576
Iteration 310, loss = 0.47722177
Iteration 311, loss = 0.46292638
Iteration 312, loss = 0.46727220
Iteration 313, loss = 0.47870377
Iteration 314, loss = 0.45756186
Iteration 315, loss = 0.45974665
Iteration 316, loss = 0.47639234
Iteration 317, loss = 0.47510537
Iteration 318, loss = 0.52422873
Iteration 319, loss = 0.53305510
Iteration 320, loss = 0.49947670
Iteration 321, loss = 0.53498262
Iteration 322, loss = 0.52018411
Iteration 323, loss = 0.49331053
Iteration 324, loss = 0.47223886
Iteration 325, loss = 0.46373772
Iteration 326, loss = 0.47230796
Iteration 327, loss = 0.46196263
Iteration 328, loss = 0.46222465
Iteration 329, loss = 0.46458669
Iteration 330, loss = 0.46000966
Iteration 331, loss = 0.46396796
Iteration 332, loss = 0.48188155
Iteration 333, loss = 0.46632806
Iteration 334, loss = 0.46233161
Iteration 335, loss = 0.50758037
Iteration 336, loss = 0.63726303
Iteration 337, loss = 0.58976809
Iteration 338, loss = 0.55259763
Iteration 339, loss = 0.53152470
Iteration 340, loss = 0.51962943
Iteration 341, loss = 0.49218839
Iteration 342, loss = 0.50413360
Iteration 343, loss = 0.49748314
Iteration 344, loss = 0.48712396
Iteration 345, loss = 0.48809951
Iteration 346, loss = 0.46708992
Iteration 347, loss = 0.45904608
Iteration 348, loss = 0.46325458
Iteration 349, loss = 0.45992619
Iteration 350, loss = 0.46340812
Iteration 351, loss = 0.46311309
Iteration 352, loss = 0.46397537
Iteration 353, loss = 0.46493412
Iteration 354, loss = 0.46351927
Iteration 355, loss = 0.47162572
Iteration 356, loss = 0.49015551
Iteration 357, loss = 0.47931019
Iteration 358, loss = 0.49266914
Iteration 359, loss = 0.47712513
Iteration 360, loss = 0.46850785
Iteration 361, loss = 0.45721069
Iteration 362, loss = 0.46161644
Iteration 363, loss = 0.46442508
Iteration 364, loss = 0.46255218
Iteration 365, loss = 0.47172215
Iteration 366, loss = 0.46160866
Iteration 367, loss = 0.46336067
Iteration 368, loss = 0.46425742
Iteration 369, loss = 0.45608936
Iteration 370, loss = 0.45786528
Iteration 371, loss = 0.46520602
Iteration 372, loss = 0.50542822
Iteration 373, loss = 0.45743512
Iteration 374, loss = 0.45647919
Iteration 375, loss = 0.49252676
Iteration 376, loss = 0.46324973
Iteration 377, loss = 0.46043787
Iteration 378, loss = 0.46387119
Iteration 379, loss = 0.49067915
Iteration 380, loss = 0.48447020
Iteration 381, loss = 0.48506652
Iteration 382, loss = 0.46615352
Iteration 383, loss = 0.46052213
Iteration 384, loss = 0.45974555
Iteration 385, loss = 0.46625696
Iteration 386, loss = 0.49278472
Iteration 387, loss = 0.47591143
Iteration 388, loss = 0.46224622
Iteration 389, loss = 0.48507470
Iteration 390, loss = 0.47255166
Iteration 391, loss = 0.46249603
Iteration 392, loss = 0.45369226
Iteration 393, loss = 0.45911460
Iteration 394, loss = 0.47351973
Iteration 395, loss = 0.45973572
Iteration 396, loss = 0.47301153
Iteration 397, loss = 0.50541366
Iteration 398, loss = 0.54167398
Iteration 399, loss = 0.51891272
Iteration 400, loss = 0.49391825
Iteration 401, loss = 0.47184427
Iteration 402, loss = 0.46383783
Iteration 403, loss = 0.45918868
Iteration 404, loss = 0.46356266
Iteration 405, loss = 0.46747114
Iteration 406, loss = 0.46083601
Iteration 407, loss = 0.45554827
Iteration 408, loss = 0.45274002
Iteration 409, loss = 0.45750751
Iteration 410, loss = 0.45237029
Iteration 411, loss = 0.45801079
Iteration 412, loss = 0.45730759
Iteration 413, loss = 0.46591586
Iteration 414, loss = 0.45354281
Iteration 415, loss = 0.45860280
Iteration 416, loss = 0.45445168
Iteration 417, loss = 0.46123361
Iteration 418, loss = 0.45641799
Iteration 419, loss = 0.45606621
Iteration 420, loss = 0.46382326
Iteration 421, loss = 0.46497220
Iteration 422, loss = 0.45590145
Iteration 423, loss = 0.45888645
Iteration 424, loss = 0.46341915
Iteration 425, loss = 0.45531613
Iteration 426, loss = 0.45291734
Iteration 427, loss = 0.45894889
Iteration 428, loss = 0.45404833
Iteration 429, loss = 0.46650043
Iteration 430, loss = 0.46073249
Iteration 431, loss = 0.45788250
Iteration 432, loss = 0.46646678
Iteration 433, loss = 0.45793411
Iteration 434, loss = 0.45842869
Iteration 435, loss = 0.46179985
Iteration 436, loss = 0.45834538
Iteration 437, loss = 0.46070271
Iteration 438, loss = 0.46258628
Iteration 439, loss = 0.45574413
Iteration 440, loss = 0.45456401
Iteration 441, loss = 0.45703313
Iteration 442, loss = 0.45802140
Iteration 443, loss = 0.45434020
Iteration 444, loss = 0.46420299
Iteration 445, loss = 0.45758945
Iteration 446, loss = 0.45500801
Iteration 447, loss = 0.45479002
Iteration 448, loss = 0.45249221
Iteration 449, loss = 0.46463709
Iteration 450, loss = 0.46167844
Iteration 451, loss = 0.45461718
Iteration 452, loss = 0.45700231
Iteration 453, loss = 0.47777909
Iteration 454, loss = 0.50581284
Iteration 455, loss = 0.50963865
Iteration 456, loss = 0.49531308
Iteration 457, loss = 0.45473837
Iteration 458, loss = 0.45459035
Iteration 459, loss = 0.45112021
Iteration 460, loss = 0.47373160
Iteration 461, loss = 0.46133693
Iteration 462, loss = 0.47329819
Iteration 463, loss = 0.49632579
Iteration 464, loss = 0.48903818
Iteration 465, loss = 0.48781082
Iteration 466, loss = 0.46700050
Iteration 467, loss = 0.47138641
Iteration 468, loss = 0.45389352
Iteration 469, loss = 0.46413946
Iteration 470, loss = 0.46209307
Iteration 471, loss = 0.46251676
Iteration 472, loss = 0.48776972
Iteration 473, loss = 0.47594656
Iteration 474, loss = 0.47383256
Iteration 475, loss = 0.50021478
Iteration 476, loss = 0.53015375
Iteration 477, loss = 0.49804635
Iteration 478, loss = 0.48305529
Iteration 479, loss = 0.47466878
Iteration 480, loss = 0.48698301
Iteration 481, loss = 0.49397817
Iteration 482, loss = 0.49746221
Iteration 483, loss = 0.49660849
Iteration 484, loss = 0.51013837
Iteration 485, loss = 0.51373956
Iteration 486, loss = 0.50170528
Iteration 487, loss = 0.48387144
Iteration 488, loss = 0.48233353
Iteration 489, loss = 0.47218605
Iteration 490, loss = 0.47225119
Iteration 491, loss = 0.49982805
Iteration 492, loss = 0.48053779
Iteration 493, loss = 0.47633338
Iteration 494, loss = 0.47168872
Iteration 495, loss = 0.45798655
Iteration 496, loss = 0.45512515
Iteration 497, loss = 0.45677033
Iteration 498, loss = 0.45636898
Iteration 499, loss = 0.46282875
Iteration 500, loss = 0.45378516
Iteration 501, loss = 0.46314493
Iteration 502, loss = 0.46703599
Iteration 503, loss = 0.46766059
Iteration 504, loss = 0.46268615
Iteration 505, loss = 0.47984701
Iteration 506, loss = 0.45626049
Iteration 507, loss = 0.46401556
Iteration 508, loss = 0.45330326
Iteration 509, loss = 0.46091936
Iteration 510, loss = 0.47103402
Iteration 511, loss = 0.48002249
Iteration 512, loss = 0.49351620
Iteration 513, loss = 0.46505214
Iteration 514, loss = 0.44919151
Iteration 515, loss = 0.45435645
Iteration 516, loss = 0.47202480
Iteration 517, loss = 0.49692131
Iteration 518, loss = 0.46923784
Iteration 519, loss = 0.45480310
Iteration 520, loss = 0.45404251
Iteration 521, loss = 0.45376806
Iteration 522, loss = 0.45725795
Iteration 523, loss = 0.44923419
Iteration 524, loss = 0.46988760
Iteration 525, loss = 0.45425667
Iteration 526, loss = 0.47438900
Iteration 527, loss = 0.45082457
Iteration 528, loss = 0.45270217
Iteration 529, loss = 0.44895149
Iteration 530, loss = 0.45173843
Iteration 531, loss = 0.44801723
Iteration 532, loss = 0.45283672
Iteration 533, loss = 0.44806002
Iteration 534, loss = 0.45019174
Iteration 535, loss = 0.45038872
Iteration 536, loss = 0.44955666
Iteration 537, loss = 0.44726965
Iteration 538, loss = 0.44399599
Iteration 539, loss = 0.44685858
Iteration 540, loss = 0.45162058
Iteration 541, loss = 0.45164634
Iteration 542, loss = 0.44768583
Iteration 543, loss = 0.45713540
Iteration 544, loss = 0.46017149
Iteration 545, loss = 0.45736427
Iteration 546, loss = 0.45882018
Iteration 547, loss = 0.44956266
Iteration 548, loss = 0.45318885
Iteration 549, loss = 0.45042893
Iteration 550, loss = 0.45412571
Iteration 551, loss = 0.45217580
Iteration 552, loss = 0.47304357
Iteration 553, loss = 0.44558288
Iteration 554, loss = 0.45001806
Iteration 555, loss = 0.44957675
Iteration 556, loss = 0.45506386
Iteration 557, loss = 0.46398230
Iteration 558, loss = 0.44798296
Iteration 559, loss = 0.45290848
Iteration 560, loss = 0.44389849
Iteration 561, loss = 0.44767954
Iteration 562, loss = 0.45311991
Iteration 563, loss = 0.46294537
Iteration 564, loss = 0.47099726
Iteration 565, loss = 0.50126108
Iteration 566, loss = 0.52599254
Iteration 567, loss = 0.48451937
Iteration 568, loss = 0.51028895
Iteration 569, loss = 0.47834941
Iteration 570, loss = 0.47520614
Iteration 571, loss = 0.44448677
Iteration 572, loss = 0.46235312
Iteration 573, loss = 0.45420313
Iteration 574, loss = 0.45285233
Iteration 575, loss = 0.45377648
Iteration 576, loss = 0.46202395
Iteration 577, loss = 0.44840767
Iteration 578, loss = 0.45434641
Iteration 579, loss = 0.46288406
Iteration 580, loss = 0.47851276
Iteration 581, loss = 0.46819621
Iteration 582, loss = 0.48102176
Iteration 583, loss = 0.48970838
Iteration 584, loss = 0.47595116
Iteration 585, loss = 0.45921213
Iteration 586, loss = 0.45102350
Iteration 587, loss = 0.45085469
Iteration 588, loss = 0.44871598
Iteration 589, loss = 0.44991201
Iteration 590, loss = 0.45251915
Iteration 591, loss = 0.46150900
Iteration 592, loss = 0.45920018
Iteration 593, loss = 0.46032024
Iteration 594, loss = 0.44306731
Iteration 595, loss = 0.45104598
Iteration 596, loss = 0.45253765
Iteration 597, loss = 0.45218394
Iteration 598, loss = 0.46080451
Iteration 599, loss = 0.44695174
Iteration 600, loss = 0.44051660
Iteration 601, loss = 0.44462056
Iteration 602, loss = 0.44414702
Iteration 603, loss = 0.44728518
Iteration 604, loss = 0.44937429
Iteration 605, loss = 0.44952767
Iteration 606, loss = 0.44976716
Iteration 607, loss = 0.47286658
Iteration 608, loss = 0.47096327
Iteration 609, loss = 0.47379350
Iteration 610, loss = 0.46831279
Iteration 611, loss = 0.45552490
Iteration 612, loss = 0.49381222
Iteration 613, loss = 0.46543277
Iteration 614, loss = 0.47004142
Iteration 615, loss = 0.45822027
Iteration 616, loss = 0.44943515
Iteration 617, loss = 0.44750156
Iteration 618, loss = 0.45196806
Iteration 619, loss = 0.45712724
Iteration 620, loss = 0.44807709
Iteration 621, loss = 0.44245943
Iteration 622, loss = 0.44398090
Iteration 623, loss = 0.45143367
Iteration 624, loss = 0.44548062
Iteration 625, loss = 0.44594159
Iteration 626, loss = 0.43982656
Iteration 627, loss = 0.43983619
Iteration 628, loss = 0.44313117
Iteration 629, loss = 0.43811073
Iteration 630, loss = 0.44373814
Iteration 631, loss = 0.43876412
Iteration 632, loss = 0.44995407
Iteration 633, loss = 0.44152995
Iteration 634, loss = 0.44597109
Iteration 635, loss = 0.45134742
Iteration 636, loss = 0.46425226
Iteration 637, loss = 0.50518124
Iteration 638, loss = 0.48947040
Iteration 639, loss = 0.46910918
Iteration 640, loss = 0.44754545
Iteration 641, loss = 0.44879019
Iteration 642, loss = 0.45239251
Iteration 643, loss = 0.45714946
Iteration 644, loss = 0.44398524
Iteration 645, loss = 0.43700724
Iteration 646, loss = 0.45064954
Iteration 647, loss = 0.43439498
Iteration 648, loss = 0.44163366
Iteration 649, loss = 0.45299193
Iteration 650, loss = 0.47659070
Iteration 651, loss = 0.49471229
Iteration 652, loss = 0.47035219
Iteration 653, loss = 0.46863145
Iteration 654, loss = 0.45704946
Iteration 655, loss = 0.47136547
Iteration 656, loss = 0.47439023
Iteration 657, loss = 0.45052158
Iteration 658, loss = 0.45807631
Iteration 659, loss = 0.47990474
Iteration 660, loss = 0.46972954
Iteration 661, loss = 0.48481235
Iteration 662, loss = 0.49449045
Iteration 663, loss = 0.56681377
Iteration 664, loss = 0.49536218
Iteration 665, loss = 0.47429138
Iteration 666, loss = 0.45244929
Iteration 667, loss = 0.46167164
Iteration 668, loss = 0.45693215
Iteration 669, loss = 0.48330181
Iteration 670, loss = 0.45484667
Iteration 671, loss = 0.45918144
Iteration 672, loss = 0.45126037
Iteration 673, loss = 0.45320673
Iteration 674, loss = 0.47994115
Iteration 675, loss = 0.50411629
Iteration 676, loss = 0.48252838
Iteration 677, loss = 0.52092629
Iteration 678, loss = 0.50000787
Iteration 679, loss = 0.47172326
Iteration 680, loss = 0.47183855
Iteration 681, loss = 0.47423257
Iteration 682, loss = 0.45416200
Iteration 683, loss = 0.47457284
Iteration 684, loss = 0.45938468
Iteration 685, loss = 0.46036313
Iteration 686, loss = 0.45021484
Iteration 687, loss = 0.46692651
Iteration 688, loss = 0.45425525
Iteration 689, loss = 0.44917468
Iteration 690, loss = 0.44809748
Iteration 691, loss = 0.45260195
Iteration 692, loss = 0.45663798
Iteration 693, loss = 0.45490190
Iteration 694, loss = 0.46201932
Iteration 695, loss = 0.51776251
Iteration 696, loss = 0.51225637
Iteration 697, loss = 0.50328305
Iteration 698, loss = 0.47752955
Iteration 699, loss = 0.46811411
Iteration 700, loss = 0.46001697
Iteration 701, loss = 0.44512113
Iteration 702, loss = 0.45051393
Iteration 703, loss = 0.44639933
Iteration 704, loss = 0.45743507
Iteration 705, loss = 0.45074906
Iteration 706, loss = 0.45439056
Iteration 707, loss = 0.46343383
Iteration 708, loss = 0.44297696
Iteration 709, loss = 0.45294001
Iteration 710, loss = 0.47634907
Iteration 711, loss = 0.49224709
Iteration 712, loss = 0.48030861
Iteration 713, loss = 0.44886750
Iteration 714, loss = 0.45979533
Iteration 715, loss = 0.45487276
Iteration 716, loss = 0.46506116
Iteration 717, loss = 0.44093104
Iteration 718, loss = 0.45867985
Iteration 719, loss = 0.46298942
Iteration 720, loss = 0.47422726
Iteration 721, loss = 0.46880645
Iteration 722, loss = 0.46480241
Iteration 723, loss = 0.44879121
Iteration 724, loss = 0.45758680
Iteration 725, loss = 0.44459734
Iteration 726, loss = 0.44715248
Iteration 727, loss = 0.43869521
Iteration 728, loss = 0.45407733
Iteration 729, loss = 0.46855900
Iteration 730, loss = 0.48052384
Iteration 731, loss = 0.45675457
Iteration 732, loss = 0.44047597
Iteration 733, loss = 0.43540979
Iteration 734, loss = 0.44352726
Iteration 735, loss = 0.43460456
Iteration 736, loss = 0.44201397
Iteration 737, loss = 0.44613177
Iteration 738, loss = 0.48118968
Iteration 739, loss = 0.45088655
Iteration 740, loss = 0.44862303
Iteration 741, loss = 0.46269697
Iteration 742, loss = 0.44174464
Iteration 743, loss = 0.43655465
Iteration 744, loss = 0.43753947
Iteration 745, loss = 0.43919391
Iteration 746, loss = 0.44924904
Iteration 747, loss = 0.45318749
Iteration 748, loss = 0.44687780
Iteration 749, loss = 0.44719132
Iteration 750, loss = 0.44378238
Iteration 751, loss = 0.43798032
Iteration 752, loss = 0.44898869
Iteration 753, loss = 0.46477468
Iteration 754, loss = 0.46572523
Iteration 755, loss = 0.48528147
Iteration 756, loss = 0.48400739
Iteration 757, loss = 0.49560921
Iteration 758, loss = 0.46713870
Iteration 759, loss = 0.45685561
Iteration 760, loss = 0.45267268
Iteration 761, loss = 0.45535782
Iteration 762, loss = 0.45400612
Iteration 763, loss = 0.44485458
Iteration 764, loss = 0.44270488
Iteration 765, loss = 0.43621809
Iteration 766, loss = 0.44358026
Iteration 767, loss = 0.43733071
Iteration 768, loss = 0.43868004
Iteration 769, loss = 0.44084938
Iteration 770, loss = 0.44634889
Iteration 771, loss = 0.43437451
Iteration 772, loss = 0.43777843
Iteration 773, loss = 0.46912925
Iteration 774, loss = 0.45891441
Iteration 775, loss = 0.46823641
Iteration 776, loss = 0.43995336
Iteration 777, loss = 0.44965168
Iteration 778, loss = 0.45806609
Iteration 779, loss = 0.45787896
Iteration 780, loss = 0.48497420
Iteration 781, loss = 0.45870495
Iteration 782, loss = 0.44985291
Iteration 783, loss = 0.44147734
Iteration 784, loss = 0.44913123
Iteration 785, loss = 0.45838527
Iteration 786, loss = 0.53045836
Iteration 787, loss = 0.50141919
Iteration 788, loss = 0.48513871
Iteration 789, loss = 0.49078431
Iteration 790, loss = 0.44112359
Iteration 791, loss = 0.45662844
Iteration 792, loss = 0.46359739
Iteration 793, loss = 0.47458626
Iteration 794, loss = 0.45375043
Iteration 795, loss = 0.45426790
Iteration 796, loss = 0.45449659
Iteration 797, loss = 0.47189715
Iteration 798, loss = 0.48693532
Iteration 799, loss = 0.47015753
Iteration 800, loss = 0.46634920
Iteration 801, loss = 0.45368735
Iteration 802, loss = 0.46370737
Iteration 803, loss = 0.44143106
Iteration 804, loss = 0.47028661
Iteration 805, loss = 0.45444388
Iteration 806, loss = 0.48718443
Iteration 807, loss = 0.48449346
Iteration 808, loss = 0.50643180
Iteration 809, loss = 0.50331947
Iteration 810, loss = 0.46947163
Iteration 811, loss = 0.46766295
Iteration 812, loss = 0.46637731
Iteration 813, loss = 0.44403577
Iteration 814, loss = 0.48237541
Iteration 815, loss = 0.46281507
Iteration 816, loss = 0.45296309
Iteration 817, loss = 0.44886648
Iteration 818, loss = 0.44386858
Iteration 819, loss = 0.44980418
Iteration 820, loss = 0.44075077
Iteration 821, loss = 0.42921691
Iteration 822, loss = 0.43578859
Iteration 823, loss = 0.43988609
Iteration 824, loss = 0.43924493
Iteration 825, loss = 0.46315944
Iteration 826, loss = 0.49232585
Iteration 827, loss = 0.47748512
Iteration 828, loss = 0.49891757
Iteration 829, loss = 0.51920279
Iteration 830, loss = 0.50541854
Iteration 831, loss = 0.48031335
Iteration 832, loss = 0.45606932
Iteration 833, loss = 0.46861058
Iteration 834, loss = 0.45101864
Iteration 835, loss = 0.46700577
Iteration 836, loss = 0.46296648
Iteration 837, loss = 0.44960668
Iteration 838, loss = 0.46489825
Iteration 839, loss = 0.44052108
Iteration 840, loss = 0.44165305
Iteration 841, loss = 0.44157748
Iteration 842, loss = 0.44070077
Iteration 843, loss = 0.43689708
Iteration 844, loss = 0.43774558
Iteration 845, loss = 0.43625304
Iteration 846, loss = 0.43433699
Iteration 847, loss = 0.43396895
Iteration 848, loss = 0.43102364
Iteration 849, loss = 0.42911564
Iteration 850, loss = 0.43485242
Iteration 851, loss = 0.43219830
Iteration 852, loss = 0.43230245
Iteration 853, loss = 0.43484874
Iteration 854, loss = 0.42971310
Iteration 855, loss = 0.43908925
Iteration 856, loss = 0.44162828
Iteration 857, loss = 0.43228556
Iteration 858, loss = 0.43074862
Iteration 859, loss = 0.42732061
Iteration 860, loss = 0.42585761
Iteration 861, loss = 0.43055337
Iteration 862, loss = 0.44201808
Iteration 863, loss = 0.43272768
Iteration 864, loss = 0.43632680
Iteration 865, loss = 0.42995822
Iteration 866, loss = 0.43158284
Iteration 867, loss = 0.42602544
Iteration 868, loss = 0.43800577
Iteration 869, loss = 0.43971583
Iteration 870, loss = 0.44790894
Iteration 871, loss = 0.44046356
Iteration 872, loss = 0.43868601
Iteration 873, loss = 0.44086416
Iteration 874, loss = 0.43446790
Iteration 875, loss = 0.44305399
Iteration 876, loss = 0.43279715
Iteration 877, loss = 0.45418856
Iteration 878, loss = 0.47123512
Iteration 879, loss = 0.47074081
Iteration 880, loss = 0.44105488
Iteration 881, loss = 0.44551233
Iteration 882, loss = 0.45604153
Iteration 883, loss = 0.43306539
Iteration 884, loss = 0.45355734
Iteration 885, loss = 0.43554434
Iteration 886, loss = 0.45073859
Iteration 887, loss = 0.48387584
Iteration 888, loss = 0.49559015
Iteration 889, loss = 0.46198943
Iteration 890, loss = 0.45018632
Iteration 891, loss = 0.44158421
Iteration 892, loss = 0.46107257
Iteration 893, loss = 0.44058295
Iteration 894, loss = 0.44414601
Iteration 895, loss = 0.43654794
Iteration 896, loss = 0.45116644
Iteration 897, loss = 0.43086432
Iteration 898, loss = 0.43286993
Iteration 899, loss = 0.44670571
Iteration 900, loss = 0.47061517
Iteration 901, loss = 0.46549625
Iteration 902, loss = 0.48055036
Iteration 903, loss = 0.45284553
Iteration 904, loss = 0.46502338
Iteration 905, loss = 0.46000452
Iteration 906, loss = 0.44541329
Iteration 907, loss = 0.43534210
Iteration 908, loss = 0.43960099
Iteration 909, loss = 0.46072126
Iteration 910, loss = 0.43836439
Iteration 911, loss = 0.43827477
Iteration 912, loss = 0.43648105
Iteration 913, loss = 0.42442246
Iteration 914, loss = 0.43528911
Iteration 915, loss = 0.42533920
Iteration 916, loss = 0.42948099
Iteration 917, loss = 0.42958013
Iteration 918, loss = 0.43580552
Iteration 919, loss = 0.42771865
Iteration 920, loss = 0.43048113
Iteration 921, loss = 0.43007069
Iteration 922, loss = 0.43049094
Iteration 923, loss = 0.42700541
Iteration 924, loss = 0.42339014
Iteration 925, loss = 0.42472346
Iteration 926, loss = 0.44445676
Iteration 927, loss = 0.45138837
Iteration 928, loss = 0.44211421
Iteration 929, loss = 0.43031904
Iteration 930, loss = 0.43361753
Iteration 931, loss = 0.42614070
Iteration 932, loss = 0.42965010
Iteration 933, loss = 0.44330915
Iteration 934, loss = 0.44579335
Iteration 935, loss = 0.45047981
Iteration 936, loss = 0.42382420
Iteration 937, loss = 0.42655619
Iteration 938, loss = 0.43627110
Iteration 939, loss = 0.43491868
Iteration 940, loss = 0.44930998
Iteration 941, loss = 0.43598614
Iteration 942, loss = 0.42275314
Iteration 943, loss = 0.44311999
Iteration 944, loss = 0.42308544
Iteration 945, loss = 0.45231935
Iteration 946, loss = 0.48083688
Iteration 947, loss = 0.43418191
Iteration 948, loss = 0.44582587
Iteration 949, loss = 0.44668661
Iteration 950, loss = 0.43634489
Iteration 951, loss = 0.44178115
Iteration 952, loss = 0.42698331
Iteration 953, loss = 0.43613344
Iteration 954, loss = 0.42726572
Iteration 955, loss = 0.42932131
Iteration 956, loss = 0.43386097
Iteration 957, loss = 0.45725287
Iteration 958, loss = 0.45437608
Iteration 959, loss = 0.45564919
Iteration 960, loss = 0.46205325
Iteration 961, loss = 0.49743715
Iteration 962, loss = 0.45512967
Iteration 963, loss = 0.46217109
Iteration 964, loss = 0.44191280
Iteration 965, loss = 0.44442826
Iteration 966, loss = 0.44546909
Iteration 967, loss = 0.43818652
Iteration 968, loss = 0.44328226
Iteration 969, loss = 0.43182663
Iteration 970, loss = 0.43260069
Iteration 971, loss = 0.41714225
Iteration 972, loss = 0.44049264
Iteration 973, loss = 0.43921127
Iteration 974, loss = 0.44667092
Iteration 975, loss = 0.43290850
Iteration 976, loss = 0.46524335
Iteration 977, loss = 0.50325478
Iteration 978, loss = 0.52447369
Iteration 979, loss = 0.48691274
Iteration 980, loss = 0.55158067
Iteration 981, loss = 0.48448280
Iteration 982, loss = 0.45687006
Iteration 983, loss = 0.45810841
Iteration 984, loss = 0.46523662
Iteration 985, loss = 0.44544441
Iteration 986, loss = 0.44568148
Iteration 987, loss = 0.44737512
Iteration 988, loss = 0.44786299
Iteration 989, loss = 0.46984659
Iteration 990, loss = 0.44476577
Iteration 991, loss = 0.43255111
Iteration 992, loss = 0.42670018
Iteration 993, loss = 0.42425880
Iteration 994, loss = 0.43728703
Iteration 995, loss = 0.43014160
Iteration 996, loss = 0.42263719
Iteration 997, loss = 0.42409795
Iteration 998, loss = 0.41856447
Iteration 999, loss = 0.42896476
Iteration 1000, loss = 0.44554799
Run 2
Iteration 1, loss = 1305.27623207
Iteration 2, loss = 282.01625689
Iteration 3, loss = 45.40799594
Iteration 4, loss = 79.21469141
Iteration 5, loss = 10.32709342
Iteration 6, loss = 24.76595170
Iteration 7, loss = 2.56648555
Iteration 8, loss = 6.48664754
Iteration 9, loss = 1.87576058
Iteration 10, loss = 3.16641168
Iteration 11, loss = 1.22688035
Iteration 12, loss = 1.53190683
Iteration 13, loss = 0.94103759
Iteration 14, loss = 1.76982056
Iteration 15, loss = 1.96154471
Iteration 16, loss = 1.35549699
Iteration 17, loss = 1.02690386
Iteration 18, loss = 0.96817473
Iteration 19, loss = 0.90772585
Iteration 20, loss = 0.86132079
Iteration 21, loss = 0.86152626
Iteration 22, loss = 0.86458117
Iteration 23, loss = 0.84805869
Iteration 24, loss = 0.83269320
Iteration 25, loss = 0.82259755
Iteration 26, loss = 0.81521834
Iteration 27, loss = 0.80227818
Iteration 28, loss = 0.91230133
Iteration 29, loss = 0.81961954
Iteration 30, loss = 0.79028924
Iteration 31, loss = 0.74938383
Iteration 32, loss = 0.72789266
Iteration 33, loss = 0.74339121
Iteration 34, loss = 0.72837648
Iteration 35, loss = 0.70850434
Iteration 36, loss = 0.70817994
Iteration 37, loss = 0.67660951
Iteration 38, loss = 0.67475082
Iteration 39, loss = 0.66151188
Iteration 40, loss = 0.65372290
Iteration 41, loss = 0.64739820
Iteration 42, loss = 0.65113395
Iteration 43, loss = 0.65066517
Iteration 44, loss = 0.68826077
Iteration 45, loss = 0.65266014
Iteration 46, loss = 0.65073615
Iteration 47, loss = 0.68085125
Iteration 48, loss = 0.62619343
Iteration 49, loss = 0.64376983
Iteration 50, loss = 0.65630257
Iteration 51, loss = 0.62798405
Iteration 52, loss = 0.63598044
Iteration 53, loss = 0.68713907
Iteration 54, loss = 0.67856496
Iteration 55, loss = 0.68670402
Iteration 56, loss = 0.66242919
Iteration 57, loss = 0.70685460
Iteration 58, loss = 0.66877341
Iteration 59, loss = 0.66146102
Iteration 60, loss = 0.68955879
Iteration 61, loss = 0.69310524
Iteration 62, loss = 0.64531251
Iteration 63, loss = 0.63279269
Iteration 64, loss = 0.61852349
Iteration 65, loss = 0.62052765
Iteration 66, loss = 0.63125352
Iteration 67, loss = 0.61941711
Iteration 68, loss = 0.61350674
Iteration 69, loss = 0.61632817
Iteration 70, loss = 0.61198708
Iteration 71, loss = 0.62360850
Iteration 72, loss = 0.60760711
Iteration 73, loss = 0.61730840
Iteration 74, loss = 0.61919038
Iteration 75, loss = 0.61032275
Iteration 76, loss = 0.61751801
Iteration 77, loss = 0.63871995
Iteration 78, loss = 0.62496224
Iteration 79, loss = 0.62534517
Iteration 80, loss = 0.70291873
Iteration 81, loss = 0.66722168
Iteration 82, loss = 0.62893386
Iteration 83, loss = 0.63228348
Iteration 84, loss = 0.60814624
Iteration 85, loss = 0.62151752
Iteration 86, loss = 0.61565064
Iteration 87, loss = 0.60588535
Iteration 88, loss = 0.60206958
Iteration 89, loss = 0.60422711
Iteration 90, loss = 0.59613468
Iteration 91, loss = 0.60533344
Iteration 92, loss = 0.61599093
Iteration 93, loss = 0.60117218
Iteration 94, loss = 0.62230738
Iteration 95, loss = 0.61929852
Iteration 96, loss = 0.59919330
Iteration 97, loss = 0.59140122
Iteration 98, loss = 0.59238669
Iteration 99, loss = 0.58888498
Iteration 100, loss = 0.58530996
Iteration 101, loss = 0.62132829
Iteration 102, loss = 0.61747761
Iteration 103, loss = 0.62378772
Iteration 104, loss = 0.61023963
Iteration 105, loss = 0.57892526
Iteration 106, loss = 0.57270446
Iteration 107, loss = 0.59902730
Iteration 108, loss = 0.57747925
Iteration 109, loss = 0.58849221
Iteration 110, loss = 0.58766951
Iteration 111, loss = 0.57787651
Iteration 112, loss = 0.58453406
Iteration 113, loss = 0.61028637
Iteration 114, loss = 0.57150513
Iteration 115, loss = 0.59973172
Iteration 116, loss = 0.58500031
Iteration 117, loss = 0.58727225
Iteration 118, loss = 0.57883449
Iteration 119, loss = 0.58231737
Iteration 120, loss = 0.56779403
Iteration 121, loss = 0.56992125
Iteration 122, loss = 0.62894263
Iteration 123, loss = 0.60083728
Iteration 124, loss = 0.57265740
Iteration 125, loss = 0.56218955
Iteration 126, loss = 0.59716510
Iteration 127, loss = 0.58314909
Iteration 128, loss = 0.61537425
Iteration 129, loss = 0.56755688
Iteration 130, loss = 0.56933079
Iteration 131, loss = 0.59720798
Iteration 132, loss = 0.62608070
Iteration 133, loss = 0.59005213
Iteration 134, loss = 0.59958814
Iteration 135, loss = 0.56821550
Iteration 136, loss = 0.56157947
Iteration 137, loss = 0.58477416
Iteration 138, loss = 0.56212530
Iteration 139, loss = 0.56085933
Iteration 140, loss = 0.57179347
Iteration 141, loss = 0.58079792
Iteration 142, loss = 0.57303608
Iteration 143, loss = 0.57333169
Iteration 144, loss = 0.56635132
Iteration 145, loss = 0.56931850
Iteration 146, loss = 0.55894478
Iteration 147, loss = 0.56313805
Iteration 148, loss = 0.56410720
Iteration 149, loss = 0.55361377
Iteration 150, loss = 0.56207931
Iteration 151, loss = 0.57065496
Iteration 152, loss = 0.55412411
Iteration 153, loss = 0.57100466
Iteration 154, loss = 0.60653477
Iteration 155, loss = 0.64585805
Iteration 156, loss = 0.60032278
Iteration 157, loss = 0.58705403
Iteration 158, loss = 0.56483477
Iteration 159, loss = 0.61765298
Iteration 160, loss = 0.57588723
Iteration 161, loss = 0.59533453
Iteration 162, loss = 0.56198695
Iteration 163, loss = 0.60758672
Iteration 164, loss = 0.59247979
Iteration 165, loss = 0.60985468
Iteration 166, loss = 0.59720539
Iteration 167, loss = 0.59339765
Iteration 168, loss = 0.57366062
Iteration 169, loss = 0.54894076
Iteration 170, loss = 0.54642455
Iteration 171, loss = 0.57131232
Iteration 172, loss = 0.59491950
Iteration 173, loss = 0.60974370
Iteration 174, loss = 0.60977973
Iteration 175, loss = 0.58406635
Iteration 176, loss = 0.62274905
Iteration 177, loss = 0.57079499
Iteration 178, loss = 0.59655824
Iteration 179, loss = 0.57264827
Iteration 180, loss = 0.56927152
Iteration 181, loss = 0.54790918
Iteration 182, loss = 0.53905870
Iteration 183, loss = 0.54872489
Iteration 184, loss = 0.54329964
Iteration 185, loss = 0.54053135
Iteration 186, loss = 0.54432753
Iteration 187, loss = 0.55723383
Iteration 188, loss = 0.55307159
Iteration 189, loss = 0.54637263
Iteration 190, loss = 0.53758728
Iteration 191, loss = 0.53951193
Iteration 192, loss = 0.54176747
Iteration 193, loss = 0.54965419
Iteration 194, loss = 0.56377438
Iteration 195, loss = 0.54165450
Iteration 196, loss = 0.54001781
Iteration 197, loss = 0.54974187
Iteration 198, loss = 0.55614552
Iteration 199, loss = 0.53621139
Iteration 200, loss = 0.55054587
Iteration 201, loss = 0.55337320
Iteration 202, loss = 0.56346603
Iteration 203, loss = 0.54322884
Iteration 204, loss = 0.54554070
Iteration 205, loss = 0.57868802
Iteration 206, loss = 0.58894455
Iteration 207, loss = 0.57855555
Iteration 208, loss = 0.60654481
Iteration 209, loss = 0.57896796
Iteration 210, loss = 0.57343356
Iteration 211, loss = 0.55000385
Iteration 212, loss = 0.52986589
Iteration 213, loss = 0.55806778
Iteration 214, loss = 0.55932064
Iteration 215, loss = 0.57111819
Iteration 216, loss = 0.57883439
Iteration 217, loss = 0.56768815
Iteration 218, loss = 0.52835010
Iteration 219, loss = 0.53026419
Iteration 220, loss = 0.53356520
Iteration 221, loss = 0.60606049
Iteration 222, loss = 0.54006898
Iteration 223, loss = 0.52761047
Iteration 224, loss = 0.55487143
Iteration 225, loss = 0.54621598
Iteration 226, loss = 0.55162180
Iteration 227, loss = 0.55905652
Iteration 228, loss = 0.56500787
Iteration 229, loss = 0.57297384
Iteration 230, loss = 0.63200344
Iteration 231, loss = 0.55675959
Iteration 232, loss = 0.56010502
Iteration 233, loss = 0.55164212
Iteration 234, loss = 0.55022365
Iteration 235, loss = 0.54650429
Iteration 236, loss = 0.56109652
Iteration 237, loss = 0.53847495
Iteration 238, loss = 0.53895554
Iteration 239, loss = 0.52745959
Iteration 240, loss = 0.53212166
Iteration 241, loss = 0.55178813
Iteration 242, loss = 0.54703648
Iteration 243, loss = 0.53481977
Iteration 244, loss = 0.52843496
Iteration 245, loss = 0.51559912
Iteration 246, loss = 0.51773334
Iteration 247, loss = 0.51129809
Iteration 248, loss = 0.51060910
Iteration 249, loss = 0.52839915
Iteration 250, loss = 0.51107805
Iteration 251, loss = 0.50922128
Iteration 252, loss = 0.50965789
Iteration 253, loss = 0.51262806
Iteration 254, loss = 0.56956766
Iteration 255, loss = 0.51950727
Iteration 256, loss = 0.53168382
Iteration 257, loss = 0.54171506
Iteration 258, loss = 0.51323731
Iteration 259, loss = 0.53417303
Iteration 260, loss = 0.52003564
Iteration 261, loss = 0.50326203
Iteration 262, loss = 0.51459459
Iteration 263, loss = 0.51933143
Iteration 264, loss = 0.50605744
Iteration 265, loss = 0.50291594
Iteration 266, loss = 0.50004911
Iteration 267, loss = 0.51369791
Iteration 268, loss = 0.54074950
Iteration 269, loss = 0.51797980
Iteration 270, loss = 0.50856096
Iteration 271, loss = 0.50900307
Iteration 272, loss = 0.56008604
Iteration 273, loss = 0.52107821
Iteration 274, loss = 0.54182218
Iteration 275, loss = 0.51817000
Iteration 276, loss = 0.50604255
Iteration 277, loss = 0.51875247
Iteration 278, loss = 0.55686684
Iteration 279, loss = 0.56117157
Iteration 280, loss = 0.51065091
Iteration 281, loss = 0.49844043
Iteration 282, loss = 0.50732811
Iteration 283, loss = 0.53100907
Iteration 284, loss = 0.48941611
Iteration 285, loss = 0.49277187
Iteration 286, loss = 0.50115000
Iteration 287, loss = 0.48656594
Iteration 288, loss = 0.49137193
Iteration 289, loss = 0.50716714
Iteration 290, loss = 0.54220074
Iteration 291, loss = 0.54235293
Iteration 292, loss = 0.50295775
Iteration 293, loss = 0.55121309
Iteration 294, loss = 0.52585920
Iteration 295, loss = 0.52630509
Iteration 296, loss = 0.50262154
Iteration 297, loss = 0.53929494
Iteration 298, loss = 0.50191744
Iteration 299, loss = 0.50060703
Iteration 300, loss = 0.48734026
Iteration 301, loss = 0.49111634
Iteration 302, loss = 0.48205767
Iteration 303, loss = 0.48699781
Iteration 304, loss = 0.49248389
Iteration 305, loss = 0.50819517
Iteration 306, loss = 0.50837949
Iteration 307, loss = 0.49772227
Iteration 308, loss = 0.51816998
Iteration 309, loss = 0.51430628
Iteration 310, loss = 0.57731139
Iteration 311, loss = 0.48034342
Iteration 312, loss = 0.49773937
Iteration 313, loss = 0.52487437
Iteration 314, loss = 0.49606525
Iteration 315, loss = 0.49616197
Iteration 316, loss = 0.49913812
Iteration 317, loss = 0.50360636
Iteration 318, loss = 0.52437540
Iteration 319, loss = 0.50072078
Iteration 320, loss = 0.48311947
Iteration 321, loss = 0.48269212
Iteration 322, loss = 0.47734978
Iteration 323, loss = 0.47320768
Iteration 324, loss = 0.47579984
Iteration 325, loss = 0.51395377
Iteration 326, loss = 0.48073876
Iteration 327, loss = 0.47622680
Iteration 328, loss = 0.49035608
Iteration 329, loss = 0.48199750
Iteration 330, loss = 0.48848975
Iteration 331, loss = 0.47870230
Iteration 332, loss = 0.49438985
Iteration 333, loss = 0.48014191
Iteration 334, loss = 0.50334035
Iteration 335, loss = 0.52004106
Iteration 336, loss = 0.50023069
Iteration 337, loss = 0.52056350
Iteration 338, loss = 0.48344204
Iteration 339, loss = 0.47214718
Iteration 340, loss = 0.47597043
Iteration 341, loss = 0.48090649
Iteration 342, loss = 0.47964223
Iteration 343, loss = 0.50518602
Iteration 344, loss = 0.49523840
Iteration 345, loss = 0.52619310
Iteration 346, loss = 0.56265340
Iteration 347, loss = 0.58561476
Iteration 348, loss = 0.57977426
Iteration 349, loss = 0.52931276
Iteration 350, loss = 0.54045690
Iteration 351, loss = 0.48528051
Iteration 352, loss = 0.50491549
Iteration 353, loss = 0.49099356
Iteration 354, loss = 0.49573782
Iteration 355, loss = 0.48865146
Iteration 356, loss = 0.48990020
Iteration 357, loss = 0.48943332
Iteration 358, loss = 0.49341639
Iteration 359, loss = 0.47395314
Iteration 360, loss = 0.47466989
Iteration 361, loss = 0.48592657
Iteration 362, loss = 0.48323715
Iteration 363, loss = 0.48632705
Iteration 364, loss = 0.48232132
Iteration 365, loss = 0.50836248
Iteration 366, loss = 0.50679978
Iteration 367, loss = 0.48494461
Iteration 368, loss = 0.48826928
Iteration 369, loss = 0.48716532
Iteration 370, loss = 0.53501566
Iteration 371, loss = 0.51645299
Iteration 372, loss = 0.49082440
Iteration 373, loss = 0.49788262
Iteration 374, loss = 0.47930305
Iteration 375, loss = 0.49006265
Iteration 376, loss = 0.48229301
Iteration 377, loss = 0.47506208
Iteration 378, loss = 0.51069957
Iteration 379, loss = 0.50031694
Iteration 380, loss = 0.47130402
Iteration 381, loss = 0.46798285
Iteration 382, loss = 0.46913996
Iteration 383, loss = 0.47317127
Iteration 384, loss = 0.47030734
Iteration 385, loss = 0.47969851
Iteration 386, loss = 0.46652580
Iteration 387, loss = 0.46872695
Iteration 388, loss = 0.49782051
Iteration 389, loss = 0.47038944
Iteration 390, loss = 0.49230216
Iteration 391, loss = 0.50318451
Iteration 392, loss = 0.48126933
Iteration 393, loss = 0.47409152
Iteration 394, loss = 0.49758361
Iteration 395, loss = 0.49713457
Iteration 396, loss = 0.46287513
Iteration 397, loss = 0.49558323
Iteration 398, loss = 0.46764618
Iteration 399, loss = 0.49678242
Iteration 400, loss = 0.47408508
Iteration 401, loss = 0.48123114
Iteration 402, loss = 0.48434587
Iteration 403, loss = 0.56101499
Iteration 404, loss = 0.50796909
Iteration 405, loss = 0.48877439
Iteration 406, loss = 0.50140443
Iteration 407, loss = 0.50894555
Iteration 408, loss = 0.47812804
Iteration 409, loss = 0.50152325
Iteration 410, loss = 0.48192563
Iteration 411, loss = 0.50554063
Iteration 412, loss = 0.46916578
Iteration 413, loss = 0.46879293
Iteration 414, loss = 0.47178440
Iteration 415, loss = 0.48781715
Iteration 416, loss = 0.49723194
Iteration 417, loss = 0.47757842
Iteration 418, loss = 0.50158184
Iteration 419, loss = 0.47796645
Iteration 420, loss = 0.46656832
Iteration 421, loss = 0.46365096
Iteration 422, loss = 0.47704538
Iteration 423, loss = 0.47546449
Iteration 424, loss = 0.49704159
Iteration 425, loss = 0.54041523
Iteration 426, loss = 0.50268562
Iteration 427, loss = 0.49337406
Iteration 428, loss = 0.46584393
Iteration 429, loss = 0.48043247
Iteration 430, loss = 0.46578806
Iteration 431, loss = 0.47493597
Iteration 432, loss = 0.48240092
Iteration 433, loss = 0.46897711
Iteration 434, loss = 0.49214856
Iteration 435, loss = 0.53977278
Iteration 436, loss = 0.50271961
Iteration 437, loss = 0.48758061
Iteration 438, loss = 0.49106088
Iteration 439, loss = 0.47846817
Iteration 440, loss = 0.46794078
Iteration 441, loss = 0.48350182
Iteration 442, loss = 0.48782692
Iteration 443, loss = 0.51088074
Iteration 444, loss = 0.48574800
Iteration 445, loss = 0.48291184
Iteration 446, loss = 0.50492217
Iteration 447, loss = 0.49609518
Iteration 448, loss = 0.47888205
Iteration 449, loss = 0.47882286
Iteration 450, loss = 0.48778050
Iteration 451, loss = 0.49349804
Iteration 452, loss = 0.53424004
Iteration 453, loss = 0.49614967
Iteration 454, loss = 0.48161624
Iteration 455, loss = 0.49731952
Iteration 456, loss = 0.50062873
Iteration 457, loss = 0.50576758
Iteration 458, loss = 0.47180801
Iteration 459, loss = 0.49860845
Iteration 460, loss = 0.49991880
Iteration 461, loss = 0.48808184
Iteration 462, loss = 0.49631158
Iteration 463, loss = 0.51599300
Iteration 464, loss = 0.52986637
Iteration 465, loss = 0.51156206
Iteration 466, loss = 0.49743412
Iteration 467, loss = 0.46647436
Iteration 468, loss = 0.47022917
Iteration 469, loss = 0.48246832
Iteration 470, loss = 0.48219834
Iteration 471, loss = 0.47207506
Iteration 472, loss = 0.48765511
Iteration 473, loss = 0.47820605
Iteration 474, loss = 0.48623115
Iteration 475, loss = 0.48816271
Iteration 476, loss = 0.47417460
Iteration 477, loss = 0.49994792
Iteration 478, loss = 0.48803376
Iteration 479, loss = 0.46371028
Iteration 480, loss = 0.46985489
Iteration 481, loss = 0.47080500
Iteration 482, loss = 0.49452939
Iteration 483, loss = 0.48780521
Iteration 484, loss = 0.50836978
Iteration 485, loss = 0.48429641
Iteration 486, loss = 0.50418420
Iteration 487, loss = 0.50510562
Iteration 488, loss = 0.49898047
Iteration 489, loss = 0.49541071
Iteration 490, loss = 0.47415938
Iteration 491, loss = 0.46925509
Iteration 492, loss = 0.47099295
Iteration 493, loss = 0.47264101
Iteration 494, loss = 0.49896002
Iteration 495, loss = 0.53090829
Iteration 496, loss = 0.48769795
Iteration 497, loss = 0.47222910
Iteration 498, loss = 0.48686862
Iteration 499, loss = 0.49025824
Iteration 500, loss = 0.50188710
Iteration 501, loss = 0.46543707
Iteration 502, loss = 0.47444418
Iteration 503, loss = 0.48332364
Iteration 504, loss = 0.48272542
Iteration 505, loss = 0.48893765
Iteration 506, loss = 0.47903911
Iteration 507, loss = 0.47187558
Iteration 508, loss = 0.47032001
Iteration 509, loss = 0.45778067
Iteration 510, loss = 0.47235520
Iteration 511, loss = 0.47122773
Iteration 512, loss = 0.48522430
Iteration 513, loss = 0.53025283
Iteration 514, loss = 0.49929736
Iteration 515, loss = 0.48682394
Iteration 516, loss = 0.50941636
Iteration 517, loss = 0.52880721
Iteration 518, loss = 0.49271935
Iteration 519, loss = 0.50306781
Iteration 520, loss = 0.47775926
Iteration 521, loss = 0.47006014
Iteration 522, loss = 0.46176086
Iteration 523, loss = 0.50460316
Iteration 524, loss = 0.47830491
Iteration 525, loss = 0.48764903
Iteration 526, loss = 0.50263696
Iteration 527, loss = 0.50145603
Iteration 528, loss = 0.50508035
Iteration 529, loss = 0.46975648
Iteration 530, loss = 0.48851522
Iteration 531, loss = 0.48694469
Iteration 532, loss = 0.48858782
Iteration 533, loss = 0.46301367
Iteration 534, loss = 0.46193133
Iteration 535, loss = 0.47675522
Iteration 536, loss = 0.48929885
Iteration 537, loss = 0.47412055
Iteration 538, loss = 0.47212273
Iteration 539, loss = 0.46285928
Iteration 540, loss = 0.46758433
Iteration 541, loss = 0.46866190
Iteration 542, loss = 0.45717047
Iteration 543, loss = 0.47438292
Iteration 544, loss = 0.46309552
Iteration 545, loss = 0.48832058
Iteration 546, loss = 0.46551508
Iteration 547, loss = 0.47159627
Iteration 548, loss = 0.46871778
Iteration 549, loss = 0.45878841
Iteration 550, loss = 0.46573306
Iteration 551, loss = 0.47252894
Iteration 552, loss = 0.46495438
Iteration 553, loss = 0.46739043
Iteration 554, loss = 0.46321572
Iteration 555, loss = 0.48091406
Iteration 556, loss = 0.46978587
Iteration 557, loss = 0.45842079
Iteration 558, loss = 0.45819330
Iteration 559, loss = 0.45927538
Iteration 560, loss = 0.46323189
Iteration 561, loss = 0.46154883
Iteration 562, loss = 0.46185836
Iteration 563, loss = 0.48418553
Iteration 564, loss = 0.49184785
Iteration 565, loss = 0.47897497
Iteration 566, loss = 0.46260112
Iteration 567, loss = 0.47760813
Iteration 568, loss = 0.47294796
Iteration 569, loss = 0.48688057
Iteration 570, loss = 0.46373888
Iteration 571, loss = 0.48058210
Iteration 572, loss = 0.47926785
Iteration 573, loss = 0.47653923
Iteration 574, loss = 0.49756784
Iteration 575, loss = 0.51498230
Iteration 576, loss = 0.48015378
Iteration 577, loss = 0.46672983
Iteration 578, loss = 0.46306902
Iteration 579, loss = 0.47027183
Iteration 580, loss = 0.47377222
Iteration 581, loss = 0.47342553
Iteration 582, loss = 0.46844899
Iteration 583, loss = 0.47217052
Iteration 584, loss = 0.48138818
Iteration 585, loss = 0.49474853
Iteration 586, loss = 0.47491728
Iteration 587, loss = 0.46050932
Iteration 588, loss = 0.47570461
Iteration 589, loss = 0.46854915
Iteration 590, loss = 0.45716176
Iteration 591, loss = 0.46090781
Iteration 592, loss = 0.46259080
Iteration 593, loss = 0.45686133
Iteration 594, loss = 0.46365044
Iteration 595, loss = 0.47143315
Iteration 596, loss = 0.48251586
Iteration 597, loss = 0.47208418
Iteration 598, loss = 0.51573405
Iteration 599, loss = 0.54467836
Iteration 600, loss = 0.50410718
Iteration 601, loss = 0.49311926
Iteration 602, loss = 0.50298798
Iteration 603, loss = 0.51971780
Iteration 604, loss = 0.60516005
Iteration 605, loss = 0.58719000
Iteration 606, loss = 0.54352141
Iteration 607, loss = 0.55375449
Iteration 608, loss = 0.54261994
Iteration 609, loss = 0.51003557
Iteration 610, loss = 0.50622137
Iteration 611, loss = 0.47512505
Iteration 612, loss = 0.45910759
Iteration 613, loss = 0.46698555
Iteration 614, loss = 0.46280391
Iteration 615, loss = 0.46023830
Iteration 616, loss = 0.46047859
Iteration 617, loss = 0.46234632
Iteration 618, loss = 0.46252876
Iteration 619, loss = 0.47622730
Iteration 620, loss = 0.49219540
Iteration 621, loss = 0.48562448
Iteration 622, loss = 0.48245704
Iteration 623, loss = 0.45646199
Iteration 624, loss = 0.46418298
Iteration 625, loss = 0.47998896
Iteration 626, loss = 0.48152969
Iteration 627, loss = 0.48403960
Iteration 628, loss = 0.47508213
Iteration 629, loss = 0.46703453
Iteration 630, loss = 0.46342122
Iteration 631, loss = 0.47897013
Iteration 632, loss = 0.46398844
Iteration 633, loss = 0.45977841
Iteration 634, loss = 0.48331127
Iteration 635, loss = 0.49012258
Iteration 636, loss = 0.46242302
Iteration 637, loss = 0.47582519
Iteration 638, loss = 0.46281581
Iteration 639, loss = 0.46790776
Iteration 640, loss = 0.46673970
Iteration 641, loss = 0.45748131
Iteration 642, loss = 0.46071184
Iteration 643, loss = 0.45543130
Iteration 644, loss = 0.45689210
Iteration 645, loss = 0.46588188
Iteration 646, loss = 0.47708351
Iteration 647, loss = 0.50116989
Iteration 648, loss = 0.48863610
Iteration 649, loss = 0.50480461
Iteration 650, loss = 0.50840671
Iteration 651, loss = 0.53036074
Iteration 652, loss = 0.47973055
Iteration 653, loss = 0.46913946
Iteration 654, loss = 0.46042181
Iteration 655, loss = 0.45533025
Iteration 656, loss = 0.47349820
Iteration 657, loss = 0.45964984
Iteration 658, loss = 0.47185264
Iteration 659, loss = 0.47467936
Iteration 660, loss = 0.47293972
Iteration 661, loss = 0.47599959
Iteration 662, loss = 0.46340021
Iteration 663, loss = 0.45443774
Iteration 664, loss = 0.45869870
Iteration 665, loss = 0.46546996
Iteration 666, loss = 0.48961571
Iteration 667, loss = 0.53312299
Iteration 668, loss = 0.60396399
Iteration 669, loss = 0.48719634
Iteration 670, loss = 0.49522722
Iteration 671, loss = 0.46448914
Iteration 672, loss = 0.45449380
Iteration 673, loss = 0.46973962
Iteration 674, loss = 0.46706093
Iteration 675, loss = 0.47126116
Iteration 676, loss = 0.47776427
Iteration 677, loss = 0.53315583
Iteration 678, loss = 0.51640451
Iteration 679, loss = 0.52740052
Iteration 680, loss = 0.49706353
Iteration 681, loss = 0.50569769
Iteration 682, loss = 0.48670018
Iteration 683, loss = 0.49409843
Iteration 684, loss = 0.49005053
Iteration 685, loss = 0.53051990
Iteration 686, loss = 0.47700346
Iteration 687, loss = 0.47052685
Iteration 688, loss = 0.47939761
Iteration 689, loss = 0.47622443
Iteration 690, loss = 0.48924097
Iteration 691, loss = 0.50436611
Iteration 692, loss = 0.46578396
Iteration 693, loss = 0.48028268
Iteration 694, loss = 0.48583739
Iteration 695, loss = 0.48832637
Iteration 696, loss = 0.46170477
Iteration 697, loss = 0.46808274
Iteration 698, loss = 0.46992144
Iteration 699, loss = 0.47741402
Iteration 700, loss = 0.45239824
Iteration 701, loss = 0.47433604
Iteration 702, loss = 0.47725627
Iteration 703, loss = 0.46862550
Iteration 704, loss = 0.46179268
Iteration 705, loss = 0.46733429
Iteration 706, loss = 0.45506666
Iteration 707, loss = 0.46510421
Iteration 708, loss = 0.46945578
Iteration 709, loss = 0.47743527
Iteration 710, loss = 0.46745141
Iteration 711, loss = 0.47248110
Iteration 712, loss = 0.45989913
Iteration 713, loss = 0.46018793
Iteration 714, loss = 0.48227353
Iteration 715, loss = 0.47567095
Iteration 716, loss = 0.46629356
Iteration 717, loss = 0.46089598
Iteration 718, loss = 0.49833585
Iteration 719, loss = 0.50046719
Iteration 720, loss = 0.48422404
Iteration 721, loss = 0.47940465
Iteration 722, loss = 0.47306946
Iteration 723, loss = 0.47235160
Iteration 724, loss = 0.45741063
Iteration 725, loss = 0.46506198
Iteration 726, loss = 0.51476387
Iteration 727, loss = 0.47401732
Iteration 728, loss = 0.46400802
Iteration 729, loss = 0.48181226
Iteration 730, loss = 0.47566901
Iteration 731, loss = 0.46960801
Iteration 732, loss = 0.47823682
Iteration 733, loss = 0.48142078
Iteration 734, loss = 0.46634418
Iteration 735, loss = 0.45580232
Iteration 736, loss = 0.45986331
Iteration 737, loss = 0.47495726
Iteration 738, loss = 0.46656656
Iteration 739, loss = 0.46209596
Iteration 740, loss = 0.45312149
Iteration 741, loss = 0.45540432
Iteration 742, loss = 0.46927419
Iteration 743, loss = 0.51494454
Iteration 744, loss = 0.47617355
Iteration 745, loss = 0.50398043
Iteration 746, loss = 0.56327414
Iteration 747, loss = 0.49627993
Iteration 748, loss = 0.50167388
Iteration 749, loss = 0.48177937
Iteration 750, loss = 0.47870267
Iteration 751, loss = 0.48233981
Iteration 752, loss = 0.45846141
Iteration 753, loss = 0.50020354
Iteration 754, loss = 0.49300809
Iteration 755, loss = 0.47429060
Iteration 756, loss = 0.47022057
Iteration 757, loss = 0.46318047
Iteration 758, loss = 0.46955455
Iteration 759, loss = 0.48931972
Iteration 760, loss = 0.50753544
Iteration 761, loss = 0.49187671
Iteration 762, loss = 0.49271002
Iteration 763, loss = 0.47677406
Iteration 764, loss = 0.48192361
Iteration 765, loss = 0.46022304
Iteration 766, loss = 0.45438083
Iteration 767, loss = 0.45038460
Iteration 768, loss = 0.46336853
Iteration 769, loss = 0.46624264
Iteration 770, loss = 0.45861587
Iteration 771, loss = 0.45499287
Iteration 772, loss = 0.45986453
Iteration 773, loss = 0.45903605
Iteration 774, loss = 0.47121510
Iteration 775, loss = 0.49606911
Iteration 776, loss = 0.50929672
Iteration 777, loss = 0.52159227
Iteration 778, loss = 0.54426328
Iteration 779, loss = 0.48802746
Iteration 780, loss = 0.46563122
Iteration 781, loss = 0.50846757
Iteration 782, loss = 0.54393037
Iteration 783, loss = 0.52406046
Iteration 784, loss = 0.47989953
Iteration 785, loss = 0.52106754
Iteration 786, loss = 0.46314048
Iteration 787, loss = 0.50298817
Iteration 788, loss = 0.48557273
Iteration 789, loss = 0.46295682
Iteration 790, loss = 0.46647655
Iteration 791, loss = 0.47639661
Iteration 792, loss = 0.46827737
Iteration 793, loss = 0.49669019
Iteration 794, loss = 0.48820770
Iteration 795, loss = 0.49313009
Iteration 796, loss = 0.49301304
Iteration 797, loss = 0.47638773
Iteration 798, loss = 0.44932810
Iteration 799, loss = 0.46122937
Iteration 800, loss = 0.45507664
Iteration 801, loss = 0.45380507
Iteration 802, loss = 0.45395449
Iteration 803, loss = 0.45391803
Iteration 804, loss = 0.45138782
Iteration 805, loss = 0.45003698
Iteration 806, loss = 0.46246995
Iteration 807, loss = 0.46241983
Iteration 808, loss = 0.46676598
Iteration 809, loss = 0.48298687
Iteration 810, loss = 0.45490750
Iteration 811, loss = 0.45484458
Iteration 812, loss = 0.46950959
Iteration 813, loss = 0.48308957
Iteration 814, loss = 0.47080938
Iteration 815, loss = 0.47290739
Iteration 816, loss = 0.46719995
Iteration 817, loss = 0.51004350
Iteration 818, loss = 0.47761261
Iteration 819, loss = 0.48798910
Iteration 820, loss = 0.52345774
Iteration 821, loss = 0.53317766
Iteration 822, loss = 0.60977449
Iteration 823, loss = 0.51919726
Iteration 824, loss = 0.54605912
Iteration 825, loss = 0.49478842
Iteration 826, loss = 0.48528122
Iteration 827, loss = 0.50744604
Iteration 828, loss = 0.50903728
Iteration 829, loss = 0.52598197
Iteration 830, loss = 0.54684586
Iteration 831, loss = 0.48479368
Iteration 832, loss = 0.47544816
Iteration 833, loss = 0.47535952
Iteration 834, loss = 0.45641177
Iteration 835, loss = 0.45149902
Iteration 836, loss = 0.45758619
Iteration 837, loss = 0.45871921
Iteration 838, loss = 0.47512526
Iteration 839, loss = 0.46504300
Iteration 840, loss = 0.46800164
Iteration 841, loss = 0.49891676
Iteration 842, loss = 0.55118160
Iteration 843, loss = 0.56680010
Iteration 844, loss = 0.48906680
Iteration 845, loss = 0.46669853
Iteration 846, loss = 0.45633175
Iteration 847, loss = 0.46406233
Iteration 848, loss = 0.46166797
Iteration 849, loss = 0.45301894
Iteration 850, loss = 0.44768033
Iteration 851, loss = 0.44850630
Iteration 852, loss = 0.45209238
Iteration 853, loss = 0.44915350
Iteration 854, loss = 0.45062373
Iteration 855, loss = 0.44833225
Iteration 856, loss = 0.45246853
Iteration 857, loss = 0.45728397
Iteration 858, loss = 0.45619826
Iteration 859, loss = 0.44662390
Iteration 860, loss = 0.45354408
Iteration 861, loss = 0.45409419
Iteration 862, loss = 0.46466958
Iteration 863, loss = 0.45747267
Iteration 864, loss = 0.45553756
Iteration 865, loss = 0.44902220
Iteration 866, loss = 0.44886172
Iteration 867, loss = 0.44823208
Iteration 868, loss = 0.44934257
Iteration 869, loss = 0.45657941
Iteration 870, loss = 0.46225404
Iteration 871, loss = 0.44556502
Iteration 872, loss = 0.45915339
Iteration 873, loss = 0.44835580
Iteration 874, loss = 0.44758679
Iteration 875, loss = 0.44975995
Iteration 876, loss = 0.44775055
Iteration 877, loss = 0.46647856
Iteration 878, loss = 0.45620547
Iteration 879, loss = 0.46024375
Iteration 880, loss = 0.45802403
Iteration 881, loss = 0.45149887
Iteration 882, loss = 0.46186222
Iteration 883, loss = 0.45538272
Iteration 884, loss = 0.47560187
Iteration 885, loss = 0.52183254
Iteration 886, loss = 0.54099516
Iteration 887, loss = 0.49401285
Iteration 888, loss = 0.45438705
Iteration 889, loss = 0.50868197
Iteration 890, loss = 0.46956586
Iteration 891, loss = 0.47370201
Iteration 892, loss = 0.47199725
Iteration 893, loss = 0.48668289
Iteration 894, loss = 0.49348272
Iteration 895, loss = 0.48698455
Iteration 896, loss = 0.44858669
Iteration 897, loss = 0.46906124
Iteration 898, loss = 0.45828666
Iteration 899, loss = 0.50938326
Iteration 900, loss = 0.50364110
Iteration 901, loss = 0.46551279
Iteration 902, loss = 0.46088088
Iteration 903, loss = 0.45387209
Iteration 904, loss = 0.46079659
Iteration 905, loss = 0.45536973
Iteration 906, loss = 0.46040965
Iteration 907, loss = 0.44388098
Iteration 908, loss = 0.44893223
Iteration 909, loss = 0.44804023
Iteration 910, loss = 0.44777033
Iteration 911, loss = 0.45343656
Iteration 912, loss = 0.45225414
Iteration 913, loss = 0.44777581
Iteration 914, loss = 0.44634891
Iteration 915, loss = 0.46727524
Iteration 916, loss = 0.48459108
Iteration 917, loss = 0.47951542
Iteration 918, loss = 0.47555377
Iteration 919, loss = 0.51453402
Iteration 920, loss = 0.53093702
Iteration 921, loss = 0.55647452
Iteration 922, loss = 0.48671239
Iteration 923, loss = 0.49680690
Iteration 924, loss = 0.48996667
Iteration 925, loss = 0.45254993
Iteration 926, loss = 0.45303139
Iteration 927, loss = 0.44821047
Iteration 928, loss = 0.45240038
Iteration 929, loss = 0.45593587
Iteration 930, loss = 0.45598870
Iteration 931, loss = 0.45228900
Iteration 932, loss = 0.45553288
Iteration 933, loss = 0.44556360
Iteration 934, loss = 0.46294252
Iteration 935, loss = 0.47865750
Iteration 936, loss = 0.47841701
Iteration 937, loss = 0.46709343
Iteration 938, loss = 0.47829535
Iteration 939, loss = 0.45958291
Iteration 940, loss = 0.45961699
Iteration 941, loss = 0.44994730
Iteration 942, loss = 0.44499672
Iteration 943, loss = 0.44338351
Iteration 944, loss = 0.44082425
Iteration 945, loss = 0.43993665
Iteration 946, loss = 0.44195867
Iteration 947, loss = 0.43952335
Iteration 948, loss = 0.43980289
Iteration 949, loss = 0.43919235
Iteration 950, loss = 0.44842980
Iteration 951, loss = 0.46848190
Iteration 952, loss = 0.51837095
Iteration 953, loss = 0.48128690
Iteration 954, loss = 0.46728896
Iteration 955, loss = 0.46245023
Iteration 956, loss = 0.44873558
Iteration 957, loss = 0.46565856
Iteration 958, loss = 0.48011791
Iteration 959, loss = 0.55110642
Iteration 960, loss = 0.48876848
Iteration 961, loss = 0.50633171
Iteration 962, loss = 0.46400372
Iteration 963, loss = 0.47762466
Iteration 964, loss = 0.45524097
Iteration 965, loss = 0.47825188
Iteration 966, loss = 0.46683122
Iteration 967, loss = 0.48068186
Iteration 968, loss = 0.51251294
Iteration 969, loss = 0.47559846
Iteration 970, loss = 0.48221947
Iteration 971, loss = 0.48678982
Iteration 972, loss = 0.47890126
Iteration 973, loss = 0.44884561
Iteration 974, loss = 0.44726115
Iteration 975, loss = 0.46556951
Iteration 976, loss = 0.44724980
Iteration 977, loss = 0.46863721
Iteration 978, loss = 0.45085284
Iteration 979, loss = 0.45882995
Iteration 980, loss = 0.46192433
Iteration 981, loss = 0.49874043
Iteration 982, loss = 0.48376987
Iteration 983, loss = 0.46589074
Iteration 984, loss = 0.44261782
Iteration 985, loss = 0.45071189
Iteration 986, loss = 0.44152248
Iteration 987, loss = 0.45278514
Iteration 988, loss = 0.44541357
Iteration 989, loss = 0.45236678
Iteration 990, loss = 0.44523786
Iteration 991, loss = 0.46662375
Iteration 992, loss = 0.44918890
Iteration 993, loss = 0.46802967
Iteration 994, loss = 0.44632128
Iteration 995, loss = 0.43997721
Iteration 996, loss = 0.44447588
Iteration 997, loss = 0.44017536
Iteration 998, loss = 0.44764945
Iteration 999, loss = 0.43956975
Iteration 1000, loss = 0.45263693
*** fcst ***
[[4.73331636 3.98485706 3.95998117]
 [4.78555379 3.94150683 3.96535356]
 [4.83779122 4.06079734 3.97072595]
 [4.73615446 4.04596794 3.97609833]
 [4.61096838 4.03113855 3.98122162]
 [4.48802144 4.01630915 3.98450604]
 [4.3650745  4.00147975 3.98779046]
 [4.24212756 3.98665036 3.99107487]
 [4.11918062 3.97182096 3.99435929]
 [2.66075336 3.24064273 3.21531834]
 [2.86420502 3.10557245 3.04781624]
 [3.068215   2.95747417 2.94635814]
 [3.27222498 2.87307092 2.86441647]
 [3.47623495 2.85204149 2.7824748 ]
 [3.68024493 2.80795206 2.70053313]
 [3.88425491 3.0647353  2.61859146]
 [4.08826488 3.39113358 3.00751922]
 [4.29227486 4.21449753 4.0036858 ]
 [4.49845377 5.29261606 4.96974988]
 [4.71154148 5.41796196 5.33394418]
 [4.89374722 5.3007929  5.16748577]
 [4.94598465 5.18362385 5.00788693]
 [4.99822208 5.06645479 4.84940656]
 [5.05045951 4.94928573 4.69092619]
 [5.10269694 4.83211667 4.53244582]
 [5.09708855 4.60347337 4.37396545]
 [4.97041557 4.44386379 4.21548508]
 [4.8437426  4.34283258 4.05641716]
 [4.71706963 4.24180137 4.02432416]
 [4.59272228 4.17140433 4.02760858]
 [4.46977534 4.15657494 4.030893  ]
 [4.3468284  4.14174554 4.03417742]
 [4.22388146 4.12691614 4.03746183]
 [2.99201672 4.29750995 3.99963747]
 [3.1960267  4.29194351 3.92207806]
 [3.40003668 4.30654927 3.82263784]
 [3.60404665 4.17938234 3.71832877]
 [3.80805663 3.97180712 3.6140197 ]
 [4.01206661 3.79622524 3.50971063]
 [4.21607658 3.96264771 3.40540157]
 [4.42008656 4.28827068 3.58243578]
 [4.6325973  4.86839702 4.17117077]
 [4.84568501 5.80283771 5.13540153]
 [5.05877271 6.34214733 5.97016706]
 [5.2631278  6.22749181 5.91575579]
 [5.31536523 6.1128363  5.81713679]
 [5.36760266 5.99818079 5.64480884]
 [5.4198401  5.88064099 5.47865087]
 [5.33134966 5.49989206 5.3124929 ]
 [5.20467669 5.20513884 5.14633492]
 [5.07800371 4.95000413 4.97915702]
 [4.95133074 4.84897292 4.81104958]
 [4.82465777 4.74794172 4.64294213]
 [4.6979848  4.64691051 4.47483468]
 [4.57447618 4.5458793  4.30710623]
 [4.45152924 4.4448481  4.14724873]
 [4.3285823  4.34381689 4.07343525]
 [1.53358765 2.01348033 1.97608806]
 [1.66058864 1.81717845 1.84524706]
 [1.78635778 1.60325085 1.71440606]
 [1.90889301 1.50137266 1.58356506]
 [2.03142824 1.52805156 1.45272406]
 [2.15396347 1.62049532 1.32188306]
 [2.2764987  2.18538283 1.82694723]
 [2.44567633 3.40795594 2.80036835]
 [2.64799324 3.7250482  3.31435337]
 [2.85200321 3.58351501 3.33023878]
 [3.05601319 3.44198182 3.34612419]
 [3.15136404 3.30044863 3.3620096 ]
 [3.19452375 3.16337586 3.37789501]
 [3.23768345 3.03183882 3.39378042]
 [3.28209186 2.90030178 3.40966583]
 [3.32772673 2.99685973 3.42555124]
 [3.37336161 3.12413301 3.44143665]
 [3.41899648 3.26231494 3.45732206]
 [3.46813779 3.40049686 3.47320747]
 [3.52351973 3.53884935 3.48909288]
 [3.57890168 3.67721669 3.50497829]
 [3.63428362 3.81558402 3.5208637 ]
 [3.68966556 3.95395136 3.53674911]
 [3.74504751 4.05962264 3.55263452]
 [1.75761596 2.22723721 2.1769835 ]
 [1.88100155 2.00020208 2.02677586]
 [2.00353678 1.78102083 1.89492758]
 [2.12607201 1.56531923 1.76408658]
 [2.24860724 1.59749127 1.63324558]
 [2.37247056 1.72802491 1.62368862]
 [2.57221987 2.04682927 2.01755324]
 [2.77580493 3.27153722 2.99097435]
 [2.97981491 4.13747951 3.67055868]
 [3.18382489 3.99776028 3.68644409]
 [3.38783486 3.85754204 3.7023295 ]
 [3.50533507 3.71600885 3.71821491]
 [3.54849477 3.57447567 3.73410032]
 [3.59165448 3.43294248 3.74998573]
 [3.63481418 3.29140929 3.76587114]
 [3.67797388 3.26358264 3.78175655]
 [3.72301627 3.40176457 3.79764196]
 [3.77839821 3.53994649 3.81352737]
 [3.83378016 3.67812842 3.82941278]
 [3.8891621  3.81631034 3.84529819]]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   4.226052 2021-12-17 15:00:00          200
1    MALMEDY_CM_FT003.F.PV   4.230805 2021-12-17 16:00:00          200
2    MALMEDY_CM_FT003.F.PV   4.289772 2021-12-17 17:00:00          200
3    MALMEDY_CM_FT003.F.PV    4.25274 2021-12-17 18:00:00          200
4    MALMEDY_CM_FT003.F.PV   4.207776 2021-12-17 19:00:00          200
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.574438 2021-12-21 15:00:00          200
97   MALMEDY_CM_FT003.F.PV   3.640808 2021-12-21 16:00:00          200
98   MALMEDY_CM_FT003.F.PV   3.710624 2021-12-21 17:00:00          200
99   MALMEDY_CM_FT003.F.PV    3.78044 2021-12-21 18:00:00          200
100  MALMEDY_CM_FT003.F.PV   3.850257 2021-12-21 19:00:00          200

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT003.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   4.226052 2021-12-17 15:00:00          200
1    MALMEDY_CM_FT003.F.PV   4.230805 2021-12-17 16:00:00          200
2    MALMEDY_CM_FT003.F.PV   4.289772 2021-12-17 17:00:00          200
3    MALMEDY_CM_FT003.F.PV    4.25274 2021-12-17 18:00:00          200
4    MALMEDY_CM_FT003.F.PV   4.207776 2021-12-17 19:00:00          200
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.574438 2021-12-21 15:00:00          200
97   MALMEDY_CM_FT003.F.PV   3.640808 2021-12-21 16:00:00          200
98   MALMEDY_CM_FT003.F.PV   3.710624 2021-12-21 17:00:00          200
99   MALMEDY_CM_FT003.F.PV    3.78044 2021-12-21 18:00:00          200
100  MALMEDY_CM_FT003.F.PV   3.850257 2021-12-21 19:00:00          200

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   4.733316 2021-12-17 15:00:00          201
1    MALMEDY_CM_FT003.F.PV   4.785554 2021-12-17 16:00:00          201
2    MALMEDY_CM_FT003.F.PV   4.837791 2021-12-17 17:00:00          201
3    MALMEDY_CM_FT003.F.PV   4.736154 2021-12-17 18:00:00          201
4    MALMEDY_CM_FT003.F.PV   4.610968 2021-12-17 19:00:00          201
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.677974 2021-12-21 15:00:00          201
97   MALMEDY_CM_FT003.F.PV   3.723016 2021-12-21 16:00:00          201
98   MALMEDY_CM_FT003.F.PV   3.778398 2021-12-21 17:00:00          201
99   MALMEDY_CM_FT003.F.PV    3.83378 2021-12-21 18:00:00          201
100  MALMEDY_CM_FT003.F.PV   3.889162 2021-12-21 19:00:00          201

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT003.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   4.733316 2021-12-17 15:00:00          201
1    MALMEDY_CM_FT003.F.PV   4.785554 2021-12-17 16:00:00          201
2    MALMEDY_CM_FT003.F.PV   4.837791 2021-12-17 17:00:00          201
3    MALMEDY_CM_FT003.F.PV   4.736154 2021-12-17 18:00:00          201
4    MALMEDY_CM_FT003.F.PV   4.610968 2021-12-17 19:00:00          201
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.677974 2021-12-21 15:00:00          201
97   MALMEDY_CM_FT003.F.PV   3.723016 2021-12-21 16:00:00          201
98   MALMEDY_CM_FT003.F.PV   3.778398 2021-12-21 17:00:00          201
99   MALMEDY_CM_FT003.F.PV    3.83378 2021-12-21 18:00:00          201
100  MALMEDY_CM_FT003.F.PV   3.889162 2021-12-21 19:00:00          201

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   3.984857 2021-12-17 15:00:00          202
1    MALMEDY_CM_FT003.F.PV   3.941507 2021-12-17 16:00:00          202
2    MALMEDY_CM_FT003.F.PV   4.060797 2021-12-17 17:00:00          202
3    MALMEDY_CM_FT003.F.PV   4.045968 2021-12-17 18:00:00          202
4    MALMEDY_CM_FT003.F.PV   4.031139 2021-12-17 19:00:00          202
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.263583 2021-12-21 15:00:00          202
97   MALMEDY_CM_FT003.F.PV   3.401765 2021-12-21 16:00:00          202
98   MALMEDY_CM_FT003.F.PV   3.539946 2021-12-21 17:00:00          202
99   MALMEDY_CM_FT003.F.PV   3.678128 2021-12-21 18:00:00          202
100  MALMEDY_CM_FT003.F.PV    3.81631 2021-12-21 19:00:00          202

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT003.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   3.984857 2021-12-17 15:00:00          202
1    MALMEDY_CM_FT003.F.PV   3.941507 2021-12-17 16:00:00          202
2    MALMEDY_CM_FT003.F.PV   4.060797 2021-12-17 17:00:00          202
3    MALMEDY_CM_FT003.F.PV   4.045968 2021-12-17 18:00:00          202
4    MALMEDY_CM_FT003.F.PV   4.031139 2021-12-17 19:00:00          202
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.263583 2021-12-21 15:00:00          202
97   MALMEDY_CM_FT003.F.PV   3.401765 2021-12-21 16:00:00          202
98   MALMEDY_CM_FT003.F.PV   3.539946 2021-12-21 17:00:00          202
99   MALMEDY_CM_FT003.F.PV   3.678128 2021-12-21 18:00:00          202
100  MALMEDY_CM_FT003.F.PV    3.81631 2021-12-21 19:00:00          202

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   3.959981 2021-12-17 15:00:00          203
1    MALMEDY_CM_FT003.F.PV   3.965354 2021-12-17 16:00:00          203
2    MALMEDY_CM_FT003.F.PV   3.970726 2021-12-17 17:00:00          203
3    MALMEDY_CM_FT003.F.PV   3.976098 2021-12-17 18:00:00          203
4    MALMEDY_CM_FT003.F.PV   3.981222 2021-12-17 19:00:00          203
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.781757 2021-12-21 15:00:00          203
97   MALMEDY_CM_FT003.F.PV   3.797642 2021-12-21 16:00:00          203
98   MALMEDY_CM_FT003.F.PV   3.813527 2021-12-21 17:00:00          203
99   MALMEDY_CM_FT003.F.PV   3.829413 2021-12-21 18:00:00          203
100  MALMEDY_CM_FT003.F.PV   3.845298 2021-12-21 19:00:00          203

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT003.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT003.F.PV   3.959981 2021-12-17 15:00:00          203
1    MALMEDY_CM_FT003.F.PV   3.965354 2021-12-17 16:00:00          203
2    MALMEDY_CM_FT003.F.PV   3.970726 2021-12-17 17:00:00          203
3    MALMEDY_CM_FT003.F.PV   3.976098 2021-12-17 18:00:00          203
4    MALMEDY_CM_FT003.F.PV   3.981222 2021-12-17 19:00:00          203
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT003.F.PV   3.781757 2021-12-21 15:00:00          203
97   MALMEDY_CM_FT003.F.PV   3.797642 2021-12-21 16:00:00          203
98   MALMEDY_CM_FT003.F.PV   3.813527 2021-12-21 17:00:00          203
99   MALMEDY_CM_FT003.F.PV   3.829413 2021-12-21 18:00:00          203
100  MALMEDY_CM_FT003.F.PV   3.845298 2021-12-21 19:00:00          203

[101 rows x 4 columns]


8 cpt.inputdata MALMEDY_CM_FT007.F.PV cpt.ao 60.0 1440.0 30240.0 MLPDYNAMIC
DELETE FROM cpt.ao WHERE scadaid='MALMEDY_CM_FT007.F.PV'... done
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT007.F.PV' ORDER BY scadadatetime DESC... done
*** ldate, fdate ***
2021-12-17 14:15:00 2021-11-18 19:30:00
>>> Water demand prediction : TOF 2021-12-20 18:00:00
*** start_train, stop_train, start_pred, stop_pred, toftime ***
2021-11-26 15:00:00 2021-12-17 15:00:00 2021-12-17 15:00:00 2021-12-21 19:00:00 2021-12-20 18:00:00
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT007.F.PV' ORDER BY scadadatetime ASC, rnum ASC... done
*** df_in ***
        rnum       scadadatetime                scadaid  scadavalue scadaquality
0     333875 2021-11-18 19:30:00  MALMEDY_CM_FT007.F.PV       0.907         None
1     333908 2021-11-18 20:00:00  MALMEDY_CM_FT007.F.PV       0.907         None
2     333918 2021-11-18 20:00:00  MALMEDY_CM_FT007.F.PV       0.907         None
3     333948 2021-11-18 20:15:00  MALMEDY_CM_FT007.F.PV       0.907         None
4     333958 2021-11-18 20:15:00  MALMEDY_CM_FT007.F.PV       0.907         None
...      ...                 ...                    ...         ...          ...
6358  498780 2021-12-17 14:00:00  MALMEDY_CM_FT007.F.PV       1.107         None
6359  498807 2021-12-17 14:00:00  MALMEDY_CM_FT007.F.PV       1.107         None
6360  498820 2021-12-17 14:00:00  MALMEDY_CM_FT007.F.PV       1.107         None
6361  498844 2021-12-17 14:15:00  MALMEDY_CM_FT007.F.PV       1.107         None
6362  498861 2021-12-17 14:15:00  MALMEDY_CM_FT007.F.PV       1.107         None

[6363 rows x 5 columns]
*** X_train, y_train, df_X, df_y, X_pred ***
[[330. 330. 330. ... 351. 351. 351.]
 [  5.   5.   5. ...   5.   5.   5.]
 [ 15.  16.  17. ...  12.  13.  14.]] [1.193      1.193      1.07       1.02366667 1.407      1.407
 1.25       1.093      1.013      1.013      0.853      0.773
 0.733      0.69416667 0.6        0.6        0.72       0.89333333
 1.46       1.46       1.5035     1.477      1.127      1.127
 1.35       1.573      1.566      1.559      1.552      1.545
 1.538      1.531      1.524      1.517      1.51       1.503
 1.496      1.489      1.482      1.475      1.468      1.461
 1.454      1.447      1.51366667 1.438      0.893      0.893
 1.273      1.62633333 1.493      1.493      1.21       0.9725
 1.2        1.2        1.1135     1.027      1.027      0.78
 0.678      0.627      0.627      0.62       0.66866667 0.693
 0.693      1.58       1.38       1.19333333 1.26       1.26
 1.1165     0.973      0.74       0.74       1.38       2.02
 1.107      1.107      1.1535     1.2        0.667      0.667
 0.667      0.667      0.667      0.62       0.642      0.653
 0.653      1.253      1.063      0.873      1.207      1.207
 1.207      0.873      0.8        0.727      0.727      0.727
 0.748      0.853      0.689      0.607      0.607      0.72
 0.63533333 0.603      0.653      0.653      0.65083333 0.64
 1.267      1.267      1.267      0.8        1.0865     1.373
 1.027      1.027      1.027      1.073      1.12       1.167
 0.94       0.94       0.7965     0.653      0.58       0.58
 0.59333333 0.6        0.6        0.553      0.553      0.553
 0.553      1.06       1.3        1.36116667 1.067      1.067
 1.17366667 1.16466667 0.853      0.853      1.043      1.17633333
 0.893      0.893      0.8465     0.8        0.8        0.853
 0.68433333 0.6        0.6        0.607      0.687      0.727
 0.727      1.047      1.1935     1.27216667 0.933      0.933
 1.1165     1.30333333 1.32       1.32       1.22       1.12
 1.467      1.467      1.1535     0.85783333 0.947      0.947
 0.947      0.947      0.888      0.593      0.72633333 0.793
 0.89416667 1.4        1.42666667 1.44       1.44216667 1.453
 1.809      1.987      2.04033333 2.307      1.653      1.653
 1.623      1.473      0.98       0.98       0.87       0.76
 0.66666667 0.62       0.61883333 0.613      0.59966667 0.593
 0.84533333 2.107      2.887      2.887      2.56033333 0.927
 1.28       1.28       1.2255     0.953      1.147      1.147
 1.11366667 0.947      0.9        0.9        0.77133333 0.707
 0.65766667 0.633      0.633      0.633      0.77566667 0.847
 0.92133333 1.293      1.24433333 1.22       1.22       1.027
 1.17766667 1.253      1.253      0.853      1.12433333 1.26
 1.26       1.04       0.92       0.86       0.85216667 0.813
 0.68433333 0.62       0.62       0.607      0.767      0.847
 0.847      1.007      1.29566667 1.44       1.44       1.44
 1.3545     0.927      0.707      0.707      0.707      1.14
 0.97       0.76666667 0.6        0.6        0.6365     0.673
 0.61966667 0.593      0.593      0.61566667 0.647      0.667
 1.073      1.073      1.073      0.733      0.7165     0.7
 0.833      0.833      0.853      0.953      1.109      1.1425
 0.92       0.92       0.81       0.7        0.693      0.693
 0.649      0.62916667 0.64       0.64       0.7035     0.767
 1.447      1.447      1.447      1.447      1.447      1.447
 1.09566667 0.92       0.92       1.093      1.09629514 1.09959028
 1.10288542 1.10618056 1.10947569 1.11277083 1.11606597 1.11936111
 1.12265625 1.12595139 1.12924653 1.13254167 1.13583681 1.13913194
 1.14242708 1.14572222 1.14901736 1.1523125  1.15560764 1.15890278
 1.16219792 1.16549306 1.16878819 1.17208333 1.17537847 1.17867361
 1.18196875 1.18526389 1.18855903 1.19185417 1.19514931 1.19844444
 1.20173958 1.20503472 1.20832986 1.211625   1.21492014 1.21821528
 1.22151042 1.22480556 1.22810069 1.23139583 1.23469097 1.23798611
 1.24128125 1.24457639 1.24787153 1.25116667 1.25446181 1.25775694
 1.26105208 1.26434722 1.26764236 1.2709375  1.27423264 1.27752778
 1.28082292 1.28411806 1.28741319 1.29070833 1.29400347 1.29729861
 1.30059375 1.30388889 1.30718403 1.31047917 1.31377431 1.31706944
 1.32036458 1.32365972 1.32695486 1.33025    1.22       1.22
 1.2735     1.22583333 0.72       0.72       0.72       0.72
 0.71533333 0.713      0.69633333 0.613      0.62116667 0.62933333
 0.6375     0.64566667 0.65383333 0.662      0.67016667 0.67833333
 0.6865     0.69466667 0.70283333 0.711      0.71916667 0.72733333
 0.7355     0.74366667 0.75183333 0.76       0.76816667 0.77633333
 0.7845     0.79266667 0.80083333 0.809      0.81716667 0.82533333
 0.8335     0.84166667 0.84983333 0.858      0.86616667 0.87433333
 0.8825     0.89066667 0.89883333 0.907      1.057      1.207
 0.987      0.987      0.897      0.78366667 0.667      0.667
 0.65366667 0.647      0.64033333 0.607      0.607      0.607
 0.65466667 0.893      1.207      1.17211111 1.13722222 1.10233333
 1.06744444 1.03255556 0.99766667 0.96277778 0.92788889 0.893
 0.893      0.89105882 0.88911765 0.88717647 0.88523529 0.88329412
 0.88135294 0.87941176 0.87747059 0.87552941 0.87358824 0.87164706
 0.86970588 0.86776471 0.86582353 0.86388235 0.86194118 0.86
 0.8465     0.833      0.833      0.833      0.87416667 1.08
 0.92866667 0.853      0.862      0.907      0.687      0.687
 0.629      0.6        0.6        0.59133333 0.6835     0.78
 1.127      1.127      1.127      1.147      1.127      1.107     ]                        DOY   WD    HR
2021-11-26 15:00:00  330.0  5.0  15.0
2021-11-26 16:00:00  330.0  5.0  16.0
2021-11-26 17:00:00  330.0  5.0  17.0
2021-11-26 18:00:00  330.0  5.0  18.0
2021-11-26 19:00:00  330.0  5.0  19.0
...                    ...  ...   ...
2021-12-21 15:00:00  355.0  2.0  15.0
2021-12-21 16:00:00  355.0  2.0  16.0
2021-12-21 17:00:00  355.0  2.0  17.0
2021-12-21 18:00:00  355.0  2.0  18.0
2021-12-21 19:00:00  355.0  2.0  19.0

[605 rows x 3 columns] 2021-11-26 15:00:00    1.193000
2021-11-26 16:00:00    1.193000
2021-11-26 17:00:00    1.070000
2021-11-26 18:00:00    1.023667
2021-11-26 19:00:00    1.407000
                         ...   
2021-12-21 15:00:00    0.000000
2021-12-21 16:00:00    0.000000
2021-12-21 17:00:00    0.000000
2021-12-21 18:00:00    0.000000
2021-12-21 19:00:00    0.000000
Freq: 60T, Name: scadavalue, Length: 605, dtype: float64 [[351. 351. 351. 351. 351. 351. 351. 351. 351. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354.
  354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 355. 355. 355.
  355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355.
  355. 355. 355.]
 [  5.   5.   5.   5.   5.   5.   5.   5.   5.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.]
 [ 15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.   3.   4.
    5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.
   19.  20.  21.  22.  23.   0.   1.   2.   3.   4.   5.   6.   7.   8.
    9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.
   23.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
   13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.
    3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.
   17.  18.  19.]]
Run 0
Iteration 1, loss = 909.99983190
Iteration 2, loss = 61.37402826
Iteration 3, loss = 16.89614137
Iteration 4, loss = 7.11107454
Iteration 5, loss = 4.75437117
Iteration 6, loss = 4.40887296
Iteration 7, loss = 2.87245450
Iteration 8, loss = 1.14767674
Iteration 9, loss = 0.68261288
Iteration 10, loss = 0.31667446
Iteration 11, loss = 0.23444697
Iteration 12, loss = 0.20812777
Iteration 13, loss = 0.11613220
Iteration 14, loss = 0.19500869
Iteration 15, loss = 0.66148170
Iteration 16, loss = 0.12799215
Iteration 17, loss = 0.07133860
Iteration 18, loss = 0.07231198
Iteration 19, loss = 0.07045944
Iteration 20, loss = 0.07038376
Iteration 21, loss = 0.08540849
Iteration 22, loss = 0.08919818
Iteration 23, loss = 0.06805935
Iteration 24, loss = 0.06794671
Iteration 25, loss = 0.07099449
Iteration 26, loss = 0.06500092
Iteration 27, loss = 0.06512331
Iteration 28, loss = 0.05994421
Iteration 29, loss = 0.05858636
Iteration 30, loss = 0.05877232
Iteration 31, loss = 0.05388448
Iteration 32, loss = 0.05457581
Iteration 33, loss = 0.05057917
Iteration 34, loss = 0.05132758
Iteration 35, loss = 0.04699052
Iteration 36, loss = 0.05299084
Iteration 37, loss = 0.04670235
Iteration 38, loss = 0.04649596
Iteration 39, loss = 0.04513650
Iteration 40, loss = 0.04304183
Iteration 41, loss = 0.04595847
Iteration 42, loss = 0.04826288
Iteration 43, loss = 0.04284631
Iteration 44, loss = 0.04307718
Iteration 45, loss = 0.04066252
Iteration 46, loss = 0.04262538
Iteration 47, loss = 0.04357892
Iteration 48, loss = 0.04289883
Iteration 49, loss = 0.04233447
Iteration 50, loss = 0.03935713
Iteration 51, loss = 0.03906673
Iteration 52, loss = 0.03943002
Iteration 53, loss = 0.05009010
Iteration 54, loss = 0.04593203
Iteration 55, loss = 0.04576615
Iteration 56, loss = 0.04037117
Iteration 57, loss = 0.04374606
Iteration 58, loss = 0.04332221
Iteration 59, loss = 0.04242296
Iteration 60, loss = 0.04164491
Iteration 61, loss = 0.03957482
Iteration 62, loss = 0.03872760
Iteration 63, loss = 0.03938715
Iteration 64, loss = 0.03823979
Iteration 65, loss = 0.03996104
Iteration 66, loss = 0.03900611
Iteration 67, loss = 0.03828144
Iteration 68, loss = 0.03876949
Iteration 69, loss = 0.04087669
Iteration 70, loss = 0.03997235
Iteration 71, loss = 0.03958218
Iteration 72, loss = 0.03920144
Iteration 73, loss = 0.04197568
Iteration 74, loss = 0.04491367
Iteration 75, loss = 0.05031779
Iteration 76, loss = 0.04326872
Iteration 77, loss = 0.04222573
Iteration 78, loss = 0.04202210
Iteration 79, loss = 0.03930056
Iteration 80, loss = 0.04278112
Iteration 81, loss = 0.03977280
Iteration 82, loss = 0.04316918
Iteration 83, loss = 0.04239586
Iteration 84, loss = 0.04501079
Iteration 85, loss = 0.04296833
Iteration 86, loss = 0.04124413
Iteration 87, loss = 0.04285165
Iteration 88, loss = 0.03936519
Iteration 89, loss = 0.03853157
Iteration 90, loss = 0.03853385
Iteration 91, loss = 0.03896146
Iteration 92, loss = 0.03937472
Iteration 93, loss = 0.03959900
Iteration 94, loss = 0.03891725
Iteration 95, loss = 0.03836577
Iteration 96, loss = 0.03825704
Iteration 97, loss = 0.03921443
Iteration 98, loss = 0.03970142
Iteration 99, loss = 0.03894147
Iteration 100, loss = 0.04027878
Iteration 101, loss = 0.04360318
Iteration 102, loss = 0.03937288
Iteration 103, loss = 0.03777306
Iteration 104, loss = 0.03967700
Iteration 105, loss = 0.03878746
Iteration 106, loss = 0.03839824
Iteration 107, loss = 0.03988900
Iteration 108, loss = 0.04064324
Iteration 109, loss = 0.04111086
Iteration 110, loss = 0.03978476
Iteration 111, loss = 0.03859718
Iteration 112, loss = 0.03883770
Iteration 113, loss = 0.03938980
Iteration 114, loss = 0.04331724
Iteration 115, loss = 0.04412036
Iteration 116, loss = 0.04173724
Iteration 117, loss = 0.04142414
Iteration 118, loss = 0.03996028
Iteration 119, loss = 0.04425908
Iteration 120, loss = 0.04826348
Iteration 121, loss = 0.04493462
Iteration 122, loss = 0.04569909
Iteration 123, loss = 0.04268837
Iteration 124, loss = 0.04564546
Iteration 125, loss = 0.04092403
Iteration 126, loss = 0.04930809
Iteration 127, loss = 0.04824112
Iteration 128, loss = 0.04585317
Iteration 129, loss = 0.03856103
Iteration 130, loss = 0.03935094
Iteration 131, loss = 0.03883319
Iteration 132, loss = 0.03885418
Iteration 133, loss = 0.03856356
Iteration 134, loss = 0.03966189
Iteration 135, loss = 0.03791190
Iteration 136, loss = 0.04028798
Iteration 137, loss = 0.03939692
Iteration 138, loss = 0.04046058
Iteration 139, loss = 0.04404273
Iteration 140, loss = 0.04644709
Iteration 141, loss = 0.04196406
Iteration 142, loss = 0.04718546
Iteration 143, loss = 0.04181922
Iteration 144, loss = 0.03876661
Iteration 145, loss = 0.04097051
Iteration 146, loss = 0.04044112
Iteration 147, loss = 0.03805909
Iteration 148, loss = 0.03965530
Iteration 149, loss = 0.03983041
Iteration 150, loss = 0.03822496
Iteration 151, loss = 0.04205745
Iteration 152, loss = 0.04791336
Iteration 153, loss = 0.05005108
Iteration 154, loss = 0.03940158
Iteration 155, loss = 0.03864040
Iteration 156, loss = 0.03977927
Iteration 157, loss = 0.03900822
Iteration 158, loss = 0.03910758
Iteration 159, loss = 0.03841841
Iteration 160, loss = 0.04227083
Iteration 161, loss = 0.04603939
Iteration 162, loss = 0.04195733
Iteration 163, loss = 0.03863590
Iteration 164, loss = 0.03870790
Iteration 165, loss = 0.03945711
Iteration 166, loss = 0.03819554
Iteration 167, loss = 0.03953995
Iteration 168, loss = 0.03879879
Iteration 169, loss = 0.03792478
Iteration 170, loss = 0.03978380
Iteration 171, loss = 0.03939190
Iteration 172, loss = 0.03778652
Iteration 173, loss = 0.03940193
Iteration 174, loss = 0.04610539
Iteration 175, loss = 0.04756290
Iteration 176, loss = 0.04523338
Iteration 177, loss = 0.04395603
Iteration 178, loss = 0.04246425
Iteration 179, loss = 0.04028312
Iteration 180, loss = 0.04995515
Iteration 181, loss = 0.04432737
Iteration 182, loss = 0.04101720
Iteration 183, loss = 0.04382803
Iteration 184, loss = 0.05162610
Iteration 185, loss = 0.04846188
Iteration 186, loss = 0.04940154
Iteration 187, loss = 0.04588848
Iteration 188, loss = 0.04407495
Iteration 189, loss = 0.03931368
Iteration 190, loss = 0.04403159
Iteration 191, loss = 0.04346649
Iteration 192, loss = 0.05689112
Iteration 193, loss = 0.04753798
Iteration 194, loss = 0.04272342
Iteration 195, loss = 0.04101927
Iteration 196, loss = 0.03961719
Iteration 197, loss = 0.03860698
Iteration 198, loss = 0.03853128
Iteration 199, loss = 0.04244638
Iteration 200, loss = 0.03858989
Iteration 201, loss = 0.03808103
Iteration 202, loss = 0.03903995
Iteration 203, loss = 0.03966992
Iteration 204, loss = 0.03846966
Iteration 205, loss = 0.03870380
Iteration 206, loss = 0.03919420
Iteration 207, loss = 0.04091667
Iteration 208, loss = 0.04183232
Iteration 209, loss = 0.04349757
Iteration 210, loss = 0.04567610
Iteration 211, loss = 0.04052070
Iteration 212, loss = 0.03912300
Iteration 213, loss = 0.03783313
Iteration 214, loss = 0.03843697
Iteration 215, loss = 0.03960513
Iteration 216, loss = 0.03906226
Iteration 217, loss = 0.04138847
Iteration 218, loss = 0.03913843
Iteration 219, loss = 0.03842841
Iteration 220, loss = 0.03982693
Iteration 221, loss = 0.04705606
Iteration 222, loss = 0.04783310
Iteration 223, loss = 0.04430570
Iteration 224, loss = 0.04764782
Iteration 225, loss = 0.07641774
Iteration 226, loss = 0.04725963
Iteration 227, loss = 0.06810650
Iteration 228, loss = 0.06261314
Iteration 229, loss = 0.06361174
Iteration 230, loss = 0.06830748
Iteration 231, loss = 0.07011101
Iteration 232, loss = 0.05384416
Iteration 233, loss = 0.04055077
Iteration 234, loss = 0.04045381
Iteration 235, loss = 0.03846351
Iteration 236, loss = 0.03915065
Iteration 237, loss = 0.03972044
Iteration 238, loss = 0.03939822
Iteration 239, loss = 0.04820337
Iteration 240, loss = 0.04837717
Iteration 241, loss = 0.04050159
Iteration 242, loss = 0.03890589
Iteration 243, loss = 0.04037580
Iteration 244, loss = 0.03906344
Iteration 245, loss = 0.04314382
Iteration 246, loss = 0.05149057
Iteration 247, loss = 0.04531127
Iteration 248, loss = 0.04387218
Iteration 249, loss = 0.04389362
Iteration 250, loss = 0.04972463
Iteration 251, loss = 0.04150968
Iteration 252, loss = 0.04306224
Iteration 253, loss = 0.04064892
Iteration 254, loss = 0.03845897
Iteration 255, loss = 0.04011258
Iteration 256, loss = 0.04912147
Iteration 257, loss = 0.04534750
Iteration 258, loss = 0.04485957
Iteration 259, loss = 0.04814120
Iteration 260, loss = 0.05397730
Iteration 261, loss = 0.04693631
Iteration 262, loss = 0.03983137
Iteration 263, loss = 0.03949899
Iteration 264, loss = 0.04600177
Iteration 265, loss = 0.04854619
Iteration 266, loss = 0.05059779
Iteration 267, loss = 0.05130747
Iteration 268, loss = 0.04619629
Iteration 269, loss = 0.03770923
Iteration 270, loss = 0.04865295
Iteration 271, loss = 0.05291539
Iteration 272, loss = 0.04920024
Iteration 273, loss = 0.04486047
Iteration 274, loss = 0.04800498
Iteration 275, loss = 0.04241089
Iteration 276, loss = 0.04037456
Iteration 277, loss = 0.04420689
Iteration 278, loss = 0.03823678
Iteration 279, loss = 0.04087417
Iteration 280, loss = 0.03901929
Iteration 281, loss = 0.03717267
Iteration 282, loss = 0.03726286
Iteration 283, loss = 0.03769688
Iteration 284, loss = 0.04264673
Iteration 285, loss = 0.04730951
Iteration 286, loss = 0.05615110
Iteration 287, loss = 0.06014292
Iteration 288, loss = 0.04829352
Iteration 289, loss = 0.05936332
Iteration 290, loss = 0.04871769
Iteration 291, loss = 0.05802991
Iteration 292, loss = 0.04917786
Iteration 293, loss = 0.07114403
Iteration 294, loss = 0.05324598
Iteration 295, loss = 0.05594239
Iteration 296, loss = 0.04287383
Iteration 297, loss = 0.04142803
Iteration 298, loss = 0.03756369
Iteration 299, loss = 0.03966190
Iteration 300, loss = 0.04005679
Iteration 301, loss = 0.04370641
Iteration 302, loss = 0.04560101
Iteration 303, loss = 0.04291559
Iteration 304, loss = 0.03866663
Iteration 305, loss = 0.03827235
Iteration 306, loss = 0.03740382
Iteration 307, loss = 0.04057986
Iteration 308, loss = 0.04944555
Iteration 309, loss = 0.04657479
Iteration 310, loss = 0.05405809
Iteration 311, loss = 0.05702078
Iteration 312, loss = 0.05142546
Iteration 313, loss = 0.05680982
Iteration 314, loss = 0.05525375
Iteration 315, loss = 0.05804473
Iteration 316, loss = 0.05062267
Iteration 317, loss = 0.06234735
Iteration 318, loss = 0.05208708
Iteration 319, loss = 0.06088983
Iteration 320, loss = 0.04874684
Iteration 321, loss = 0.05340839
Iteration 322, loss = 0.04991643
Iteration 323, loss = 0.04229784
Iteration 324, loss = 0.04310171
Iteration 325, loss = 0.04126523
Iteration 326, loss = 0.04628672
Iteration 327, loss = 0.05401722
Iteration 328, loss = 0.04103323
Iteration 329, loss = 0.04165660
Iteration 330, loss = 0.03869941
Iteration 331, loss = 0.05594604
Iteration 332, loss = 0.03957236
Iteration 333, loss = 0.04581860
Iteration 334, loss = 0.03827664
Iteration 335, loss = 0.04603518
Iteration 336, loss = 0.04853996
Iteration 337, loss = 0.04326290
Iteration 338, loss = 0.03813279
Iteration 339, loss = 0.03850880
Iteration 340, loss = 0.03827301
Iteration 341, loss = 0.03882639
Iteration 342, loss = 0.04061351
Iteration 343, loss = 0.03988709
Iteration 344, loss = 0.03703212
Iteration 345, loss = 0.03712414
Iteration 346, loss = 0.04203485
Iteration 347, loss = 0.04088612
Iteration 348, loss = 0.03691410
Iteration 349, loss = 0.03815384
Iteration 350, loss = 0.03725806
Iteration 351, loss = 0.03795355
Iteration 352, loss = 0.04183408
Iteration 353, loss = 0.04231119
Iteration 354, loss = 0.04339250
Iteration 355, loss = 0.04803992
Iteration 356, loss = 0.04921551
Iteration 357, loss = 0.04579804
Iteration 358, loss = 0.05582515
Iteration 359, loss = 0.07530712
Iteration 360, loss = 0.06431597
Iteration 361, loss = 0.05206152
Iteration 362, loss = 0.06129059
Iteration 363, loss = 0.07732993
Iteration 364, loss = 0.07946406
Iteration 365, loss = 0.05808884
Iteration 366, loss = 0.05067677
Iteration 367, loss = 0.05335421
Iteration 368, loss = 0.06123027
Iteration 369, loss = 0.05004655
Iteration 370, loss = 0.04876415
Iteration 371, loss = 0.04895220
Iteration 372, loss = 0.03666646
Iteration 373, loss = 0.03676733
Iteration 374, loss = 0.03930535
Iteration 375, loss = 0.05201055
Iteration 376, loss = 0.05369003
Iteration 377, loss = 0.05537483
Iteration 378, loss = 0.04573004
Iteration 379, loss = 0.03848492
Iteration 380, loss = 0.03956144
Iteration 381, loss = 0.04056970
Iteration 382, loss = 0.03734372
Iteration 383, loss = 0.03987727
Iteration 384, loss = 0.03982992
Iteration 385, loss = 0.03825970
Iteration 386, loss = 0.03926418
Iteration 387, loss = 0.04786075
Iteration 388, loss = 0.03803980
Iteration 389, loss = 0.04127238
Iteration 390, loss = 0.04149565
Iteration 391, loss = 0.03740204
Iteration 392, loss = 0.03910038
Iteration 393, loss = 0.05426474
Iteration 394, loss = 0.04506400
Iteration 395, loss = 0.03814818
Iteration 396, loss = 0.04265897
Iteration 397, loss = 0.05140093
Iteration 398, loss = 0.04563853
Iteration 399, loss = 0.05491605
Iteration 400, loss = 0.06795920
Iteration 401, loss = 0.05046708
Iteration 402, loss = 0.06013330
Iteration 403, loss = 0.04123889
Iteration 404, loss = 0.03887667
Iteration 405, loss = 0.03725588
Iteration 406, loss = 0.03654415
Iteration 407, loss = 0.04129835
Iteration 408, loss = 0.04330396
Iteration 409, loss = 0.05147042
Iteration 410, loss = 0.04913959
Iteration 411, loss = 0.05438438
Iteration 412, loss = 0.05165019
Iteration 413, loss = 0.04709178
Iteration 414, loss = 0.03853930
Iteration 415, loss = 0.03908973
Iteration 416, loss = 0.04255595
Iteration 417, loss = 0.05120738
Iteration 418, loss = 0.04152307
Iteration 419, loss = 0.04038292
Iteration 420, loss = 0.04082345
Iteration 421, loss = 0.05316067
Iteration 422, loss = 0.04090435
Iteration 423, loss = 0.03814780
Iteration 424, loss = 0.05924786
Iteration 425, loss = 0.05482549
Iteration 426, loss = 0.04228884
Iteration 427, loss = 0.05203822
Iteration 428, loss = 0.03692096
Iteration 429, loss = 0.04723498
Iteration 430, loss = 0.05128015
Iteration 431, loss = 0.05119621
Iteration 432, loss = 0.06172889
Iteration 433, loss = 0.07714596
Iteration 434, loss = 0.05435215
Iteration 435, loss = 0.04848953
Iteration 436, loss = 0.05295824
Iteration 437, loss = 0.03955612
Iteration 438, loss = 0.03523019
Iteration 439, loss = 0.04393517
Iteration 440, loss = 0.04032175
Iteration 441, loss = 0.04714604
Iteration 442, loss = 0.03944695
Iteration 443, loss = 0.03652231
Iteration 444, loss = 0.03777584
Iteration 445, loss = 0.03852312
Iteration 446, loss = 0.03663240
Iteration 447, loss = 0.04436663
Iteration 448, loss = 0.04266984
Iteration 449, loss = 0.03817230
Iteration 450, loss = 0.03663682
Iteration 451, loss = 0.03628770
Iteration 452, loss = 0.03742733
Iteration 453, loss = 0.03810867
Iteration 454, loss = 0.06326162
Iteration 455, loss = 0.05773436
Iteration 456, loss = 0.03799946
Iteration 457, loss = 0.05954770
Iteration 458, loss = 0.03968767
Iteration 459, loss = 0.03954811
Iteration 460, loss = 0.04091259
Iteration 461, loss = 0.03756954
Iteration 462, loss = 0.03442392
Iteration 463, loss = 0.03645769
Iteration 464, loss = 0.06612744
Iteration 465, loss = 0.04778905
Iteration 466, loss = 0.06142687
Iteration 467, loss = 0.06969498
Iteration 468, loss = 0.05247545
Iteration 469, loss = 0.05192063
Iteration 470, loss = 0.04388695
Iteration 471, loss = 0.06097875
Iteration 472, loss = 0.05890188
Iteration 473, loss = 0.08599181
Iteration 474, loss = 0.07333294
Iteration 475, loss = 0.09275557
Iteration 476, loss = 0.12257444
Iteration 477, loss = 0.09706305
Iteration 478, loss = 0.06908658
Iteration 479, loss = 0.04802731
Iteration 480, loss = 0.03785403
Iteration 481, loss = 0.03628339
Iteration 482, loss = 0.03909306
Iteration 483, loss = 0.04321411
Iteration 484, loss = 0.04643515
Iteration 485, loss = 0.04395432
Iteration 486, loss = 0.04094779
Iteration 487, loss = 0.03500030
Iteration 488, loss = 0.03437620
Iteration 489, loss = 0.03559083
Iteration 490, loss = 0.03657428
Iteration 491, loss = 0.03871518
Iteration 492, loss = 0.03872871
Iteration 493, loss = 0.03520154
Iteration 494, loss = 0.04052623
Iteration 495, loss = 0.04572610
Iteration 496, loss = 0.05187251
Iteration 497, loss = 0.05569436
Iteration 498, loss = 0.06365155
Iteration 499, loss = 0.06766875
Iteration 500, loss = 0.06252426
Iteration 501, loss = 0.05744100
Iteration 502, loss = 0.06232114
Iteration 503, loss = 0.06022926
Iteration 504, loss = 0.04694020
Iteration 505, loss = 0.04119300
Iteration 506, loss = 0.04088524
Iteration 507, loss = 0.03488084
Iteration 508, loss = 0.03449758
Iteration 509, loss = 0.03780900
Iteration 510, loss = 0.05262121
Iteration 511, loss = 0.05301372
Iteration 512, loss = 0.03778547
Iteration 513, loss = 0.03690091
Iteration 514, loss = 0.03599376
Iteration 515, loss = 0.03772488
Iteration 516, loss = 0.04893721
Iteration 517, loss = 0.05956641
Iteration 518, loss = 0.04812554
Iteration 519, loss = 0.04785230
Iteration 520, loss = 0.03425155
Iteration 521, loss = 0.04207264
Iteration 522, loss = 0.03888442
Iteration 523, loss = 0.04281980
Iteration 524, loss = 0.03880633
Iteration 525, loss = 0.04562651
Iteration 526, loss = 0.04483097
Iteration 527, loss = 0.07546407
Iteration 528, loss = 0.07034849
Iteration 529, loss = 0.05537002
Iteration 530, loss = 0.04137869
Iteration 531, loss = 0.04163392
Iteration 532, loss = 0.04394350
Iteration 533, loss = 0.04763558
Iteration 534, loss = 0.07226011
Iteration 535, loss = 0.06957829
Iteration 536, loss = 0.06107009
Iteration 537, loss = 0.07044481
Iteration 538, loss = 0.05352275
Iteration 539, loss = 0.05567994
Iteration 540, loss = 0.07169178
Iteration 541, loss = 0.06063142
Iteration 542, loss = 0.05487176
Iteration 543, loss = 0.06750624
Iteration 544, loss = 0.06413693
Iteration 545, loss = 0.07177779
Iteration 546, loss = 0.07820999
Iteration 547, loss = 0.06584873
Iteration 548, loss = 0.05064168
Iteration 549, loss = 0.05404426
Iteration 550, loss = 0.06147725
Iteration 551, loss = 0.08849761
Iteration 552, loss = 0.08210743
Iteration 553, loss = 0.05495794
Iteration 554, loss = 0.05186623
Iteration 555, loss = 0.04148750
Iteration 556, loss = 0.04783689
Iteration 557, loss = 0.03585307
Iteration 558, loss = 0.03569425
Iteration 559, loss = 0.03597152
Iteration 560, loss = 0.04143139
Iteration 561, loss = 0.03531025
Iteration 562, loss = 0.03468792
Iteration 563, loss = 0.03605194
Iteration 564, loss = 0.03616351
Iteration 565, loss = 0.03537251
Iteration 566, loss = 0.03389700
Iteration 567, loss = 0.03524922
Iteration 568, loss = 0.03538998
Iteration 569, loss = 0.04045411
Iteration 570, loss = 0.03420608
Iteration 571, loss = 0.03381167
Iteration 572, loss = 0.04030196
Iteration 573, loss = 0.03366100
Iteration 574, loss = 0.03602849
Iteration 575, loss = 0.03891999
Iteration 576, loss = 0.03984755
Iteration 577, loss = 0.04350865
Iteration 578, loss = 0.03716857
Iteration 579, loss = 0.04638535
Iteration 580, loss = 0.04498443
Iteration 581, loss = 0.04306023
Iteration 582, loss = 0.04946513
Iteration 583, loss = 0.04020297
Iteration 584, loss = 0.03648917
Iteration 585, loss = 0.03503186
Iteration 586, loss = 0.03558903
Iteration 587, loss = 0.03365637
Iteration 588, loss = 0.03464411
Iteration 589, loss = 0.05517299
Iteration 590, loss = 0.05001084
Iteration 591, loss = 0.05655828
Iteration 592, loss = 0.06421233
Iteration 593, loss = 0.07105658
Iteration 594, loss = 0.05490107
Iteration 595, loss = 0.05067473
Iteration 596, loss = 0.04539804
Iteration 597, loss = 0.04646087
Iteration 598, loss = 0.06854808
Iteration 599, loss = 0.05776209
Iteration 600, loss = 0.04918376
Iteration 601, loss = 0.03642495
Iteration 602, loss = 0.03656078
Iteration 603, loss = 0.03841362
Iteration 604, loss = 0.03567328
Iteration 605, loss = 0.03668334
Iteration 606, loss = 0.03544729
Iteration 607, loss = 0.03501546
Iteration 608, loss = 0.03486811
Iteration 609, loss = 0.03967925
Iteration 610, loss = 0.03595225
Iteration 611, loss = 0.03594242
Iteration 612, loss = 0.03839089
Iteration 613, loss = 0.03395365
Iteration 614, loss = 0.03368390
Iteration 615, loss = 0.03519796
Iteration 616, loss = 0.03632728
Iteration 617, loss = 0.03734657
Iteration 618, loss = 0.03348675
Iteration 619, loss = 0.03382007
Iteration 620, loss = 0.03505315
Iteration 621, loss = 0.03535158
Iteration 622, loss = 0.03447482
Iteration 623, loss = 0.03690268
Iteration 624, loss = 0.04141960
Iteration 625, loss = 0.04741895
Iteration 626, loss = 0.05269242
Iteration 627, loss = 0.05427448
Iteration 628, loss = 0.06245296
Iteration 629, loss = 0.05166370
Iteration 630, loss = 0.03918250
Iteration 631, loss = 0.04762602
Iteration 632, loss = 0.03380364
Iteration 633, loss = 0.03398308
Iteration 634, loss = 0.03449287
Iteration 635, loss = 0.03595965
Iteration 636, loss = 0.04252637
Iteration 637, loss = 0.04242789
Iteration 638, loss = 0.04484941
Iteration 639, loss = 0.04048497
Iteration 640, loss = 0.04769633
Iteration 641, loss = 0.04901696
Iteration 642, loss = 0.05029500
Iteration 643, loss = 0.04929180
Iteration 644, loss = 0.05513883
Iteration 645, loss = 0.06209778
Iteration 646, loss = 0.05247326
Iteration 647, loss = 0.04579017
Iteration 648, loss = 0.04007168
Iteration 649, loss = 0.03852834
Iteration 650, loss = 0.03479181
Iteration 651, loss = 0.03355818
Iteration 652, loss = 0.03505154
Iteration 653, loss = 0.04197343
Iteration 654, loss = 0.04173723
Iteration 655, loss = 0.04707815
Iteration 656, loss = 0.04446320
Iteration 657, loss = 0.03793873
Iteration 658, loss = 0.03652577
Iteration 659, loss = 0.03492606
Iteration 660, loss = 0.03943621
Iteration 661, loss = 0.05016185
Iteration 662, loss = 0.04022430
Iteration 663, loss = 0.03985346
Iteration 664, loss = 0.03678566
Iteration 665, loss = 0.03547485
Iteration 666, loss = 0.03791200
Iteration 667, loss = 0.03956733
Iteration 668, loss = 0.04022451
Iteration 669, loss = 0.04590180
Iteration 670, loss = 0.03929881
Iteration 671, loss = 0.03471017
Iteration 672, loss = 0.04594918
Iteration 673, loss = 0.08815983
Iteration 674, loss = 0.09138895
Iteration 675, loss = 0.10524622
Iteration 676, loss = 0.08070357
Iteration 677, loss = 0.05365759
Iteration 678, loss = 0.05266200
Iteration 679, loss = 0.04998507
Iteration 680, loss = 0.05892069
Iteration 681, loss = 0.05815981
Iteration 682, loss = 0.05284391
Iteration 683, loss = 0.04328842
Iteration 684, loss = 0.04205413
Iteration 685, loss = 0.04244881
Iteration 686, loss = 0.07413136
Iteration 687, loss = 0.05493788
Iteration 688, loss = 0.05362788
Iteration 689, loss = 0.04283418
Iteration 690, loss = 0.03707373
Iteration 691, loss = 0.03862497
Iteration 692, loss = 0.03685134
Iteration 693, loss = 0.04759600
Iteration 694, loss = 0.05156007
Iteration 695, loss = 0.05955859
Iteration 696, loss = 0.04671652
Iteration 697, loss = 0.03770509
Iteration 698, loss = 0.03880391
Iteration 699, loss = 0.04735635
Iteration 700, loss = 0.04141400
Iteration 701, loss = 0.04336067
Iteration 702, loss = 0.04675369
Iteration 703, loss = 0.05312238
Iteration 704, loss = 0.04765578
Iteration 705, loss = 0.04704789
Iteration 706, loss = 0.04033061
Iteration 707, loss = 0.04834038
Iteration 708, loss = 0.06759298
Iteration 709, loss = 0.05685193
Iteration 710, loss = 0.04807387
Iteration 711, loss = 0.04222421
Iteration 712, loss = 0.04831297
Iteration 713, loss = 0.04920907
Iteration 714, loss = 0.05844288
Iteration 715, loss = 0.05174407
Iteration 716, loss = 0.07949248
Iteration 717, loss = 0.07867108
Iteration 718, loss = 0.06030868
Iteration 719, loss = 0.06408655
Iteration 720, loss = 0.04860414
Iteration 721, loss = 0.04034845
Iteration 722, loss = 0.05531371
Iteration 723, loss = 0.04374588
Iteration 724, loss = 0.03458131
Iteration 725, loss = 0.03440101
Iteration 726, loss = 0.03445212
Iteration 727, loss = 0.03901125
Iteration 728, loss = 0.04161818
Iteration 729, loss = 0.03915574
Iteration 730, loss = 0.03685938
Iteration 731, loss = 0.03439311
Iteration 732, loss = 0.03523150
Iteration 733, loss = 0.03812305
Iteration 734, loss = 0.03523127
Iteration 735, loss = 0.03472984
Iteration 736, loss = 0.03906330
Iteration 737, loss = 0.04554010
Iteration 738, loss = 0.04202647
Iteration 739, loss = 0.03878974
Iteration 740, loss = 0.03767695
Iteration 741, loss = 0.03488667
Iteration 742, loss = 0.03690645
Iteration 743, loss = 0.03384737
Iteration 744, loss = 0.03715325
Iteration 745, loss = 0.03417129
Iteration 746, loss = 0.03815913
Iteration 747, loss = 0.05023427
Iteration 748, loss = 0.04210333
Iteration 749, loss = 0.03522640
Iteration 750, loss = 0.03385084
Iteration 751, loss = 0.03994597
Iteration 752, loss = 0.03850537
Iteration 753, loss = 0.03444704
Iteration 754, loss = 0.03344383
Iteration 755, loss = 0.03379062
Iteration 756, loss = 0.03396482
Iteration 757, loss = 0.03418414
Iteration 758, loss = 0.03801515
Iteration 759, loss = 0.03766370
Iteration 760, loss = 0.03650933
Iteration 761, loss = 0.03754782
Iteration 762, loss = 0.04104508
Iteration 763, loss = 0.05157036
Iteration 764, loss = 0.05666884
Iteration 765, loss = 0.04827479
Iteration 766, loss = 0.04698942
Iteration 767, loss = 0.04112962
Iteration 768, loss = 0.03973405
Iteration 769, loss = 0.04332327
Iteration 770, loss = 0.03972225
Iteration 771, loss = 0.03386701
Iteration 772, loss = 0.04115389
Iteration 773, loss = 0.03659809
Iteration 774, loss = 0.03563716
Iteration 775, loss = 0.03963945
Iteration 776, loss = 0.06161152
Iteration 777, loss = 0.05025149
Iteration 778, loss = 0.05481182
Iteration 779, loss = 0.05400962
Iteration 780, loss = 0.06193176
Iteration 781, loss = 0.07925979
Iteration 782, loss = 0.05637177
Iteration 783, loss = 0.05183924
Iteration 784, loss = 0.04897173
Iteration 785, loss = 0.04121589
Iteration 786, loss = 0.04189738
Iteration 787, loss = 0.04355415
Iteration 788, loss = 0.04087573
Iteration 789, loss = 0.04160705
Iteration 790, loss = 0.04807577
Iteration 791, loss = 0.03797812
Iteration 792, loss = 0.03514988
Iteration 793, loss = 0.03505951
Iteration 794, loss = 0.03435253
Iteration 795, loss = 0.03447044
Iteration 796, loss = 0.03419915
Iteration 797, loss = 0.03470741
Iteration 798, loss = 0.03892968
Iteration 799, loss = 0.04778329
Iteration 800, loss = 0.05048669
Iteration 801, loss = 0.05464867
Iteration 802, loss = 0.05062882
Iteration 803, loss = 0.03767068
Iteration 804, loss = 0.04215544
Iteration 805, loss = 0.04084433
Iteration 806, loss = 0.03537544
Iteration 807, loss = 0.03432478
Iteration 808, loss = 0.03370000
Iteration 809, loss = 0.03654003
Iteration 810, loss = 0.03675664
Iteration 811, loss = 0.03568536
Iteration 812, loss = 0.03382517
Iteration 813, loss = 0.03636601
Iteration 814, loss = 0.03540628
Iteration 815, loss = 0.03600530
Iteration 816, loss = 0.03927027
Iteration 817, loss = 0.03713937
Iteration 818, loss = 0.03570362
Iteration 819, loss = 0.03647339
Iteration 820, loss = 0.03959038
Iteration 821, loss = 0.03900144
Iteration 822, loss = 0.04152752
Iteration 823, loss = 0.04530029
Iteration 824, loss = 0.04571326
Iteration 825, loss = 0.04256069
Iteration 826, loss = 0.04494909
Iteration 827, loss = 0.03926146
Iteration 828, loss = 0.03406444
Iteration 829, loss = 0.03433819
Iteration 830, loss = 0.03436429
Iteration 831, loss = 0.03361625
Iteration 832, loss = 0.03755741
Iteration 833, loss = 0.04422895
Iteration 834, loss = 0.03650490
Iteration 835, loss = 0.03403261
Iteration 836, loss = 0.03388912
Iteration 837, loss = 0.03407952
Iteration 838, loss = 0.03457271
Iteration 839, loss = 0.03367162
Iteration 840, loss = 0.03448770
Iteration 841, loss = 0.03389793
Iteration 842, loss = 0.04357029
Iteration 843, loss = 0.05019880
Iteration 844, loss = 0.05073696
Iteration 845, loss = 0.06075879
Iteration 846, loss = 0.06829948
Iteration 847, loss = 0.05554647
Iteration 848, loss = 0.04395885
Iteration 849, loss = 0.04478526
Iteration 850, loss = 0.05396383
Iteration 851, loss = 0.04708700
Iteration 852, loss = 0.04218003
Iteration 853, loss = 0.04773232
Iteration 854, loss = 0.03772773
Iteration 855, loss = 0.03521707
Iteration 856, loss = 0.04163758
Iteration 857, loss = 0.04355439
Iteration 858, loss = 0.03842238
Iteration 859, loss = 0.03399865
Iteration 860, loss = 0.03638409
Iteration 861, loss = 0.03379538
Iteration 862, loss = 0.03403764
Iteration 863, loss = 0.03895627
Iteration 864, loss = 0.03374443
Iteration 865, loss = 0.03389941
Iteration 866, loss = 0.03596054
Iteration 867, loss = 0.04402177
Iteration 868, loss = 0.03959051
Iteration 869, loss = 0.03343820
Iteration 870, loss = 0.03307637
Iteration 871, loss = 0.03288097
Iteration 872, loss = 0.03308394
Iteration 873, loss = 0.03462693
Iteration 874, loss = 0.03291764
Iteration 875, loss = 0.03687046
Iteration 876, loss = 0.04200605
Iteration 877, loss = 0.04742486
Iteration 878, loss = 0.05014196
Iteration 879, loss = 0.04565843
Iteration 880, loss = 0.04018981
Iteration 881, loss = 0.03476149
Iteration 882, loss = 0.03427788
Iteration 883, loss = 0.03587774
Iteration 884, loss = 0.03245671
Iteration 885, loss = 0.03399341
Iteration 886, loss = 0.03260733
Iteration 887, loss = 0.03339396
Iteration 888, loss = 0.03282186
Iteration 889, loss = 0.03642276
Iteration 890, loss = 0.03442583
Iteration 891, loss = 0.04020840
Iteration 892, loss = 0.07351332
Iteration 893, loss = 0.06989466
Iteration 894, loss = 0.05129235
Iteration 895, loss = 0.04499604
Iteration 896, loss = 0.04792167
Iteration 897, loss = 0.04141988
Iteration 898, loss = 0.04174289
Iteration 899, loss = 0.03974072
Iteration 900, loss = 0.05115521
Iteration 901, loss = 0.04637024
Iteration 902, loss = 0.03926920
Iteration 903, loss = 0.03835938
Iteration 904, loss = 0.05111111
Iteration 905, loss = 0.04843737
Iteration 906, loss = 0.05235791
Iteration 907, loss = 0.03645562
Iteration 908, loss = 0.04041051
Iteration 909, loss = 0.04393717
Iteration 910, loss = 0.03356699
Iteration 911, loss = 0.03228932
Iteration 912, loss = 0.03346302
Iteration 913, loss = 0.03442976
Iteration 914, loss = 0.03492757
Iteration 915, loss = 0.03213612
Iteration 916, loss = 0.03195538
Iteration 917, loss = 0.03204013
Iteration 918, loss = 0.03426044
Iteration 919, loss = 0.03177021
Iteration 920, loss = 0.03146236
Iteration 921, loss = 0.03718012
Iteration 922, loss = 0.03895257
Iteration 923, loss = 0.04156476
Iteration 924, loss = 0.03611118
Iteration 925, loss = 0.03395707
Iteration 926, loss = 0.03463523
Iteration 927, loss = 0.03708870
Iteration 928, loss = 0.03385263
Iteration 929, loss = 0.03289412
Iteration 930, loss = 0.03165792
Iteration 931, loss = 0.03158227
Iteration 932, loss = 0.03194316
Iteration 933, loss = 0.03108577
Iteration 934, loss = 0.03185986
Iteration 935, loss = 0.03150977
Iteration 936, loss = 0.03329382
Iteration 937, loss = 0.03278493
Iteration 938, loss = 0.03500611
Iteration 939, loss = 0.03914101
Iteration 940, loss = 0.04721485
Iteration 941, loss = 0.05560171
Iteration 942, loss = 0.05338970
Iteration 943, loss = 0.03973592
Iteration 944, loss = 0.03991751
Iteration 945, loss = 0.04247815
Iteration 946, loss = 0.04273723
Iteration 947, loss = 0.03779555
Iteration 948, loss = 0.04312816
Iteration 949, loss = 0.03982228
Iteration 950, loss = 0.03706178
Iteration 951, loss = 0.03393776
Iteration 952, loss = 0.03433139
Iteration 953, loss = 0.03347503
Iteration 954, loss = 0.03740281
Iteration 955, loss = 0.03423378
Iteration 956, loss = 0.03112797
Iteration 957, loss = 0.03337054
Iteration 958, loss = 0.03484363
Iteration 959, loss = 0.03322680
Iteration 960, loss = 0.03905663
Iteration 961, loss = 0.03409633
Iteration 962, loss = 0.03629583
Iteration 963, loss = 0.03193986
Iteration 964, loss = 0.03027201
Iteration 965, loss = 0.03067059
Iteration 966, loss = 0.03302915
Iteration 967, loss = 0.03368745
Iteration 968, loss = 0.03274477
Iteration 969, loss = 0.03211725
Iteration 970, loss = 0.03178305
Iteration 971, loss = 0.03836822
Iteration 972, loss = 0.03898708
Iteration 973, loss = 0.03853728
Iteration 974, loss = 0.03301590
Iteration 975, loss = 0.03292132
Iteration 976, loss = 0.03296422
Iteration 977, loss = 0.03194458
Iteration 978, loss = 0.03222185
Iteration 979, loss = 0.03787276
Iteration 980, loss = 0.04533975
Iteration 981, loss = 0.04939537
Iteration 982, loss = 0.04615320
Iteration 983, loss = 0.04533721
Iteration 984, loss = 0.03954404
Iteration 985, loss = 0.04260441
Iteration 986, loss = 0.05643893
Iteration 987, loss = 0.05870997
Iteration 988, loss = 0.04095300
Iteration 989, loss = 0.03209574
Iteration 990, loss = 0.03324370
Iteration 991, loss = 0.02994996
Iteration 992, loss = 0.02992621
Iteration 993, loss = 0.03007034
Iteration 994, loss = 0.03010987
Iteration 995, loss = 0.03135736
Iteration 996, loss = 0.03229354
Iteration 997, loss = 0.03620167
Iteration 998, loss = 0.03758069
Iteration 999, loss = 0.04351504
Iteration 1000, loss = 0.03574699
Run 1
Iteration 1, loss = 2865.32425553
Iteration 2, loss = 717.67213829
Iteration 3, loss = 62.94793176
Iteration 4, loss = 114.93095979
Iteration 5, loss = 44.59789504
Iteration 6, loss = 7.36041653
Iteration 7, loss = 20.95023343
Iteration 8, loss = 2.37698396
Iteration 9, loss = 7.40347734
Iteration 10, loss = 1.22073265
Iteration 11, loss = 0.97872811
Iteration 12, loss = 0.47559184
Iteration 13, loss = 0.28074225
Iteration 14, loss = 0.22346063
Iteration 15, loss = 0.89123994
Iteration 16, loss = 0.55858802
Iteration 17, loss = 0.18384487
Iteration 18, loss = 0.12435685
Iteration 19, loss = 0.06891186
Iteration 20, loss = 0.05909337
Iteration 21, loss = 0.05605863
Iteration 22, loss = 0.05788712
Iteration 23, loss = 0.05602743
Iteration 24, loss = 0.05652772
Iteration 25, loss = 0.05632038
Iteration 26, loss = 0.05679104
Iteration 27, loss = 0.05641973
Iteration 28, loss = 0.05591767
Iteration 29, loss = 0.05562340
Iteration 30, loss = 0.05591738
Iteration 31, loss = 0.05732137
Iteration 32, loss = 0.05816924
Iteration 33, loss = 0.05625722
Iteration 34, loss = 0.05549598
Iteration 35, loss = 0.05600066
Iteration 36, loss = 0.05612268
Iteration 37, loss = 0.05673380
Iteration 38, loss = 0.05647528
Iteration 39, loss = 0.05558445
Iteration 40, loss = 0.05703341
Iteration 41, loss = 0.05746165
Iteration 42, loss = 0.05531064
Iteration 43, loss = 0.05633532
Iteration 44, loss = 0.05573424
Iteration 45, loss = 0.05527930
Iteration 46, loss = 0.05545167
Iteration 47, loss = 0.05515961
Iteration 48, loss = 0.05523651
Iteration 49, loss = 0.05560027
Iteration 50, loss = 0.05539663
Iteration 51, loss = 0.05615432
Iteration 52, loss = 0.05592115
Iteration 53, loss = 0.05620189
Iteration 54, loss = 0.05563915
Iteration 55, loss = 0.05636412
Iteration 56, loss = 0.05498739
Iteration 57, loss = 0.05506367
Iteration 58, loss = 0.05472707
Iteration 59, loss = 0.05496697
Iteration 60, loss = 0.05602588
Iteration 61, loss = 0.05547341
Iteration 62, loss = 0.05461394
Iteration 63, loss = 0.05513032
Iteration 64, loss = 0.05550852
Iteration 65, loss = 0.05471140
Iteration 66, loss = 0.05483855
Iteration 67, loss = 0.05494763
Iteration 68, loss = 0.05524601
Iteration 69, loss = 0.05541686
Iteration 70, loss = 0.05827645
Iteration 71, loss = 0.05592192
Iteration 72, loss = 0.05569970
Iteration 73, loss = 0.06175396
Iteration 74, loss = 0.05573071
Iteration 75, loss = 0.05493981
Iteration 76, loss = 0.05538702
Iteration 77, loss = 0.05795391
Iteration 78, loss = 0.05626468
Iteration 79, loss = 0.05633285
Iteration 80, loss = 0.05568768
Iteration 81, loss = 0.05479459
Iteration 82, loss = 0.05775644
Iteration 83, loss = 0.05718286
Iteration 84, loss = 0.05651973
Iteration 85, loss = 0.05655834
Iteration 86, loss = 0.05675840
Iteration 87, loss = 0.05859611
Iteration 88, loss = 0.05409708
Iteration 89, loss = 0.05412579
Iteration 90, loss = 0.05526614
Iteration 91, loss = 0.05640907
Iteration 92, loss = 0.05473886
Iteration 93, loss = 0.05555291
Iteration 94, loss = 0.05371705
Iteration 95, loss = 0.05503024
Iteration 96, loss = 0.05529872
Iteration 97, loss = 0.05535846
Iteration 98, loss = 0.05628939
Iteration 99, loss = 0.05400885
Iteration 100, loss = 0.05365477
Iteration 101, loss = 0.05377208
Iteration 102, loss = 0.05455362
Iteration 103, loss = 0.05484094
Iteration 104, loss = 0.05962897
Iteration 105, loss = 0.06112624
Iteration 106, loss = 0.05556135
Iteration 107, loss = 0.05492797
Iteration 108, loss = 0.05402010
Iteration 109, loss = 0.05653537
Iteration 110, loss = 0.05741356
Iteration 111, loss = 0.05660303
Iteration 112, loss = 0.05439105
Iteration 113, loss = 0.05505735
Iteration 114, loss = 0.05387840
Iteration 115, loss = 0.05591081
Iteration 116, loss = 0.05407372
Iteration 117, loss = 0.05452164
Iteration 118, loss = 0.05532591
Iteration 119, loss = 0.05476869
Iteration 120, loss = 0.05564084
Iteration 121, loss = 0.05487719
Iteration 122, loss = 0.05341608
Iteration 123, loss = 0.05481118
Iteration 124, loss = 0.05467760
Iteration 125, loss = 0.05414955
Iteration 126, loss = 0.05393503
Iteration 127, loss = 0.05274370
Iteration 128, loss = 0.05341078
Iteration 129, loss = 0.05457780
Iteration 130, loss = 0.05976321
Iteration 131, loss = 0.06060874
Iteration 132, loss = 0.05442661
Iteration 133, loss = 0.05742695
Iteration 134, loss = 0.05350528
Iteration 135, loss = 0.05397305
Iteration 136, loss = 0.05361253
Iteration 137, loss = 0.05242301
Iteration 138, loss = 0.05247615
Iteration 139, loss = 0.05203311
Iteration 140, loss = 0.05615860
Iteration 141, loss = 0.05517409
Iteration 142, loss = 0.05581187
Iteration 143, loss = 0.06734940
Iteration 144, loss = 0.05755349
Iteration 145, loss = 0.06323183
Iteration 146, loss = 0.07935580
Iteration 147, loss = 0.06619981
Iteration 148, loss = 0.06436275
Iteration 149, loss = 0.06020123
Iteration 150, loss = 0.06646518
Iteration 151, loss = 0.06177248
Iteration 152, loss = 0.06058771
Iteration 153, loss = 0.06199175
Iteration 154, loss = 0.06331443
Iteration 155, loss = 0.06502885
Iteration 156, loss = 0.06164314
Iteration 157, loss = 0.05684339
Iteration 158, loss = 0.05355780
Iteration 159, loss = 0.05607170
Iteration 160, loss = 0.06069541
Iteration 161, loss = 0.06154958
Iteration 162, loss = 0.06934074
Iteration 163, loss = 0.05692861
Iteration 164, loss = 0.05100032
Iteration 165, loss = 0.05281996
Iteration 166, loss = 0.05182612
Iteration 167, loss = 0.05324141
Iteration 168, loss = 0.05140822
Iteration 169, loss = 0.05183329
Iteration 170, loss = 0.05227062
Iteration 171, loss = 0.05322939
Iteration 172, loss = 0.05304863
Iteration 173, loss = 0.05437912
Iteration 174, loss = 0.05417684
Iteration 175, loss = 0.05526722
Iteration 176, loss = 0.05276873
Iteration 177, loss = 0.05203126
Iteration 178, loss = 0.05110654
Iteration 179, loss = 0.05348738
Iteration 180, loss = 0.05149938
Iteration 181, loss = 0.05126847
Iteration 182, loss = 0.05222799
Iteration 183, loss = 0.05405641
Iteration 184, loss = 0.05916649
Iteration 185, loss = 0.05740020
Iteration 186, loss = 0.05635676
Iteration 187, loss = 0.06257040
Iteration 188, loss = 0.06590926
Iteration 189, loss = 0.05741905
Iteration 190, loss = 0.05307354
Iteration 191, loss = 0.05598218
Iteration 192, loss = 0.06653858
Iteration 193, loss = 0.06045122
Iteration 194, loss = 0.05525977
Iteration 195, loss = 0.05241883
Iteration 196, loss = 0.05196817
Iteration 197, loss = 0.05451768
Iteration 198, loss = 0.05054133
Iteration 199, loss = 0.05096382
Iteration 200, loss = 0.05098262
Iteration 201, loss = 0.05104195
Iteration 202, loss = 0.05519097
Iteration 203, loss = 0.05920034
Iteration 204, loss = 0.05957233
Iteration 205, loss = 0.05716358
Iteration 206, loss = 0.05249204
Iteration 207, loss = 0.05385675
Iteration 208, loss = 0.05937505
Iteration 209, loss = 0.06050933
Iteration 210, loss = 0.05871178
Iteration 211, loss = 0.05531967
Iteration 212, loss = 0.06600140
Iteration 213, loss = 0.05523012
Iteration 214, loss = 0.05060300
Iteration 215, loss = 0.05007924
Iteration 216, loss = 0.04979471
Iteration 217, loss = 0.05187930
Iteration 218, loss = 0.05164453
Iteration 219, loss = 0.06168619
Iteration 220, loss = 0.05543735
Iteration 221, loss = 0.06194707
Iteration 222, loss = 0.05691544
Iteration 223, loss = 0.05502410
Iteration 224, loss = 0.06634572
Iteration 225, loss = 0.05776420
Iteration 226, loss = 0.05971544
Iteration 227, loss = 0.05533536
Iteration 228, loss = 0.05273302
Iteration 229, loss = 0.05363071
Iteration 230, loss = 0.04971222
Iteration 231, loss = 0.05042864
Iteration 232, loss = 0.05646191
Iteration 233, loss = 0.05188688
Iteration 234, loss = 0.05376036
Iteration 235, loss = 0.05560684
Iteration 236, loss = 0.05470838
Iteration 237, loss = 0.05277348
Iteration 238, loss = 0.05038288
Iteration 239, loss = 0.04899817
Iteration 240, loss = 0.04986227
Iteration 241, loss = 0.04915366
Iteration 242, loss = 0.04899468
Iteration 243, loss = 0.04937086
Iteration 244, loss = 0.05297617
Iteration 245, loss = 0.04991923
Iteration 246, loss = 0.05180362
Iteration 247, loss = 0.05146302
Iteration 248, loss = 0.06092603
Iteration 249, loss = 0.05301546
Iteration 250, loss = 0.04973574
Iteration 251, loss = 0.05142473
Iteration 252, loss = 0.05395637
Iteration 253, loss = 0.06546372
Iteration 254, loss = 0.05582266
Iteration 255, loss = 0.05695198
Iteration 256, loss = 0.05593280
Iteration 257, loss = 0.05353810
Iteration 258, loss = 0.05181737
Iteration 259, loss = 0.05046198
Iteration 260, loss = 0.05006662
Iteration 261, loss = 0.05250413
Iteration 262, loss = 0.05147285
Iteration 263, loss = 0.05160878
Iteration 264, loss = 0.05545969
Iteration 265, loss = 0.05214595
Iteration 266, loss = 0.05701524
Iteration 267, loss = 0.05931671
Iteration 268, loss = 0.05771399
Iteration 269, loss = 0.05222482
Iteration 270, loss = 0.05118096
Iteration 271, loss = 0.05028145
Iteration 272, loss = 0.05230527
Iteration 273, loss = 0.05118901
Iteration 274, loss = 0.05117846
Iteration 275, loss = 0.05378713
Iteration 276, loss = 0.05748967
Iteration 277, loss = 0.05746532
Iteration 278, loss = 0.04784970
Iteration 279, loss = 0.04856926
Iteration 280, loss = 0.04738511
Iteration 281, loss = 0.04687280
Iteration 282, loss = 0.05052196
Iteration 283, loss = 0.04909448
Iteration 284, loss = 0.04907601
Iteration 285, loss = 0.04937217
Iteration 286, loss = 0.04856324
Iteration 287, loss = 0.04985686
Iteration 288, loss = 0.04830178
Iteration 289, loss = 0.04967863
Iteration 290, loss = 0.05140796
Iteration 291, loss = 0.04763166
Iteration 292, loss = 0.04670086
Iteration 293, loss = 0.04695813
Iteration 294, loss = 0.04744250
Iteration 295, loss = 0.04649740
Iteration 296, loss = 0.04858816
Iteration 297, loss = 0.04694696
Iteration 298, loss = 0.04917402
Iteration 299, loss = 0.04895135
Iteration 300, loss = 0.04952454
Iteration 301, loss = 0.05474482
Iteration 302, loss = 0.05001074
Iteration 303, loss = 0.05244912
Iteration 304, loss = 0.04982097
Iteration 305, loss = 0.04603452
Iteration 306, loss = 0.04715735
Iteration 307, loss = 0.04915419
Iteration 308, loss = 0.05370974
Iteration 309, loss = 0.05117752
Iteration 310, loss = 0.05057973
Iteration 311, loss = 0.04706134
Iteration 312, loss = 0.04767745
Iteration 313, loss = 0.04916535
Iteration 314, loss = 0.05023591
Iteration 315, loss = 0.05706690
Iteration 316, loss = 0.05457813
Iteration 317, loss = 0.05605467
Iteration 318, loss = 0.05193381
Iteration 319, loss = 0.04968796
Iteration 320, loss = 0.04725129
Iteration 321, loss = 0.04761913
Iteration 322, loss = 0.04792660
Iteration 323, loss = 0.04740071
Iteration 324, loss = 0.05354660
Iteration 325, loss = 0.05331725
Iteration 326, loss = 0.06938984
Iteration 327, loss = 0.05073039
Iteration 328, loss = 0.05231829
Iteration 329, loss = 0.05246554
Iteration 330, loss = 0.05274130
Iteration 331, loss = 0.04588363
Iteration 332, loss = 0.04534261
Iteration 333, loss = 0.04557088
Iteration 334, loss = 0.04709830
Iteration 335, loss = 0.04798778
Iteration 336, loss = 0.04564808
Iteration 337, loss = 0.04553616
Iteration 338, loss = 0.04642421
Iteration 339, loss = 0.05703924
Iteration 340, loss = 0.05454375
Iteration 341, loss = 0.05203870
Iteration 342, loss = 0.05263413
Iteration 343, loss = 0.05734570
Iteration 344, loss = 0.05302111
Iteration 345, loss = 0.05357124
Iteration 346, loss = 0.05558586
Iteration 347, loss = 0.05813689
Iteration 348, loss = 0.05371500
Iteration 349, loss = 0.05957645
Iteration 350, loss = 0.06068102
Iteration 351, loss = 0.06048036
Iteration 352, loss = 0.05182195
Iteration 353, loss = 0.05528759
Iteration 354, loss = 0.05015627
Iteration 355, loss = 0.06173804
Iteration 356, loss = 0.04911714
Iteration 357, loss = 0.04839393
Iteration 358, loss = 0.04617213
Iteration 359, loss = 0.04727155
Iteration 360, loss = 0.04590148
Iteration 361, loss = 0.04846864
Iteration 362, loss = 0.05183178
Iteration 363, loss = 0.05254567
Iteration 364, loss = 0.04475475
Iteration 365, loss = 0.04613032
Iteration 366, loss = 0.04799464
Iteration 367, loss = 0.05134501
Iteration 368, loss = 0.04689904
Iteration 369, loss = 0.05420093
Iteration 370, loss = 0.05087672
Iteration 371, loss = 0.04928791
Iteration 372, loss = 0.06390407
Iteration 373, loss = 0.05113063
Iteration 374, loss = 0.05397869
Iteration 375, loss = 0.05567590
Iteration 376, loss = 0.04907654
Iteration 377, loss = 0.05440887
Iteration 378, loss = 0.04893770
Iteration 379, loss = 0.04512740
Iteration 380, loss = 0.04349116
Iteration 381, loss = 0.04363263
Iteration 382, loss = 0.05028014
Iteration 383, loss = 0.04844399
Iteration 384, loss = 0.04633818
Iteration 385, loss = 0.04926704
Iteration 386, loss = 0.05025855
Iteration 387, loss = 0.04567221
Iteration 388, loss = 0.04350769
Iteration 389, loss = 0.04722951
Iteration 390, loss = 0.04787745
Iteration 391, loss = 0.04852823
Iteration 392, loss = 0.05230587
Iteration 393, loss = 0.04656057
Iteration 394, loss = 0.04936378
Iteration 395, loss = 0.04889112
Iteration 396, loss = 0.05482491
Iteration 397, loss = 0.05245006
Iteration 398, loss = 0.05610149
Iteration 399, loss = 0.06691083
Iteration 400, loss = 0.05197996
Iteration 401, loss = 0.04653535
Iteration 402, loss = 0.04539915
Iteration 403, loss = 0.04494372
Iteration 404, loss = 0.04896946
Iteration 405, loss = 0.04667619
Iteration 406, loss = 0.04491431
Iteration 407, loss = 0.04360719
Iteration 408, loss = 0.04311495
Iteration 409, loss = 0.04311981
Iteration 410, loss = 0.04256622
Iteration 411, loss = 0.04377968
Iteration 412, loss = 0.04224889
Iteration 413, loss = 0.04305839
Iteration 414, loss = 0.04910154
Iteration 415, loss = 0.05100204
Iteration 416, loss = 0.05120302
Iteration 417, loss = 0.05284806
Iteration 418, loss = 0.05089663
Iteration 419, loss = 0.05325080
Iteration 420, loss = 0.04635175
Iteration 421, loss = 0.04609998
Iteration 422, loss = 0.04562750
Iteration 423, loss = 0.04871445
Iteration 424, loss = 0.06734541
Iteration 425, loss = 0.04910116
Iteration 426, loss = 0.04461663
Iteration 427, loss = 0.05012126
Iteration 428, loss = 0.04911220
Iteration 429, loss = 0.05961105
Iteration 430, loss = 0.05628335
Iteration 431, loss = 0.05119000
Iteration 432, loss = 0.04408043
Iteration 433, loss = 0.04899079
Iteration 434, loss = 0.04482264
Iteration 435, loss = 0.04715529
Iteration 436, loss = 0.04310098
Iteration 437, loss = 0.04253512
Iteration 438, loss = 0.04140538
Iteration 439, loss = 0.04289330
Iteration 440, loss = 0.04246867
Iteration 441, loss = 0.04264912
Iteration 442, loss = 0.04242312
Iteration 443, loss = 0.05352659
Iteration 444, loss = 0.04796665
Iteration 445, loss = 0.04821862
Iteration 446, loss = 0.04722627
Iteration 447, loss = 0.04367291
Iteration 448, loss = 0.05263957
Iteration 449, loss = 0.04643621
Iteration 450, loss = 0.04429404
Iteration 451, loss = 0.04487184
Iteration 452, loss = 0.05054275
Iteration 453, loss = 0.04894158
Iteration 454, loss = 0.05363343
Iteration 455, loss = 0.05131282
Iteration 456, loss = 0.04521954
Iteration 457, loss = 0.04584962
Iteration 458, loss = 0.04326665
Iteration 459, loss = 0.04067224
Iteration 460, loss = 0.04361043
Iteration 461, loss = 0.04439913
Iteration 462, loss = 0.04458230
Iteration 463, loss = 0.04683151
Iteration 464, loss = 0.04944544
Iteration 465, loss = 0.05827645
Iteration 466, loss = 0.05010015
Iteration 467, loss = 0.04929959
Iteration 468, loss = 0.04432270
Iteration 469, loss = 0.04057825
Iteration 470, loss = 0.04128767
Iteration 471, loss = 0.04230272
Iteration 472, loss = 0.04465669
Iteration 473, loss = 0.04663065
Iteration 474, loss = 0.04460396
Iteration 475, loss = 0.04218939
Iteration 476, loss = 0.04264449
Iteration 477, loss = 0.04074292
Iteration 478, loss = 0.04907484
Iteration 479, loss = 0.06133307
Iteration 480, loss = 0.04806852
Iteration 481, loss = 0.04361340
Iteration 482, loss = 0.04029936
Iteration 483, loss = 0.04039469
Iteration 484, loss = 0.04004942
Iteration 485, loss = 0.04030317
Iteration 486, loss = 0.04141031
Iteration 487, loss = 0.04042757
Iteration 488, loss = 0.04413226
Iteration 489, loss = 0.04090978
Iteration 490, loss = 0.03972544
Iteration 491, loss = 0.04070281
Iteration 492, loss = 0.03997821
Iteration 493, loss = 0.04313687
Iteration 494, loss = 0.04349374
Iteration 495, loss = 0.04266237
Iteration 496, loss = 0.04879679
Iteration 497, loss = 0.04373470
Iteration 498, loss = 0.04099465
Iteration 499, loss = 0.03987467
Iteration 500, loss = 0.04096542
Iteration 501, loss = 0.04007691
Iteration 502, loss = 0.04073048
Iteration 503, loss = 0.03951386
Iteration 504, loss = 0.04345771
Iteration 505, loss = 0.04398710
Iteration 506, loss = 0.04139235
Iteration 507, loss = 0.04427979
Iteration 508, loss = 0.04528143
Iteration 509, loss = 0.04033200
Iteration 510, loss = 0.04693870
Iteration 511, loss = 0.04615895
Iteration 512, loss = 0.03968523
Iteration 513, loss = 0.04142832
Iteration 514, loss = 0.04095299
Iteration 515, loss = 0.04519225
Iteration 516, loss = 0.03948881
Iteration 517, loss = 0.04121071
Iteration 518, loss = 0.04050569
Iteration 519, loss = 0.04252365
Iteration 520, loss = 0.04212144
Iteration 521, loss = 0.04257974
Iteration 522, loss = 0.04434044
Iteration 523, loss = 0.04810896
Iteration 524, loss = 0.04512756
Iteration 525, loss = 0.04760538
Iteration 526, loss = 0.04143526
Iteration 527, loss = 0.04053817
Iteration 528, loss = 0.04035920
Iteration 529, loss = 0.03916958
Iteration 530, loss = 0.03962436
Iteration 531, loss = 0.04286369
Iteration 532, loss = 0.04100907
Iteration 533, loss = 0.03972435
Iteration 534, loss = 0.04026996
Iteration 535, loss = 0.03977999
Iteration 536, loss = 0.04068846
Iteration 537, loss = 0.03863049
Iteration 538, loss = 0.04200804
Iteration 539, loss = 0.04043002
Iteration 540, loss = 0.04437524
Iteration 541, loss = 0.04610605
Iteration 542, loss = 0.04328059
Iteration 543, loss = 0.04161794
Iteration 544, loss = 0.03994285
Iteration 545, loss = 0.03919379
Iteration 546, loss = 0.04229868
Iteration 547, loss = 0.04016119
Iteration 548, loss = 0.03836430
Iteration 549, loss = 0.03837899
Iteration 550, loss = 0.04251219
Iteration 551, loss = 0.04388203
Iteration 552, loss = 0.04394969
Iteration 553, loss = 0.05274925
Iteration 554, loss = 0.05472788
Iteration 555, loss = 0.05183777
Iteration 556, loss = 0.04956184
Iteration 557, loss = 0.04358330
Iteration 558, loss = 0.04618330
Iteration 559, loss = 0.04586836
Iteration 560, loss = 0.04336144
Iteration 561, loss = 0.04208615
Iteration 562, loss = 0.04370765
Iteration 563, loss = 0.04672255
Iteration 564, loss = 0.04769225
Iteration 565, loss = 0.04347728
Iteration 566, loss = 0.04030822
Iteration 567, loss = 0.04437739
Iteration 568, loss = 0.04103996
Iteration 569, loss = 0.04073815
Iteration 570, loss = 0.04752603
Iteration 571, loss = 0.04401972
Iteration 572, loss = 0.05559165
Iteration 573, loss = 0.05329847
Iteration 574, loss = 0.04438942
Iteration 575, loss = 0.04274984
Iteration 576, loss = 0.04834643
Iteration 577, loss = 0.04619246
Iteration 578, loss = 0.04436115
Iteration 579, loss = 0.04339103
Iteration 580, loss = 0.04325339
Iteration 581, loss = 0.04455518
Iteration 582, loss = 0.04864162
Iteration 583, loss = 0.04370526
Iteration 584, loss = 0.04539506
Iteration 585, loss = 0.04776199
Iteration 586, loss = 0.04606224
Iteration 587, loss = 0.04203374
Iteration 588, loss = 0.04318957
Iteration 589, loss = 0.05493044
Iteration 590, loss = 0.04721251
Iteration 591, loss = 0.04823537
Iteration 592, loss = 0.03968517
Iteration 593, loss = 0.04351992
Iteration 594, loss = 0.04824515
Iteration 595, loss = 0.04059556
Iteration 596, loss = 0.05039508
Iteration 597, loss = 0.04866607
Iteration 598, loss = 0.04124498
Iteration 599, loss = 0.04348037
Iteration 600, loss = 0.04407237
Iteration 601, loss = 0.04728542
Iteration 602, loss = 0.05385035
Iteration 603, loss = 0.05443614
Iteration 604, loss = 0.04734017
Iteration 605, loss = 0.04074438
Iteration 606, loss = 0.04065759
Iteration 607, loss = 0.04112630
Iteration 608, loss = 0.03988708
Iteration 609, loss = 0.04653998
Iteration 610, loss = 0.04106178
Iteration 611, loss = 0.05571746
Iteration 612, loss = 0.05368088
Iteration 613, loss = 0.04759472
Iteration 614, loss = 0.04635665
Iteration 615, loss = 0.06176130
Iteration 616, loss = 0.05239095
Iteration 617, loss = 0.07610247
Iteration 618, loss = 0.05244176
Iteration 619, loss = 0.04232044
Iteration 620, loss = 0.04386485
Iteration 621, loss = 0.04055198
Iteration 622, loss = 0.04197536
Iteration 623, loss = 0.04305738
Iteration 624, loss = 0.04172831
Iteration 625, loss = 0.04004201
Iteration 626, loss = 0.04221378
Iteration 627, loss = 0.04249425
Iteration 628, loss = 0.04928334
Iteration 629, loss = 0.04689095
Iteration 630, loss = 0.04213685
Iteration 631, loss = 0.04722695
Iteration 632, loss = 0.04118090
Iteration 633, loss = 0.04176171
Iteration 634, loss = 0.03998859
Iteration 635, loss = 0.04575539
Iteration 636, loss = 0.04793071
Iteration 637, loss = 0.04781090
Iteration 638, loss = 0.04092629
Iteration 639, loss = 0.04384270
Iteration 640, loss = 0.04735868
Iteration 641, loss = 0.04060782
Iteration 642, loss = 0.04287556
Iteration 643, loss = 0.04221735
Iteration 644, loss = 0.05965625
Iteration 645, loss = 0.04785713
Iteration 646, loss = 0.04789626
Iteration 647, loss = 0.06574424
Iteration 648, loss = 0.05063964
Iteration 649, loss = 0.04679669
Iteration 650, loss = 0.04386369
Iteration 651, loss = 0.03875645
Iteration 652, loss = 0.04024239
Iteration 653, loss = 0.03943504
Iteration 654, loss = 0.04199520
Iteration 655, loss = 0.05002319
Iteration 656, loss = 0.05600432
Iteration 657, loss = 0.05498764
Iteration 658, loss = 0.06201165
Iteration 659, loss = 0.04292872
Iteration 660, loss = 0.04022956
Iteration 661, loss = 0.04039390
Iteration 662, loss = 0.04642676
Iteration 663, loss = 0.04537533
Iteration 664, loss = 0.04060282
Iteration 665, loss = 0.04151380
Iteration 666, loss = 0.03942475
Iteration 667, loss = 0.03777171
Iteration 668, loss = 0.03918850
Iteration 669, loss = 0.04008027
Iteration 670, loss = 0.03967758
Iteration 671, loss = 0.04001122
Iteration 672, loss = 0.03995563
Iteration 673, loss = 0.03828876
Iteration 674, loss = 0.03776760
Iteration 675, loss = 0.03831745
Iteration 676, loss = 0.04715369
Iteration 677, loss = 0.04790300
Iteration 678, loss = 0.04001137
Iteration 679, loss = 0.04570090
Iteration 680, loss = 0.05012836
Iteration 681, loss = 0.04184409
Iteration 682, loss = 0.04134228
Iteration 683, loss = 0.04135027
Iteration 684, loss = 0.04039867
Iteration 685, loss = 0.04035808
Iteration 686, loss = 0.03778185
Iteration 687, loss = 0.03791755
Iteration 688, loss = 0.03843360
Iteration 689, loss = 0.03802258
Iteration 690, loss = 0.03882192
Iteration 691, loss = 0.03769765
Iteration 692, loss = 0.03841555
Iteration 693, loss = 0.03831336
Iteration 694, loss = 0.03822845
Iteration 695, loss = 0.04230411
Iteration 696, loss = 0.04076084
Iteration 697, loss = 0.04029998
Iteration 698, loss = 0.04349128
Iteration 699, loss = 0.04948943
Iteration 700, loss = 0.05853229
Iteration 701, loss = 0.06798691
Iteration 702, loss = 0.05018990
Iteration 703, loss = 0.05508831
Iteration 704, loss = 0.06694192
Iteration 705, loss = 0.05329860
Iteration 706, loss = 0.06507240
Iteration 707, loss = 0.06559347
Iteration 708, loss = 0.05417537
Iteration 709, loss = 0.06221517
Iteration 710, loss = 0.05511263
Iteration 711, loss = 0.05745670
Iteration 712, loss = 0.06339233
Iteration 713, loss = 0.06122741
Iteration 714, loss = 0.06643194
Iteration 715, loss = 0.05320781
Iteration 716, loss = 0.05697612
Iteration 717, loss = 0.05101149
Iteration 718, loss = 0.05627553
Iteration 719, loss = 0.05062962
Iteration 720, loss = 0.05958399
Iteration 721, loss = 0.04699658
Iteration 722, loss = 0.04426013
Iteration 723, loss = 0.04008173
Iteration 724, loss = 0.04161686
Iteration 725, loss = 0.04027783
Iteration 726, loss = 0.03920027
Iteration 727, loss = 0.03933070
Iteration 728, loss = 0.04356392
Iteration 729, loss = 0.04000614
Iteration 730, loss = 0.03759940
Iteration 731, loss = 0.03929811
Iteration 732, loss = 0.04506000
Iteration 733, loss = 0.04312012
Iteration 734, loss = 0.04165303
Iteration 735, loss = 0.03876231
Iteration 736, loss = 0.05179336
Iteration 737, loss = 0.04681249
Iteration 738, loss = 0.04199471
Iteration 739, loss = 0.04046084
Iteration 740, loss = 0.03973872
Iteration 741, loss = 0.04032208
Iteration 742, loss = 0.03886896
Iteration 743, loss = 0.04293971
Iteration 744, loss = 0.04340615
Iteration 745, loss = 0.04038129
Iteration 746, loss = 0.03921267
Iteration 747, loss = 0.03851264
Iteration 748, loss = 0.04119000
Iteration 749, loss = 0.04071745
Iteration 750, loss = 0.04328949
Iteration 751, loss = 0.05035997
Iteration 752, loss = 0.04513003
Iteration 753, loss = 0.05382245
Iteration 754, loss = 0.05706100
Iteration 755, loss = 0.04555368
Iteration 756, loss = 0.03883735
Iteration 757, loss = 0.04146474
Iteration 758, loss = 0.03815079
Iteration 759, loss = 0.04566757
Iteration 760, loss = 0.04308792
Iteration 761, loss = 0.05018150
Iteration 762, loss = 0.04581936
Iteration 763, loss = 0.04014682
Iteration 764, loss = 0.04010902
Iteration 765, loss = 0.04267328
Iteration 766, loss = 0.04001935
Iteration 767, loss = 0.04144695
Iteration 768, loss = 0.04274885
Iteration 769, loss = 0.04159911
Iteration 770, loss = 0.04229896
Iteration 771, loss = 0.03946054
Iteration 772, loss = 0.03909739
Iteration 773, loss = 0.04342356
Iteration 774, loss = 0.04601253
Iteration 775, loss = 0.04340157
Iteration 776, loss = 0.03978861
Iteration 777, loss = 0.04572622
Iteration 778, loss = 0.04574202
Iteration 779, loss = 0.05656916
Iteration 780, loss = 0.05507835
Iteration 781, loss = 0.05131902
Iteration 782, loss = 0.04748145
Iteration 783, loss = 0.04407008
Iteration 784, loss = 0.04074589
Iteration 785, loss = 0.05169379
Iteration 786, loss = 0.04794095
Iteration 787, loss = 0.04220668
Iteration 788, loss = 0.03940194
Iteration 789, loss = 0.03872648
Iteration 790, loss = 0.03808975
Iteration 791, loss = 0.04066116
Iteration 792, loss = 0.04133145
Iteration 793, loss = 0.03841305
Iteration 794, loss = 0.03850008
Iteration 795, loss = 0.04035950
Iteration 796, loss = 0.04173159
Iteration 797, loss = 0.03850762
Iteration 798, loss = 0.04089879
Iteration 799, loss = 0.04349182
Iteration 800, loss = 0.04402064
Iteration 801, loss = 0.04252029
Iteration 802, loss = 0.04294347
Iteration 803, loss = 0.03849112
Iteration 804, loss = 0.04146947
Iteration 805, loss = 0.03859746
Iteration 806, loss = 0.04342242
Iteration 807, loss = 0.03958569
Iteration 808, loss = 0.03779449
Iteration 809, loss = 2.04083290
Iteration 810, loss = 1.57644310
Iteration 811, loss = 0.66225606
Iteration 812, loss = 0.54965399
Iteration 813, loss = 0.40969533
Iteration 814, loss = 0.28296202
Iteration 815, loss = 0.20685795
Iteration 816, loss = 0.15990111
Iteration 817, loss = 0.11412872
Iteration 818, loss = 0.07759080
Iteration 819, loss = 0.06248309
Iteration 820, loss = 0.06494882
Iteration 821, loss = 0.06858660
Iteration 822, loss = 0.06037552
Iteration 823, loss = 0.05490592
Iteration 824, loss = 0.05059972
Iteration 825, loss = 0.04894822
Iteration 826, loss = 0.04743809
Iteration 827, loss = 0.04801699
Iteration 828, loss = 0.04567528
Iteration 829, loss = 0.04409532
Iteration 830, loss = 0.04321906
Iteration 831, loss = 0.04437884
Iteration 832, loss = 0.04296061
Iteration 833, loss = 0.04303513
Iteration 834, loss = 0.04284026
Iteration 835, loss = 0.04271875
Iteration 836, loss = 0.04252870
Iteration 837, loss = 0.04260672
Iteration 838, loss = 0.04347430
Iteration 839, loss = 0.04266211
Iteration 840, loss = 0.04269420
Iteration 841, loss = 0.04226489
Iteration 842, loss = 0.04229096
Iteration 843, loss = 0.04250772
Iteration 844, loss = 0.04207655
Iteration 845, loss = 0.04189516
Iteration 846, loss = 0.04183037
Iteration 847, loss = 0.04175311
Iteration 848, loss = 0.04247503
Iteration 849, loss = 0.04201062
Iteration 850, loss = 0.04209335
Iteration 851, loss = 0.04212242
Iteration 852, loss = 0.04137563
Iteration 853, loss = 0.04200170
Iteration 854, loss = 0.04200679
Iteration 855, loss = 0.04254281
Iteration 856, loss = 0.04261516
Iteration 857, loss = 0.04205453
Iteration 858, loss = 0.04224103
Iteration 859, loss = 0.04208102
Iteration 860, loss = 0.04158737
Iteration 861, loss = 0.04138455
Iteration 862, loss = 0.04135617
Iteration 863, loss = 0.04121341
Iteration 864, loss = 0.04252747
Iteration 865, loss = 0.04133974
Iteration 866, loss = 0.04081906
Iteration 867, loss = 0.04134207
Iteration 868, loss = 0.04203625
Iteration 869, loss = 0.04289855
Iteration 870, loss = 0.04222515
Iteration 871, loss = 0.04170186
Iteration 872, loss = 0.04107358
Iteration 873, loss = 0.04177492
Iteration 874, loss = 0.04342892
Iteration 875, loss = 0.04184526
Iteration 876, loss = 0.04132245
Iteration 877, loss = 0.04074424
Iteration 878, loss = 0.04180393
Iteration 879, loss = 0.04079838
Iteration 880, loss = 0.04115787
Iteration 881, loss = 0.04062447
Iteration 882, loss = 0.04051684
Iteration 883, loss = 0.04070302
Iteration 884, loss = 0.04053872
Iteration 885, loss = 0.04047195
Iteration 886, loss = 0.04047071
Iteration 887, loss = 0.04046322
Iteration 888, loss = 0.04050891
Iteration 889, loss = 0.04049548
Iteration 890, loss = 0.04052697
Iteration 891, loss = 0.04047626
Iteration 892, loss = 0.04162946
Iteration 893, loss = 0.04309136
Iteration 894, loss = 0.04268702
Iteration 895, loss = 0.04154949
Iteration 896, loss = 0.04199657
Iteration 897, loss = 0.04126030
Iteration 898, loss = 0.04063737
Iteration 899, loss = 0.04090853
Iteration 900, loss = 0.04056211
Iteration 901, loss = 0.04158170
Iteration 902, loss = 0.04019952
Iteration 903, loss = 0.04029627
Iteration 904, loss = 0.04012099
Iteration 905, loss = 0.04010418
Iteration 906, loss = 0.04021385
Iteration 907, loss = 0.04044403
Iteration 908, loss = 0.04002852
Iteration 909, loss = 0.03996789
Iteration 910, loss = 0.04124179
Iteration 911, loss = 0.04016086
Iteration 912, loss = 0.04022785
Iteration 913, loss = 0.04067185
Iteration 914, loss = 0.04049384
Iteration 915, loss = 0.04039712
Iteration 916, loss = 0.04156284
Iteration 917, loss = 0.04003309
Iteration 918, loss = 0.04053377
Iteration 919, loss = 0.04013387
Iteration 920, loss = 0.04007938
Iteration 921, loss = 0.03982144
Iteration 922, loss = 0.04011440
Iteration 923, loss = 0.04082568
Iteration 924, loss = 0.03973807
Iteration 925, loss = 0.03971886
Iteration 926, loss = 0.04021522
Iteration 927, loss = 0.04010183
Iteration 928, loss = 0.04055769
Iteration 929, loss = 0.03970927
Iteration 930, loss = 0.04012448
Iteration 931, loss = 0.04010726
Iteration 932, loss = 0.04095851
Iteration 933, loss = 0.04015551
Iteration 934, loss = 0.03989616
Iteration 935, loss = 0.03951492
Iteration 936, loss = 0.04027778
Iteration 937, loss = 0.04153453
Iteration 938, loss = 0.04033917
Iteration 939, loss = 0.03964592
Iteration 940, loss = 0.04027369
Iteration 941, loss = 0.04043677
Iteration 942, loss = 0.04061681
Iteration 943, loss = 0.03960285
Iteration 944, loss = 0.03995814
Iteration 945, loss = 0.04248130
Iteration 946, loss = 0.04131106
Iteration 947, loss = 0.03976847
Iteration 948, loss = 0.03944352
Iteration 949, loss = 0.03958027
Iteration 950, loss = 0.03957068
Iteration 951, loss = 0.03939155
Iteration 952, loss = 0.03969406
Iteration 953, loss = 0.04097945
Iteration 954, loss = 0.04082538
Iteration 955, loss = 0.04036470
Iteration 956, loss = 0.04008158
Iteration 957, loss = 0.04005701
Iteration 958, loss = 0.03934399
Iteration 959, loss = 0.03974437
Iteration 960, loss = 0.04053634
Iteration 961, loss = 0.04074839
Iteration 962, loss = 0.04255313
Iteration 963, loss = 0.04103142
Iteration 964, loss = 0.03964617
Iteration 965, loss = 0.04049927
Iteration 966, loss = 0.04086017
Iteration 967, loss = 0.04052646
Iteration 968, loss = 0.04051075
Iteration 969, loss = 0.04144729
Iteration 970, loss = 0.04258284
Iteration 971, loss = 0.03996709
Iteration 972, loss = 0.03911956
Iteration 973, loss = 0.04076980
Iteration 974, loss = 0.03921265
Iteration 975, loss = 0.03951711
Iteration 976, loss = 0.03920884
Iteration 977, loss = 0.03928080
Iteration 978, loss = 0.03955934
Iteration 979, loss = 0.04034840
Iteration 980, loss = 0.03938719
Iteration 981, loss = 0.03919883
Iteration 982, loss = 0.03937807
Iteration 983, loss = 0.03964724
Iteration 984, loss = 0.03935823
Iteration 985, loss = 0.03944804
Iteration 986, loss = 0.03945411
Iteration 987, loss = 0.03930507
Iteration 988, loss = 0.03919899
Iteration 989, loss = 0.03926337
Iteration 990, loss = 0.03956444
Iteration 991, loss = 0.03987575
Iteration 992, loss = 0.03942729
Iteration 993, loss = 0.04001108
Iteration 994, loss = 0.03935369
Iteration 995, loss = 0.03938876
Iteration 996, loss = 0.03959571
Iteration 997, loss = 0.04075085
Iteration 998, loss = 0.04207501
Iteration 999, loss = 0.04003343
Iteration 1000, loss = 0.04147720
Run 2
Iteration 1, loss = 1650.58892438
Iteration 2, loss = 801.80928627
Iteration 3, loss = 79.75468859
Iteration 4, loss = 30.75563558
Iteration 5, loss = 33.43513024
Iteration 6, loss = 2.43805127
Iteration 7, loss = 11.67650711
Iteration 8, loss = 2.07242799
Iteration 9, loss = 3.97039067
Iteration 10, loss = 0.62401022
Iteration 11, loss = 0.86315478
Iteration 12, loss = 0.38175019
Iteration 13, loss = 0.43787567
Iteration 14, loss = 0.18136630
Iteration 15, loss = 0.58910388
Iteration 16, loss = 0.45376933
Iteration 17, loss = 0.20395724
Iteration 18, loss = 0.15165972
Iteration 19, loss = 0.08719740
Iteration 20, loss = 0.07733699
Iteration 21, loss = 0.07272299
Iteration 22, loss = 0.06891938
Iteration 23, loss = 0.07074322
Iteration 24, loss = 0.06142866
Iteration 25, loss = 0.05652783
Iteration 26, loss = 0.05883021
Iteration 27, loss = 0.05501754
Iteration 28, loss = 0.05039347
Iteration 29, loss = 0.04681277
Iteration 30, loss = 0.04905236
Iteration 31, loss = 0.04786208
Iteration 32, loss = 0.04890967
Iteration 33, loss = 0.04788546
Iteration 34, loss = 0.04359374
Iteration 35, loss = 0.04152457
Iteration 36, loss = 0.04198407
Iteration 37, loss = 0.04101289
Iteration 38, loss = 0.04090113
Iteration 39, loss = 0.04009825
Iteration 40, loss = 0.03962221
Iteration 41, loss = 0.03962214
Iteration 42, loss = 0.03884010
Iteration 43, loss = 0.03877275
Iteration 44, loss = 0.03839795
Iteration 45, loss = 0.03883990
Iteration 46, loss = 0.03790740
Iteration 47, loss = 0.03777310
Iteration 48, loss = 0.03743914
Iteration 49, loss = 0.03897239
Iteration 50, loss = 0.03734867
Iteration 51, loss = 0.03763500
Iteration 52, loss = 0.03751191
Iteration 53, loss = 0.03656857
Iteration 54, loss = 0.03645365
Iteration 55, loss = 0.03702352
Iteration 56, loss = 0.03883735
Iteration 57, loss = 0.04060624
Iteration 58, loss = 0.03936767
Iteration 59, loss = 0.03618852
Iteration 60, loss = 0.03747742
Iteration 61, loss = 0.03637442
Iteration 62, loss = 0.03665316
Iteration 63, loss = 0.03837389
Iteration 64, loss = 0.03523455
Iteration 65, loss = 0.03609558
Iteration 66, loss = 0.03646683
Iteration 67, loss = 0.03510413
Iteration 68, loss = 0.03530683
Iteration 69, loss = 0.03625368
Iteration 70, loss = 0.03502092
Iteration 71, loss = 0.03862818
Iteration 72, loss = 0.04093816
Iteration 73, loss = 0.03734041
Iteration 74, loss = 0.03918959
Iteration 75, loss = 0.04238828
Iteration 76, loss = 0.04008673
Iteration 77, loss = 0.03804684
Iteration 78, loss = 0.03626957
Iteration 79, loss = 0.03650247
Iteration 80, loss = 0.03660084
Iteration 81, loss = 0.03574999
Iteration 82, loss = 0.04035922
Iteration 83, loss = 0.03668856
Iteration 84, loss = 0.03769649
Iteration 85, loss = 0.03664516
Iteration 86, loss = 0.03960765
Iteration 87, loss = 0.03832118
Iteration 88, loss = 0.03562599
Iteration 89, loss = 0.03429611
Iteration 90, loss = 0.03491795
Iteration 91, loss = 0.03447007
Iteration 92, loss = 0.03447412
Iteration 93, loss = 0.03689271
Iteration 94, loss = 0.03791023
Iteration 95, loss = 0.03737792
Iteration 96, loss = 0.03575615
Iteration 97, loss = 0.03457066
Iteration 98, loss = 0.03347287
Iteration 99, loss = 0.03518257
Iteration 100, loss = 0.03684349
Iteration 101, loss = 0.03345405
Iteration 102, loss = 0.03384318
Iteration 103, loss = 0.03363798
Iteration 104, loss = 0.03400338
Iteration 105, loss = 0.03378149
Iteration 106, loss = 0.03422192
Iteration 107, loss = 0.03577819
Iteration 108, loss = 0.03352335
Iteration 109, loss = 0.03446310
Iteration 110, loss = 0.03556698
Iteration 111, loss = 0.03351747
Iteration 112, loss = 0.03588665
Iteration 113, loss = 0.03960445
Iteration 114, loss = 0.03534800
Iteration 115, loss = 0.03544567
Iteration 116, loss = 0.03636400
Iteration 117, loss = 0.03660002
Iteration 118, loss = 0.04108644
Iteration 119, loss = 0.03669298
Iteration 120, loss = 0.03943749
Iteration 121, loss = 0.03508871
Iteration 122, loss = 0.03820031
Iteration 123, loss = 0.03782124
Iteration 124, loss = 0.03727860
Iteration 125, loss = 0.03366765
Iteration 126, loss = 0.03454644
Iteration 127, loss = 0.03407087
Iteration 128, loss = 0.03794639
Iteration 129, loss = 0.03616605
Iteration 130, loss = 0.03971212
Iteration 131, loss = 0.03491814
Iteration 132, loss = 0.03290049
Iteration 133, loss = 0.03490394
Iteration 134, loss = 0.03401051
Iteration 135, loss = 0.03345968
Iteration 136, loss = 0.03343771
Iteration 137, loss = 0.03403714
Iteration 138, loss = 0.03350624
Iteration 139, loss = 0.03307889
Iteration 140, loss = 0.03283915
Iteration 141, loss = 0.03323672
Iteration 142, loss = 0.03303702
Iteration 143, loss = 0.03258756
Iteration 144, loss = 0.03279519
Iteration 145, loss = 0.03269144
Iteration 146, loss = 0.03377756
Iteration 147, loss = 0.03499934
Iteration 148, loss = 0.03534856
Iteration 149, loss = 0.03435143
Iteration 150, loss = 0.03689188
Iteration 151, loss = 0.03823663
Iteration 152, loss = 0.03813441
Iteration 153, loss = 0.03760888
Iteration 154, loss = 0.03791827
Iteration 155, loss = 0.03747225
Iteration 156, loss = 0.04132033
Iteration 157, loss = 0.03785854
Iteration 158, loss = 0.03283252
Iteration 159, loss = 0.03606236
Iteration 160, loss = 0.03621792
Iteration 161, loss = 0.03481247
Iteration 162, loss = 0.03439130
Iteration 163, loss = 0.03557075
Iteration 164, loss = 0.03559358
Iteration 165, loss = 0.03489610
Iteration 166, loss = 0.03554688
Iteration 167, loss = 0.03229894
Iteration 168, loss = 0.03248107
Iteration 169, loss = 0.03229437
Iteration 170, loss = 0.03296663
Iteration 171, loss = 0.03274537
Iteration 172, loss = 0.03210652
Iteration 173, loss = 0.03412929
Iteration 174, loss = 0.03369753
Iteration 175, loss = 0.03721481
Iteration 176, loss = 0.03477072
Iteration 177, loss = 0.03478830
Iteration 178, loss = 0.03440618
Iteration 179, loss = 0.03226838
Iteration 180, loss = 0.03223308
Iteration 181, loss = 0.03212546
Iteration 182, loss = 0.03163347
Iteration 183, loss = 0.03225106
Iteration 184, loss = 0.03201347
Iteration 185, loss = 0.03167767
Iteration 186, loss = 0.03138685
Iteration 187, loss = 0.03184440
Iteration 188, loss = 0.03179432
Iteration 189, loss = 0.03300593
Iteration 190, loss = 0.03338211
Iteration 191, loss = 0.03698829
Iteration 192, loss = 0.03884777
Iteration 193, loss = 0.03408029
Iteration 194, loss = 0.03316834
Iteration 195, loss = 0.03437748
Iteration 196, loss = 0.04365734
Iteration 197, loss = 0.04136593
Iteration 198, loss = 0.03619701
Iteration 199, loss = 0.03507087
Iteration 200, loss = 0.03520868
Iteration 201, loss = 0.03245527
Iteration 202, loss = 0.03174092
Iteration 203, loss = 0.03238009
Iteration 204, loss = 0.03096080
Iteration 205, loss = 0.03171901
Iteration 206, loss = 0.03159757
Iteration 207, loss = 0.03097844
Iteration 208, loss = 0.03127286
Iteration 209, loss = 0.03117208
Iteration 210, loss = 0.03096886
Iteration 211, loss = 0.03045293
Iteration 212, loss = 0.03205744
Iteration 213, loss = 0.03169156
Iteration 214, loss = 0.03071430
Iteration 215, loss = 0.03049849
Iteration 216, loss = 0.03076579
Iteration 217, loss = 0.03063512
Iteration 218, loss = 0.03072661
Iteration 219, loss = 0.03074042
Iteration 220, loss = 0.03027760
Iteration 221, loss = 0.03032374
Iteration 222, loss = 0.03026673
Iteration 223, loss = 0.03045081
Iteration 224, loss = 0.03070839
Iteration 225, loss = 0.03026700
Iteration 226, loss = 0.03059692
Iteration 227, loss = 0.03028519
Iteration 228, loss = 0.03402355
Iteration 229, loss = 0.03111755
Iteration 230, loss = 0.03179117
Iteration 231, loss = 0.03200031
Iteration 232, loss = 0.03057690
Iteration 233, loss = 0.03118325
Iteration 234, loss = 0.03182714
Iteration 235, loss = 0.03132074
Iteration 236, loss = 0.03225875
Iteration 237, loss = 0.03061599
Iteration 238, loss = 0.03105845
Iteration 239, loss = 0.03125650
Iteration 240, loss = 0.03059442
Iteration 241, loss = 0.03020179
Iteration 242, loss = 0.03006287
Iteration 243, loss = 0.03012779
Iteration 244, loss = 0.02994936
Iteration 245, loss = 0.03121394
Iteration 246, loss = 0.03198312
Iteration 247, loss = 0.03018382
Iteration 248, loss = 0.03082021
Iteration 249, loss = 0.03073990
Iteration 250, loss = 0.03126529
Iteration 251, loss = 0.03041927
Iteration 252, loss = 0.03033625
Iteration 253, loss = 0.03105132
Iteration 254, loss = 0.03096251
Iteration 255, loss = 0.03040293
Iteration 256, loss = 0.03113342
Iteration 257, loss = 0.03074062
Iteration 258, loss = 0.03116897
Iteration 259, loss = 0.03519432
Iteration 260, loss = 0.03525604
Iteration 261, loss = 0.03208716
Iteration 262, loss = 0.03240077
Iteration 263, loss = 0.03295912
Iteration 264, loss = 0.03407470
Iteration 265, loss = 0.03560669
Iteration 266, loss = 0.03658023
Iteration 267, loss = 0.03377542
Iteration 268, loss = 0.03430064
Iteration 269, loss = 0.03734067
Iteration 270, loss = 0.03414411
Iteration 271, loss = 0.03154613
Iteration 272, loss = 0.03404308
Iteration 273, loss = 0.03569060
Iteration 274, loss = 0.03582706
Iteration 275, loss = 0.03064485
Iteration 276, loss = 0.03173884
Iteration 277, loss = 0.03535871
Iteration 278, loss = 0.03565749
Iteration 279, loss = 0.03489749
Iteration 280, loss = 0.03503918
Iteration 281, loss = 0.03156625
Iteration 282, loss = 0.03559339
Iteration 283, loss = 0.03387224
Iteration 284, loss = 0.03467217
Iteration 285, loss = 0.03329431
Iteration 286, loss = 0.03094364
Iteration 287, loss = 0.03404772
Iteration 288, loss = 0.03128062
Iteration 289, loss = 0.03253572
Iteration 290, loss = 0.03400339
Iteration 291, loss = 0.03415598
Iteration 292, loss = 0.03387718
Iteration 293, loss = 0.03271062
Iteration 294, loss = 0.03281309
Iteration 295, loss = 0.03349495
Iteration 296, loss = 0.03424842
Iteration 297, loss = 0.03244767
Iteration 298, loss = 0.03010396
Iteration 299, loss = 0.03086972
Iteration 300, loss = 0.03248563
Iteration 301, loss = 0.03402987
Iteration 302, loss = 0.03261389
Iteration 303, loss = 0.03227225
Iteration 304, loss = 0.03123741
Iteration 305, loss = 0.03042880
Iteration 306, loss = 0.02994185
Iteration 307, loss = 0.03058187
Iteration 308, loss = 0.02959861
Iteration 309, loss = 0.02992115
Iteration 310, loss = 0.02998074
Iteration 311, loss = 0.03095249
Iteration 312, loss = 0.03232787
Iteration 313, loss = 0.03003109
Iteration 314, loss = 0.03008816
Iteration 315, loss = 0.03034299
Iteration 316, loss = 0.03165250
Iteration 317, loss = 0.03075242
Iteration 318, loss = 0.03015286
Iteration 319, loss = 0.03014912
Iteration 320, loss = 0.02973666
Iteration 321, loss = 0.03061778
Iteration 322, loss = 0.03035490
Iteration 323, loss = 0.03000397
Iteration 324, loss = 0.02950762
Iteration 325, loss = 0.03047212
Iteration 326, loss = 0.03094191
Iteration 327, loss = 0.02985334
Iteration 328, loss = 0.02989141
Iteration 329, loss = 0.03008088
Iteration 330, loss = 0.03024708
Iteration 331, loss = 0.02975313
Iteration 332, loss = 0.03053069
Iteration 333, loss = 0.03105315
Iteration 334, loss = 0.03087591
Iteration 335, loss = 0.02988019
Iteration 336, loss = 0.03067547
Iteration 337, loss = 0.03186675
Iteration 338, loss = 0.03365365
Iteration 339, loss = 0.03162917
Iteration 340, loss = 0.02943753
Iteration 341, loss = 0.03070388
Iteration 342, loss = 0.03298262
Iteration 343, loss = 0.03206718
Iteration 344, loss = 0.03022740
Iteration 345, loss = 0.02987918
Iteration 346, loss = 0.03001064
Iteration 347, loss = 0.03013343
Iteration 348, loss = 0.02965284
Iteration 349, loss = 0.03254020
Iteration 350, loss = 0.03010826
Iteration 351, loss = 0.03085725
Iteration 352, loss = 0.03310986
Iteration 353, loss = 0.03403526
Iteration 354, loss = 0.03104105
Iteration 355, loss = 0.03105644
Iteration 356, loss = 0.03406057
Iteration 357, loss = 0.03079976
Iteration 358, loss = 0.03031811
Iteration 359, loss = 0.03099877
Iteration 360, loss = 0.03147119
Iteration 361, loss = 0.03216902
Iteration 362, loss = 0.03254785
Iteration 363, loss = 0.03612835
Iteration 364, loss = 0.03627957
Iteration 365, loss = 0.03362027
Iteration 366, loss = 0.03735270
Iteration 367, loss = 0.03469048
Iteration 368, loss = 0.03956927
Iteration 369, loss = 0.03674609
Iteration 370, loss = 0.03440180
Iteration 371, loss = 0.03569535
Iteration 372, loss = 0.03676024
Iteration 373, loss = 0.03455073
Iteration 374, loss = 0.03270168
Iteration 375, loss = 0.03064460
Iteration 376, loss = 0.03135150
Iteration 377, loss = 0.02993276
Iteration 378, loss = 0.03004896
Iteration 379, loss = 0.02996620
Iteration 380, loss = 0.03058704
Iteration 381, loss = 0.03049628
Iteration 382, loss = 0.03217115
Iteration 383, loss = 0.03043215
Iteration 384, loss = 0.03052520
Iteration 385, loss = 0.03115817
Iteration 386, loss = 0.03021145
Iteration 387, loss = 0.03001970
Iteration 388, loss = 0.03145374
Iteration 389, loss = 0.03085398
Iteration 390, loss = 0.02985177
Iteration 391, loss = 0.03162974
Iteration 392, loss = 0.03492347
Iteration 393, loss = 0.03633254
Iteration 394, loss = 0.03640413
Iteration 395, loss = 0.03588825
Iteration 396, loss = 0.03308410
Iteration 397, loss = 0.03506223
Iteration 398, loss = 0.03494358
Iteration 399, loss = 0.03377603
Iteration 400, loss = 0.03085918
Iteration 401, loss = 0.03075848
Iteration 402, loss = 0.03261788
Iteration 403, loss = 0.03170728
Iteration 404, loss = 0.03248398
Iteration 405, loss = 0.02970600
Iteration 406, loss = 0.02959564
Iteration 407, loss = 0.03061303
Iteration 408, loss = 0.03074502
Iteration 409, loss = 0.03155790
Iteration 410, loss = 0.03304778
Iteration 411, loss = 0.03175792
Iteration 412, loss = 0.03036822
Iteration 413, loss = 0.02965229
Iteration 414, loss = 0.02969344
Iteration 415, loss = 0.03038435
Iteration 416, loss = 0.02952609
Iteration 417, loss = 0.03094527
Iteration 418, loss = 0.03104589
Iteration 419, loss = 0.02996939
Iteration 420, loss = 0.02953362
Iteration 421, loss = 0.02944698
Iteration 422, loss = 0.02945352
Iteration 423, loss = 0.02981100
Iteration 424, loss = 0.02925091
Iteration 425, loss = 0.03053817
Iteration 426, loss = 0.03105507
Iteration 427, loss = 0.03292782
Iteration 428, loss = 0.03095652
Iteration 429, loss = 0.03116580
Iteration 430, loss = 0.02972347
Iteration 431, loss = 0.03084644
Iteration 432, loss = 0.02968919
Iteration 433, loss = 0.02977390
Iteration 434, loss = 0.02958137
Iteration 435, loss = 0.02997595
Iteration 436, loss = 0.03308617
Iteration 437, loss = 0.03297652
Iteration 438, loss = 0.03393513
Iteration 439, loss = 0.03129142
Iteration 440, loss = 0.03272736
Iteration 441, loss = 0.03186247
Iteration 442, loss = 0.03032564
Iteration 443, loss = 0.02977665
Iteration 444, loss = 0.03014246
Iteration 445, loss = 0.02948576
Iteration 446, loss = 0.03052884
Iteration 447, loss = 0.03141252
Iteration 448, loss = 0.03091519
Iteration 449, loss = 0.03227184
Iteration 450, loss = 0.03255592
Iteration 451, loss = 0.03328263
Iteration 452, loss = 0.02989211
Iteration 453, loss = 0.02952426
Iteration 454, loss = 0.02983652
Iteration 455, loss = 0.03028599
Iteration 456, loss = 0.02998746
Iteration 457, loss = 0.03034377
Iteration 458, loss = 0.03087369
Iteration 459, loss = 0.03079715
Iteration 460, loss = 0.03098414
Iteration 461, loss = 0.03193531
Iteration 462, loss = 0.03317161
Iteration 463, loss = 0.03104831
Iteration 464, loss = 0.03196029
Iteration 465, loss = 0.03073450
Iteration 466, loss = 0.03004298
Iteration 467, loss = 0.03031650
Iteration 468, loss = 0.03726005
Iteration 469, loss = 0.03409445
Iteration 470, loss = 0.03107275
Iteration 471, loss = 0.03043864
Iteration 472, loss = 0.03016239
Iteration 473, loss = 0.03037728
Iteration 474, loss = 0.03162613
Iteration 475, loss = 0.03374059
Iteration 476, loss = 0.03344937
Iteration 477, loss = 0.03535545
Iteration 478, loss = 0.03334303
Iteration 479, loss = 0.02943394
Iteration 480, loss = 0.03034217
Iteration 481, loss = 0.02974576
Iteration 482, loss = 0.03116723
Iteration 483, loss = 0.03317958
Iteration 484, loss = 0.03322270
Iteration 485, loss = 0.03434309
Iteration 486, loss = 0.03437531
Iteration 487, loss = 0.03284233
Iteration 488, loss = 0.03307505
Iteration 489, loss = 0.03239187
Iteration 490, loss = 0.03143199
Iteration 491, loss = 0.03019922
Iteration 492, loss = 0.03130313
Iteration 493, loss = 0.03255204
Iteration 494, loss = 0.03286754
Iteration 495, loss = 0.03300631
Iteration 496, loss = 0.02947933
Iteration 497, loss = 0.02909158
Iteration 498, loss = 0.03100720
Iteration 499, loss = 0.03235546
Iteration 500, loss = 0.03327224
Iteration 501, loss = 0.03214801
Iteration 502, loss = 0.03386498
Iteration 503, loss = 0.03201410
Iteration 504, loss = 0.02977867
Iteration 505, loss = 0.02926148
Iteration 506, loss = 0.03032007
Iteration 507, loss = 0.03071028
Iteration 508, loss = 0.03083267
Iteration 509, loss = 0.03251016
Iteration 510, loss = 0.03089868
Iteration 511, loss = 0.03074020
Iteration 512, loss = 0.02974416
Iteration 513, loss = 0.02989835
Iteration 514, loss = 0.02933279
Iteration 515, loss = 0.02985925
Iteration 516, loss = 0.02937807
Iteration 517, loss = 0.02960071
Iteration 518, loss = 0.02978664
Iteration 519, loss = 0.02921194
Iteration 520, loss = 0.02924889
Iteration 521, loss = 0.02935354
Iteration 522, loss = 0.02945845
Iteration 523, loss = 0.02948009
Iteration 524, loss = 0.02993364
Iteration 525, loss = 0.03018048
Iteration 526, loss = 0.03166720
Iteration 527, loss = 0.03137423
Iteration 528, loss = 0.02972377
Iteration 529, loss = 0.03036091
Iteration 530, loss = 0.03017854
Iteration 531, loss = 0.03238602
Iteration 532, loss = 0.03096236
Iteration 533, loss = 0.03024661
Iteration 534, loss = 0.03087806
Iteration 535, loss = 0.02955820
Iteration 536, loss = 0.03022381
Iteration 537, loss = 0.03190830
Iteration 538, loss = 0.02974198
Iteration 539, loss = 0.03051159
Iteration 540, loss = 0.03012099
Iteration 541, loss = 0.02935828
Iteration 542, loss = 0.02982064
Iteration 543, loss = 0.03273831
Iteration 544, loss = 0.03149501
Iteration 545, loss = 0.03068917
Iteration 546, loss = 0.02928001
Iteration 547, loss = 0.03017026
Iteration 548, loss = 0.02969555
Iteration 549, loss = 0.03073057
Iteration 550, loss = 0.03350490
Iteration 551, loss = 0.03131742
Iteration 552, loss = 0.03168052
Iteration 553, loss = 0.03354639
Iteration 554, loss = 0.03148058
Iteration 555, loss = 0.03572806
Iteration 556, loss = 0.03340076
Iteration 557, loss = 0.03222365
Iteration 558, loss = 0.02982874
Iteration 559, loss = 0.02964047
Iteration 560, loss = 0.03205436
Iteration 561, loss = 0.03206676
Iteration 562, loss = 0.03118968
Iteration 563, loss = 0.03223876
Iteration 564, loss = 0.03226144
Iteration 565, loss = 0.03580235
Iteration 566, loss = 0.03407404
Iteration 567, loss = 0.03425396
Iteration 568, loss = 0.03703502
Iteration 569, loss = 0.03796777
Iteration 570, loss = 0.03487240
Iteration 571, loss = 0.03080769
Iteration 572, loss = 0.03109615
Iteration 573, loss = 0.02962204
Iteration 574, loss = 0.02982402
Iteration 575, loss = 0.02945743
Iteration 576, loss = 0.03241402
Iteration 577, loss = 0.03179720
Iteration 578, loss = 0.03084201
Iteration 579, loss = 0.03329413
Iteration 580, loss = 0.03165192
Iteration 581, loss = 0.03376887
Iteration 582, loss = 0.03086785
Iteration 583, loss = 0.03154204
Iteration 584, loss = 0.03059593
Iteration 585, loss = 0.02960079
Iteration 586, loss = 0.02941950
Iteration 587, loss = 0.02990111
Iteration 588, loss = 0.02927242
Iteration 589, loss = 0.02915771
Iteration 590, loss = 0.02948090
Iteration 591, loss = 0.02948791
Iteration 592, loss = 0.02955682
Iteration 593, loss = 0.03003327
Iteration 594, loss = 0.03267564
Iteration 595, loss = 0.03651050
Iteration 596, loss = 0.03643726
Iteration 597, loss = 0.03388705
Iteration 598, loss = 0.03107590
Iteration 599, loss = 0.03037198
Iteration 600, loss = 0.03021621
Iteration 601, loss = 0.03022048
Iteration 602, loss = 0.02906319
Iteration 603, loss = 0.02978269
Iteration 604, loss = 0.03016822
Iteration 605, loss = 0.02964270
Iteration 606, loss = 0.02922490
Iteration 607, loss = 0.02942512
Iteration 608, loss = 0.02901952
Iteration 609, loss = 0.02913177
Iteration 610, loss = 0.02934882
Iteration 611, loss = 0.02971122
Iteration 612, loss = 0.03040642
Iteration 613, loss = 0.03022405
Iteration 614, loss = 0.02985649
Iteration 615, loss = 0.02963932
Iteration 616, loss = 0.03019508
Iteration 617, loss = 0.03036670
Iteration 618, loss = 0.03128209
Iteration 619, loss = 0.02973118
Iteration 620, loss = 0.03254624
Iteration 621, loss = 0.03092198
Iteration 622, loss = 0.02980420
Iteration 623, loss = 0.03085503
Iteration 624, loss = 0.03137117
Iteration 625, loss = 0.03436117
Iteration 626, loss = 0.03171145
Iteration 627, loss = 0.03433516
Iteration 628, loss = 0.03075496
Iteration 629, loss = 0.03070573
Iteration 630, loss = 0.03192406
Iteration 631, loss = 0.02910574
Iteration 632, loss = 0.03264385
Iteration 633, loss = 0.03401334
Iteration 634, loss = 0.03269396
Iteration 635, loss = 0.02938615
Iteration 636, loss = 0.02914875
Iteration 637, loss = 0.03202370
Iteration 638, loss = 0.02956048
Iteration 639, loss = 0.02887665
Iteration 640, loss = 0.02939092
Iteration 641, loss = 0.03025018
Iteration 642, loss = 0.03069444
Iteration 643, loss = 0.03044088
Iteration 644, loss = 0.02927136
Iteration 645, loss = 0.02939965
Iteration 646, loss = 0.03013900
Iteration 647, loss = 0.02895053
Iteration 648, loss = 0.02926726
Iteration 649, loss = 0.02914461
Iteration 650, loss = 0.02983491
Iteration 651, loss = 0.03317438
Iteration 652, loss = 0.03131983
Iteration 653, loss = 0.03671461
Iteration 654, loss = 0.03127092
Iteration 655, loss = 0.03449960
Iteration 656, loss = 0.03212331
Iteration 657, loss = 0.03054381
Iteration 658, loss = 0.03268569
Iteration 659, loss = 0.03194389
Iteration 660, loss = 0.03240758
Iteration 661, loss = 0.03130179
Iteration 662, loss = 0.03462575
Iteration 663, loss = 0.03234092
Iteration 664, loss = 0.03267271
Iteration 665, loss = 0.03223184
Iteration 666, loss = 0.03261731
Iteration 667, loss = 0.03193048
Iteration 668, loss = 0.03140785
Iteration 669, loss = 0.03015145
Iteration 670, loss = 0.03110743
Iteration 671, loss = 0.03186916
Iteration 672, loss = 0.03146360
Iteration 673, loss = 0.03500875
Iteration 674, loss = 0.03559734
Iteration 675, loss = 0.02979108
Iteration 676, loss = 0.03051183
Iteration 677, loss = 0.03296574
Iteration 678, loss = 0.03454096
Iteration 679, loss = 0.03413996
Iteration 680, loss = 0.03314145
Iteration 681, loss = 0.03377774
Iteration 682, loss = 0.03561066
Iteration 683, loss = 0.03220524
Iteration 684, loss = 0.03067462
Iteration 685, loss = 0.03027383
Iteration 686, loss = 0.03032061
Iteration 687, loss = 0.02951882
Iteration 688, loss = 0.02984271
Iteration 689, loss = 0.03267757
Iteration 690, loss = 0.03337602
Iteration 691, loss = 0.03130005
Iteration 692, loss = 0.03344848
Iteration 693, loss = 0.03055271
Iteration 694, loss = 0.03254761
Iteration 695, loss = 0.03131139
Iteration 696, loss = 0.03617523
Iteration 697, loss = 0.03326257
Iteration 698, loss = 0.03557417
Iteration 699, loss = 0.04165737
Iteration 700, loss = 0.03706176
Iteration 701, loss = 0.03394978
Iteration 702, loss = 0.02980903
Iteration 703, loss = 0.02979502
Iteration 704, loss = 0.02925605
Iteration 705, loss = 0.03098795
Iteration 706, loss = 0.02978574
Iteration 707, loss = 0.03091874
Iteration 708, loss = 0.03127965
Iteration 709, loss = 0.03094208
Iteration 710, loss = 0.02940732
Iteration 711, loss = 0.03357874
Iteration 712, loss = 0.03074122
Iteration 713, loss = 0.03482864
Iteration 714, loss = 0.03868712
Iteration 715, loss = 0.03723067
Iteration 716, loss = 0.03302869
Iteration 717, loss = 0.02920985
Iteration 718, loss = 0.03030530
Iteration 719, loss = 0.03029254
Iteration 720, loss = 0.03136577
Iteration 721, loss = 0.03244580
Iteration 722, loss = 0.03486218
Iteration 723, loss = 0.03157141
Iteration 724, loss = 0.03170338
Iteration 725, loss = 0.03041712
Iteration 726, loss = 0.02906659
Iteration 727, loss = 0.02957633
Iteration 728, loss = 0.02979260
Iteration 729, loss = 0.03137928
Iteration 730, loss = 0.03439404
Iteration 731, loss = 0.03152442
Iteration 732, loss = 0.03473618
Iteration 733, loss = 0.03529420
Iteration 734, loss = 0.04024013
Iteration 735, loss = 0.03388533
Iteration 736, loss = 0.03233470
Iteration 737, loss = 0.03139796
Iteration 738, loss = 0.02957765
Iteration 739, loss = 0.02921606
Iteration 740, loss = 0.02892059
Iteration 741, loss = 0.02980397
Iteration 742, loss = 0.02967853
Iteration 743, loss = 0.03011520
Iteration 744, loss = 0.03097204
Iteration 745, loss = 0.03281385
Iteration 746, loss = 0.03102065
Iteration 747, loss = 0.03240993
Iteration 748, loss = 0.03099082
Iteration 749, loss = 0.03313961
Iteration 750, loss = 0.02979461
Iteration 751, loss = 0.02970891
Iteration 752, loss = 0.02934231
Iteration 753, loss = 0.03001068
Iteration 754, loss = 0.03037457
Iteration 755, loss = 0.02941315
Iteration 756, loss = 0.02961542
Iteration 757, loss = 0.03054494
Iteration 758, loss = 0.03056423
Iteration 759, loss = 0.03351836
Iteration 760, loss = 0.03177141
Iteration 761, loss = 0.03414840
Iteration 762, loss = 0.03526474
Iteration 763, loss = 0.03268048
Iteration 764, loss = 0.02926983
Iteration 765, loss = 0.02963611
Iteration 766, loss = 0.03241299
Iteration 767, loss = 0.02995172
Iteration 768, loss = 0.02927362
Iteration 769, loss = 0.02938452
Iteration 770, loss = 0.03189498
Iteration 771, loss = 0.02975716
Iteration 772, loss = 0.03028344
Iteration 773, loss = 0.02986034
Iteration 774, loss = 0.03066666
Iteration 775, loss = 0.03080281
Iteration 776, loss = 0.02972913
Iteration 777, loss = 0.03643879
Iteration 778, loss = 0.03675104
Iteration 779, loss = 0.03726053
Iteration 780, loss = 0.03226074
Iteration 781, loss = 0.03370151
Iteration 782, loss = 0.03667605
Iteration 783, loss = 0.03455922
Iteration 784, loss = 0.03661706
Iteration 785, loss = 0.03902260
Iteration 786, loss = 0.04893523
Iteration 787, loss = 0.04410954
Iteration 788, loss = 0.05163262
Iteration 789, loss = 0.04412933
Iteration 790, loss = 0.03878085
Iteration 791, loss = 0.03600266
Iteration 792, loss = 0.04129441
Iteration 793, loss = 0.03666930
Iteration 794, loss = 0.03583992
Iteration 795, loss = 0.03475852
Iteration 796, loss = 0.04973661
Iteration 797, loss = 0.03502973
Iteration 798, loss = 0.03403891
Iteration 799, loss = 0.03306482
Iteration 800, loss = 0.03040718
Iteration 801, loss = 0.02952959
Iteration 802, loss = 0.02971895
Iteration 803, loss = 0.02994728
Iteration 804, loss = 0.02938896
Iteration 805, loss = 0.02973815
Iteration 806, loss = 0.03010522
Iteration 807, loss = 0.03029301
Iteration 808, loss = 0.03208135
Iteration 809, loss = 0.03212629
Iteration 810, loss = 0.03361962
Iteration 811, loss = 0.03214883
Iteration 812, loss = 0.02887790
Iteration 813, loss = 0.02979223
Iteration 814, loss = 0.03116094
Iteration 815, loss = 0.03003049
Iteration 816, loss = 0.03209224
Iteration 817, loss = 0.02961768
Iteration 818, loss = 0.03056864
Iteration 819, loss = 0.03175424
Iteration 820, loss = 0.03495327
Iteration 821, loss = 0.03453896
Iteration 822, loss = 0.03509829
Iteration 823, loss = 0.03230338
Iteration 824, loss = 0.03223387
Iteration 825, loss = 0.03495887
Iteration 826, loss = 0.02902957
Iteration 827, loss = 0.03182815
Iteration 828, loss = 0.03076263
Iteration 829, loss = 0.03164681
Iteration 830, loss = 0.03028700
Iteration 831, loss = 0.03273663
Iteration 832, loss = 0.02927128
Iteration 833, loss = 0.02984167
Iteration 834, loss = 0.03188565
Iteration 835, loss = 0.02975616
Iteration 836, loss = 0.03077226
Iteration 837, loss = 0.03273963
Iteration 838, loss = 0.03621124
Iteration 839, loss = 0.03651402
Iteration 840, loss = 0.03217246
Iteration 841, loss = 0.03299860
Iteration 842, loss = 0.03375459
Iteration 843, loss = 0.02981538
Iteration 844, loss = 0.03199178
Iteration 845, loss = 0.03463006
Iteration 846, loss = 0.03261505
Iteration 847, loss = 0.03316850
Iteration 848, loss = 0.03271114
Iteration 849, loss = 0.02989529
Iteration 850, loss = 0.03125729
Iteration 851, loss = 0.03009192
Iteration 852, loss = 0.03204911
Iteration 853, loss = 0.02990876
Iteration 854, loss = 0.03388938
Iteration 855, loss = 0.03215363
Iteration 856, loss = 0.03962181
Iteration 857, loss = 0.03497723
Iteration 858, loss = 0.03314971
Iteration 859, loss = 0.03441462
Iteration 860, loss = 0.03618496
Iteration 861, loss = 0.03459475
Iteration 862, loss = 0.03039235
Iteration 863, loss = 0.02937626
Iteration 864, loss = 0.03504486
Iteration 865, loss = 0.03696241
Iteration 866, loss = 0.03917127
Iteration 867, loss = 0.03411014
Iteration 868, loss = 0.03439409
Iteration 869, loss = 0.03175817
Iteration 870, loss = 0.03562567
Iteration 871, loss = 0.03129854
Iteration 872, loss = 0.02969245
Iteration 873, loss = 0.03246978
Iteration 874, loss = 0.03023449
Iteration 875, loss = 0.03087058
Iteration 876, loss = 0.03473034
Iteration 877, loss = 0.03559589
Iteration 878, loss = 0.03043880
Iteration 879, loss = 0.03288610
Iteration 880, loss = 0.03340218
Iteration 881, loss = 0.03034847
Iteration 882, loss = 0.03024751
Iteration 883, loss = 0.03225655
Iteration 884, loss = 0.03144545
Iteration 885, loss = 0.02902354
Iteration 886, loss = 0.02865417
Iteration 887, loss = 0.02946936
Iteration 888, loss = 0.02940490
Iteration 889, loss = 0.03025099
Iteration 890, loss = 0.03002326
Iteration 891, loss = 0.03066465
Iteration 892, loss = 0.03103290
Iteration 893, loss = 0.03192650
Iteration 894, loss = 0.03004495
Iteration 895, loss = 0.03000383
Iteration 896, loss = 0.03137329
Iteration 897, loss = 0.02920240
Iteration 898, loss = 0.03019565
Iteration 899, loss = 0.02946021
Iteration 900, loss = 0.03061701
Iteration 901, loss = 0.03000838
Iteration 902, loss = 0.03035310
Iteration 903, loss = 0.02862100
Iteration 904, loss = 0.02872072
Iteration 905, loss = 0.02884786
Iteration 906, loss = 0.03035658
Iteration 907, loss = 0.03010499
Iteration 908, loss = 0.02968528
Iteration 909, loss = 0.02869021
Iteration 910, loss = 0.02944172
Iteration 911, loss = 0.02999218
Iteration 912, loss = 0.03036013
Iteration 913, loss = 0.03066636
Iteration 914, loss = 0.03047210
Iteration 915, loss = 0.02993090
Iteration 916, loss = 0.02922714
Iteration 917, loss = 0.03168375
Iteration 918, loss = 0.03259221
Iteration 919, loss = 0.02916120
Iteration 920, loss = 0.02909321
Iteration 921, loss = 0.03072823
Iteration 922, loss = 0.03124416
Iteration 923, loss = 0.02963325
Iteration 924, loss = 0.02896756
Iteration 925, loss = 0.02845235
Iteration 926, loss = 0.02956177
Iteration 927, loss = 0.02936373
Iteration 928, loss = 0.02907608
Iteration 929, loss = 0.02885783
Iteration 930, loss = 0.02988054
Iteration 931, loss = 0.03098749
Iteration 932, loss = 0.03157902
Iteration 933, loss = 0.03584339
Iteration 934, loss = 0.03734480
Iteration 935, loss = 0.03802124
Iteration 936, loss = 0.03787562
Iteration 937, loss = 0.03733263
Iteration 938, loss = 0.03289866
Iteration 939, loss = 0.02970535
Iteration 940, loss = 0.03544654
Iteration 941, loss = 0.04313824
Iteration 942, loss = 0.05066637
Iteration 943, loss = 0.04071083
Iteration 944, loss = 0.03468993
Iteration 945, loss = 0.03932678
Iteration 946, loss = 0.03515162
Iteration 947, loss = 0.03381788
Iteration 948, loss = 0.03537495
Iteration 949, loss = 0.03309116
Iteration 950, loss = 0.03359831
Iteration 951, loss = 0.03714926
Iteration 952, loss = 0.03476100
Iteration 953, loss = 0.03278064
Iteration 954, loss = 0.03233203
Iteration 955, loss = 0.03345016
Iteration 956, loss = 0.03787739
Iteration 957, loss = 0.03670259
Iteration 958, loss = 0.04047460
Iteration 959, loss = 0.04324913
Iteration 960, loss = 0.05250678
Iteration 961, loss = 0.04924466
Iteration 962, loss = 0.04182871
Iteration 963, loss = 0.03753350
Iteration 964, loss = 0.02962915
Iteration 965, loss = 0.03049425
Iteration 966, loss = 0.02951371
Iteration 967, loss = 0.03191800
Iteration 968, loss = 0.03076087
Iteration 969, loss = 0.02959438
Iteration 970, loss = 0.03082273
Iteration 971, loss = 0.03404701
Iteration 972, loss = 0.03244904
Iteration 973, loss = 0.03057258
Iteration 974, loss = 0.02963999
Iteration 975, loss = 0.03423602
Iteration 976, loss = 0.03043838
Iteration 977, loss = 0.03070866
Iteration 978, loss = 0.03098920
Iteration 979, loss = 0.03239494
Iteration 980, loss = 0.03591483
Iteration 981, loss = 0.03197714
Iteration 982, loss = 0.02919832
Iteration 983, loss = 0.02913025
Iteration 984, loss = 0.03014125
Iteration 985, loss = 0.03138341
Iteration 986, loss = 0.03527344
Iteration 987, loss = 0.03297376
Iteration 988, loss = 0.02901617
Iteration 989, loss = 0.03080027
Iteration 990, loss = 0.02999835
Iteration 991, loss = 0.03009822
Iteration 992, loss = 0.02910884
Iteration 993, loss = 0.02861098
Iteration 994, loss = 0.02946227
Iteration 995, loss = 0.02997733
Iteration 996, loss = 0.03111976
Iteration 997, loss = 0.03134381
Iteration 998, loss = 0.03265393
Iteration 999, loss = 0.03303215
Iteration 1000, loss = 0.03373868
*** fcst ***
[[1.36371197 1.10378511 1.31226163]
 [1.36287204 1.12044998 1.30604493]
 [1.36203211 1.13711485 1.29982822]
 [1.36119218 1.15377972 1.29361151]
 [1.36035225 1.17044459 1.2873948 ]
 [1.35951232 1.18710946 1.28117809]
 [1.3586724  1.20377433 1.27496139]
 [1.35783247 1.2204392  1.26690794]
 [1.35699254 1.23710407 1.25818654]
 [1.13972291 0.93207196 1.09552415]
 [1.13303299 0.94663965 1.07910472]
 [1.12634307 0.96120733 1.06268528]
 [1.11965314 0.97577502 1.04626584]
 [1.11296322 0.99034271 1.0298464 ]
 [1.10627329 1.00543221 1.01342697]
 [1.09958337 1.02209708 0.99700753]
 [1.13307892 1.03876195 0.98452123]
 [1.23620186 1.05542683 1.07439646]
 [1.3393248  1.0720917  1.24598143]
 [1.44244773 1.08875657 1.45355457]
 [1.51296836 1.10542144 1.49488137]
 [1.51904487 1.12208631 1.48866466]
 [1.51820494 1.13875118 1.48244795]
 [1.51736501 1.15541605 1.47623124]
 [1.51652508 1.17208092 1.47001453]
 [1.51568516 1.18874579 1.46330517]
 [1.51484523 1.20541066 1.45458377]
 [1.5140053  1.22207553 1.44586236]
 [1.51316537 1.2387404  1.43714096]
 [1.51232544 1.25540527 1.42841956]
 [1.51148551 1.27207014 1.41969815]
 [1.51064558 1.28873501 1.41097675]
 [1.50980565 1.30539988 1.40028736]
 [1.3025171  1.00236682 1.26071343]
 [1.29582717 1.01693451 1.244294  ]
 [1.28913725 1.03150219 1.22787456]
 [1.28244732 1.04606988 1.21145512]
 [1.2757574  1.06063757 1.19503569]
 [1.26906747 1.07520526 1.17861625]
 [1.26237755 1.09039289 1.16219681]
 [1.27988465 1.10705776 1.14577737]
 [1.38300759 1.12372263 1.21100306]
 [1.48613053 1.1403875  1.3882104 ]
 [1.57781406 1.15705237 1.59578354]
 [1.64407543 1.17371724 1.65098099]
 [1.6362166  1.19038211 1.6348963 ]
 [1.62580185 1.20704698 1.61479109]
 [1.6153871  1.22371185 1.59468589]
 [1.60497235 1.24037672 1.57458068]
 [1.5945576  1.25704159 1.55447548]
 [1.58414286 1.27370646 1.53437027]
 [1.57372811 1.29037133 1.51426507]
 [1.56331336 1.3070362  1.49415986]
 [1.55289861 1.32370107 1.47405466]
 [1.54248387 1.34036594 1.45394945]
 [1.53206912 1.35703081 1.43384425]
 [1.52165437 1.37369568 1.41373904]
 [0.96161598 0.59291346 0.89287629]
 [0.9450812  0.60957833 0.86846562]
 [0.92854642 0.6262432  0.84405496]
 [0.91201164 0.64290807 0.81964429]
 [0.89547686 0.65957294 0.79523362]
 [0.87894208 0.67623781 0.77082295]
 [0.8624073  0.69290268 0.74641228]
 [0.85654288 0.70956755 0.75370729]
 [0.95151558 0.72623242 0.83664823]
 [1.04648828 0.74289729 0.92546943]
 [1.14146097 0.75956216 1.13627434]
 [1.23643367 0.77622703 1.19590041]
 [1.23678719 0.7928919  1.180045  ]
 [1.23348183 0.80955677 1.16418959]
 [1.22797028 0.82622164 1.14833417]
 [1.21793235 0.84288651 1.13247876]
 [1.20789442 0.85955139 1.11742145]
 [1.1978565  0.87621626 1.10424171]
 [1.18781857 0.89288113 1.09106198]
 [1.17807314 0.909546   1.07788224]
 [1.16846672 0.92621087 1.06470251]
 [1.1588603  0.94287574 1.05152277]
 [1.14925388 0.95954061 1.03834304]
 [1.13964745 0.97620548 1.02608773]
 [0.96788872 0.6631508  0.89273045]
 [0.95135394 0.67787414 0.86831978]
 [0.93481916 0.69453901 0.84390911]
 [0.91828438 0.71120388 0.81949844]
 [0.9017496  0.72786875 0.79508777]
 [0.88521482 0.74453362 0.77067711]
 [0.86868004 0.76119849 0.74626644]
 [0.87376952 0.77786336 0.75516593]
 [0.96874221 0.79452823 0.83810686]
 [1.06371491 0.8111931  0.94637323]
 [1.1586876  0.82785797 1.15717814]
 [1.24609381 0.84452284 1.21274133]
 [1.24549006 0.86118771 1.19688592]
 [1.2421847  0.87785258 1.18103051]
 [1.2345462  0.89451745 1.16517509]
 [1.22450827 0.91118232 1.14931968]
 [1.21447034 0.92784719 1.13346427]
 [1.20443242 0.94451206 1.1179326 ]
 [1.19439449 0.96117693 1.10475287]
 [1.18435656 0.9778418  1.09157313]]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV    1.25992 2021-12-17 15:00:00          200
1    MALMEDY_CM_FT007.F.PV   1.263122 2021-12-17 16:00:00          200
2    MALMEDY_CM_FT007.F.PV   1.266325 2021-12-17 17:00:00          200
3    MALMEDY_CM_FT007.F.PV   1.269528 2021-12-17 18:00:00          200
4    MALMEDY_CM_FT007.F.PV   1.272731 2021-12-17 19:00:00          200
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV   1.095003 2021-12-21 15:00:00          200
97   MALMEDY_CM_FT007.F.PV   1.091927 2021-12-21 16:00:00          200
98   MALMEDY_CM_FT007.F.PV   1.088959 2021-12-21 17:00:00          200
99   MALMEDY_CM_FT007.F.PV   1.086775 2021-12-21 18:00:00          200
100  MALMEDY_CM_FT007.F.PV    1.08459 2021-12-21 19:00:00          200

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT007.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV    1.25992 2021-12-17 15:00:00          200
1    MALMEDY_CM_FT007.F.PV   1.263122 2021-12-17 16:00:00          200
2    MALMEDY_CM_FT007.F.PV   1.266325 2021-12-17 17:00:00          200
3    MALMEDY_CM_FT007.F.PV   1.269528 2021-12-17 18:00:00          200
4    MALMEDY_CM_FT007.F.PV   1.272731 2021-12-17 19:00:00          200
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV   1.095003 2021-12-21 15:00:00          200
97   MALMEDY_CM_FT007.F.PV   1.091927 2021-12-21 16:00:00          200
98   MALMEDY_CM_FT007.F.PV   1.088959 2021-12-21 17:00:00          200
99   MALMEDY_CM_FT007.F.PV   1.086775 2021-12-21 18:00:00          200
100  MALMEDY_CM_FT007.F.PV    1.08459 2021-12-21 19:00:00          200

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV   1.363712 2021-12-17 15:00:00          201
1    MALMEDY_CM_FT007.F.PV   1.362872 2021-12-17 16:00:00          201
2    MALMEDY_CM_FT007.F.PV   1.362032 2021-12-17 17:00:00          201
3    MALMEDY_CM_FT007.F.PV   1.361192 2021-12-17 18:00:00          201
4    MALMEDY_CM_FT007.F.PV   1.360352 2021-12-17 19:00:00          201
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV   1.224508 2021-12-21 15:00:00          201
97   MALMEDY_CM_FT007.F.PV    1.21447 2021-12-21 16:00:00          201
98   MALMEDY_CM_FT007.F.PV   1.204432 2021-12-21 17:00:00          201
99   MALMEDY_CM_FT007.F.PV   1.194394 2021-12-21 18:00:00          201
100  MALMEDY_CM_FT007.F.PV   1.184357 2021-12-21 19:00:00          201

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT007.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV   1.363712 2021-12-17 15:00:00          201
1    MALMEDY_CM_FT007.F.PV   1.362872 2021-12-17 16:00:00          201
2    MALMEDY_CM_FT007.F.PV   1.362032 2021-12-17 17:00:00          201
3    MALMEDY_CM_FT007.F.PV   1.361192 2021-12-17 18:00:00          201
4    MALMEDY_CM_FT007.F.PV   1.360352 2021-12-17 19:00:00          201
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV   1.224508 2021-12-21 15:00:00          201
97   MALMEDY_CM_FT007.F.PV    1.21447 2021-12-21 16:00:00          201
98   MALMEDY_CM_FT007.F.PV   1.204432 2021-12-21 17:00:00          201
99   MALMEDY_CM_FT007.F.PV   1.194394 2021-12-21 18:00:00          201
100  MALMEDY_CM_FT007.F.PV   1.184357 2021-12-21 19:00:00          201

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV   1.103785 2021-12-17 15:00:00          202
1    MALMEDY_CM_FT007.F.PV    1.12045 2021-12-17 16:00:00          202
2    MALMEDY_CM_FT007.F.PV   1.137115 2021-12-17 17:00:00          202
3    MALMEDY_CM_FT007.F.PV    1.15378 2021-12-17 18:00:00          202
4    MALMEDY_CM_FT007.F.PV   1.170445 2021-12-17 19:00:00          202
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV   0.911182 2021-12-21 15:00:00          202
97   MALMEDY_CM_FT007.F.PV   0.927847 2021-12-21 16:00:00          202
98   MALMEDY_CM_FT007.F.PV   0.944512 2021-12-21 17:00:00          202
99   MALMEDY_CM_FT007.F.PV   0.961177 2021-12-21 18:00:00          202
100  MALMEDY_CM_FT007.F.PV   0.977842 2021-12-21 19:00:00          202

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT007.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV   1.103785 2021-12-17 15:00:00          202
1    MALMEDY_CM_FT007.F.PV    1.12045 2021-12-17 16:00:00          202
2    MALMEDY_CM_FT007.F.PV   1.137115 2021-12-17 17:00:00          202
3    MALMEDY_CM_FT007.F.PV    1.15378 2021-12-17 18:00:00          202
4    MALMEDY_CM_FT007.F.PV   1.170445 2021-12-17 19:00:00          202
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV   0.911182 2021-12-21 15:00:00          202
97   MALMEDY_CM_FT007.F.PV   0.927847 2021-12-21 16:00:00          202
98   MALMEDY_CM_FT007.F.PV   0.944512 2021-12-21 17:00:00          202
99   MALMEDY_CM_FT007.F.PV   0.961177 2021-12-21 18:00:00          202
100  MALMEDY_CM_FT007.F.PV   0.977842 2021-12-21 19:00:00          202

[101 rows x 4 columns]
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV   1.312262 2021-12-17 15:00:00          203
1    MALMEDY_CM_FT007.F.PV   1.306045 2021-12-17 16:00:00          203
2    MALMEDY_CM_FT007.F.PV   1.299828 2021-12-17 17:00:00          203
3    MALMEDY_CM_FT007.F.PV   1.293612 2021-12-17 18:00:00          203
4    MALMEDY_CM_FT007.F.PV   1.287395 2021-12-17 19:00:00          203
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV    1.14932 2021-12-21 15:00:00          203
97   MALMEDY_CM_FT007.F.PV   1.133464 2021-12-21 16:00:00          203
98   MALMEDY_CM_FT007.F.PV   1.117933 2021-12-21 17:00:00          203
99   MALMEDY_CM_FT007.F.PV   1.104753 2021-12-21 18:00:00          203
100  MALMEDY_CM_FT007.F.PV   1.091573 2021-12-21 19:00:00          203

[101 rows x 4 columns]
WRITE TO DATABASE FOR MALMEDY_CM_FT007.F.PV*** df_out ***
                   scadaid scadavalue       scadadatetime scadaquality
0    MALMEDY_CM_FT007.F.PV   1.312262 2021-12-17 15:00:00          203
1    MALMEDY_CM_FT007.F.PV   1.306045 2021-12-17 16:00:00          203
2    MALMEDY_CM_FT007.F.PV   1.299828 2021-12-17 17:00:00          203
3    MALMEDY_CM_FT007.F.PV   1.293612 2021-12-17 18:00:00          203
4    MALMEDY_CM_FT007.F.PV   1.287395 2021-12-17 19:00:00          203
..                     ...        ...                 ...          ...
96   MALMEDY_CM_FT007.F.PV    1.14932 2021-12-21 15:00:00          203
97   MALMEDY_CM_FT007.F.PV   1.133464 2021-12-21 16:00:00          203
98   MALMEDY_CM_FT007.F.PV   1.117933 2021-12-21 17:00:00          203
99   MALMEDY_CM_FT007.F.PV   1.104753 2021-12-21 18:00:00          203
100  MALMEDY_CM_FT007.F.PV   1.091573 2021-12-21 19:00:00          203

[101 rows x 4 columns]


9 cpt.inputdata MALMEDY_CM_FT008.F.PV cpt.ao 60.0 1440.0 30240.0 MLPDYNAMIC
DELETE FROM cpt.ao WHERE scadaid='MALMEDY_CM_FT008.F.PV'... done
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT008.F.PV' ORDER BY scadadatetime DESC... done
*** ldate, fdate ***
2021-12-17 14:15:00 2021-11-18 19:30:00
>>> Water demand prediction : TOF 2021-12-20 18:00:00
*** start_train, stop_train, start_pred, stop_pred, toftime ***
2021-11-26 15:00:00 2021-12-17 15:00:00 2021-12-17 15:00:00 2021-12-21 19:00:00 2021-12-20 18:00:00
SELECT * FROM cpt.inputdata WHERE scadaid='MALMEDY_CM_FT008.F.PV' ORDER BY scadadatetime ASC, rnum ASC... done
*** df_in ***
        rnum       scadadatetime                scadaid  scadavalue scadaquality
0     333856 2021-11-18 19:30:00  MALMEDY_CM_FT008.F.PV       3.273         None
1     333889 2021-11-18 20:00:00  MALMEDY_CM_FT008.F.PV       3.273         None
2     333913 2021-11-18 20:00:00  MALMEDY_CM_FT008.F.PV       3.273         None
3     333929 2021-11-18 20:15:00  MALMEDY_CM_FT008.F.PV       4.313         None
4     333953 2021-11-18 20:15:00  MALMEDY_CM_FT008.F.PV       4.313         None
...      ...                 ...                    ...         ...          ...
6358  498777 2021-12-17 14:00:00  MALMEDY_CM_FT008.F.PV       4.073         None
6359  498800 2021-12-17 14:00:00  MALMEDY_CM_FT008.F.PV       4.073         None
6360  498818 2021-12-17 14:00:00  MALMEDY_CM_FT008.F.PV       4.073         None
6361  498836 2021-12-17 14:15:00  MALMEDY_CM_FT008.F.PV       4.073         None
6362  498858 2021-12-17 14:15:00  MALMEDY_CM_FT008.F.PV       4.073         None

[6363 rows x 5 columns]
*** X_train, y_train, df_X, df_y, X_pred ***
[[330. 330. 330. ... 351. 351. 351.]
 [  5.   5.   5. ...   5.   5.   5.]
 [ 15.  16.  17. ...  12.  13.  14.]] [ 3.087       3.27766667  3.48633333  4.053       4.053       4.32
  4.31916667  2.98        2.98        2.878       2.35        1.873
  1.75        1.627       1.627       1.55566667  1.91        2.3
  2.3         3.59333333  4.36883333  5.013       5.013       4.693
  4.48083333  5.02        5.00038889  4.98077778  4.96116667  4.94155556
  4.92194444  4.90233333  4.88272222  4.86311111  4.8435      4.82388889
  4.80427778  4.78466667  4.76505556  4.74544444  4.72583333  4.70622222
  4.68661111  4.667       5.267       5.867       5.867       5.96033333
  4.9635      3.92        3.92        3.73        3.60466667  3.453
  4.24        3.582       3.253       3.142       2.587       1.95566667
  1.64        1.60333333  1.42        1.913       1.913       2.4675
  5.24        4.48466667  4.107       4.107       4.5         3.97133333
  3.707       3.707       2.893       3.01        3.127       3.127
  4.02433333  3.7         2.927       2.927       2.307       2.13
  1.953       1.89083333  1.58        1.64        1.64        2.16783333
  4.807       3.58033333  2.967       2.967       2.811       2.832
  3.327       2.67366667  2.347       2.347       2.649       3.4
  4.          2.78        2.78        2.69116667  2.247       1.99366667
  1.867       1.867       1.647       1.8335      2.02        2.02
  3.67333333  4.3445      3.567       3.64233333  3.68        3.68
  3.498       3.22583333  2.32        2.32        2.32        2.32
  5.567       4.0335      2.5         2.5         2.327       2.0235
  1.72        1.68333333  1.5         1.58        1.58        1.97883333
  3.973       3.413       3.413       3.413       3.4         3.4
  3.4         3.4         3.4         3.12        2.98        2.98
  3.807       3.371       3.153       3.00633333  2.273       1.96233333
  1.807       1.75583333  1.5         2.06        2.06        2.74883333
  6.193       5.193       4.693       4.693       3.187       3.34233333
  3.42        3.42        3.34        3.9         4.46        4.46
  3.6         3.3535      3.107       3.107       2.227       2.09366667
  2.027       1.97466667  1.713       1.873       1.873       2.08533333
  3.147       6.12        6.12        5.94883333  5.093       4.507
  4.507       4.5635      4.62        4.62        4.198       4.1035
  4.22        4.22        3.62866667  2.95566667  2.767       2.767
  2.34033333  2.02916667  1.54        1.773       1.773       1.9965
  2.22        2.22        4.65466667  5.4505      5.029       4.653
  4.653       4.203       3.753       4.54        4.444       4.3775
  4.27366667  3.847       3.847       2.32        2.32        2.32
  1.818       1.56366667  1.547       1.547       1.547       1.547
  1.547       3.587       4.13233333  4.44666667  4.655       4.127
  5.01        5.893       5.893       3.82        4.2335      4.647
  4.647       4.867       3.8635      2.86        2.74216667  2.153
  1.98433333  1.9         1.9         1.873       2.803       3.733
  3.733       4.12633333  4.0985      3.874       3.874       3.874
  3.72833333  3.          3.          3.36333333  5.18        3.833
  3.8265      3.82        3.82        2.753       2.613       1.913
  1.893       1.873       1.873       1.909       2.369       4.579
  3.79033333  3.396       3.396       3.973       3.653       3.333
  3.187       3.428       3.669       3.887       3.887       3.887
  4.333       4.1965      4.06        4.06        2.98        2.44
  2.2235      2.007       1.9         1.97133333  2.007       2.007
  3.88233333  4.70116667  4.107       3.727       3.347       3.347
  3.34233333  3.52216667  4.433       4.433       4.42652778  4.42005556
  4.41358333  4.40711111  4.40063889  4.39416667  4.38769444  4.38122222
  4.37475     4.36827778  4.36180556  4.35533333  4.34886111  4.34238889
  4.33591667  4.32944444  4.32297222  4.3165      4.31002778  4.30355556
  4.29708333  4.29061111  4.28413889  4.27766667  4.27119444  4.26472222
  4.25825     4.25177778  4.24530556  4.23883333  4.23236111  4.22588889
  4.21941667  4.21294444  4.20647222  4.2         4.19352778  4.18705556
  4.18058333  4.17411111  4.16763889  4.16116667  4.15469444  4.14822222
  4.14175     4.13527778  4.12880556  4.12233333  4.11586111  4.10938889
  4.10291667  4.09644444  4.08997222  4.0835      4.07702778  4.07055556
  4.06408333  4.05761111  4.05113889  4.04466667  4.03819444  4.03172222
  4.02525     4.01877778  4.01230556  4.00583333  3.99936111  3.99288889
  3.98641667  3.97994444  3.97347222  3.967       4.447       4.571
  4.633       4.633       4.093       2.253       2.253       2.253
  1.99566667  1.83366667  1.667       1.913       1.96881481  2.02462963
  2.08044444  2.13625926  2.19207407  2.24788889  2.3037037   2.35951852
  2.41533333  2.47114815  2.52696296  2.58277778  2.63859259  2.69440741
  2.75022222  2.80603704  2.86185185  2.91766667  2.97348148  3.0292963
  3.08511111  3.14092593  3.19674074  3.25255556  3.30837037  3.36418519
  3.42        3.47581481  3.53162963  3.58744444  3.64325926  3.69907407
  3.75488889  3.8107037   3.86651852  3.92233333  4.28        4.28
  4.28        3.8565      3.433       3.433       2.473       2.073
  1.873       1.81533333  1.527       1.527       1.92233333  3.2365
  4.353       4.353       4.353       4.38044444  4.40788889  4.43533333
  4.46277778  4.49022222  4.51766667  4.54511111  4.57255556  4.6
  4.6         4.6         4.6         4.6         4.6         4.6
  4.6         4.6         4.6         4.6         4.6         4.6
  4.6         4.6         4.6         4.6         4.6         4.6
  4.8865      5.173       5.2         5.2         5.2         6.16
  5.33333333  4.92        4.74116667  3.847       3.40233333  3.18
  3.09        3.          3.          3.11133333  3.62133333  5.893
 16.69766667 19.29066667  5.244       5.244       5.04883333  4.073     ]                        DOY   WD    HR
2021-11-26 15:00:00  330.0  5.0  15.0
2021-11-26 16:00:00  330.0  5.0  16.0
2021-11-26 17:00:00  330.0  5.0  17.0
2021-11-26 18:00:00  330.0  5.0  18.0
2021-11-26 19:00:00  330.0  5.0  19.0
...                    ...  ...   ...
2021-12-21 15:00:00  355.0  2.0  15.0
2021-12-21 16:00:00  355.0  2.0  16.0
2021-12-21 17:00:00  355.0  2.0  17.0
2021-12-21 18:00:00  355.0  2.0  18.0
2021-12-21 19:00:00  355.0  2.0  19.0

[605 rows x 3 columns] 2021-11-26 15:00:00    3.087000
2021-11-26 16:00:00    3.277667
2021-11-26 17:00:00    3.486333
2021-11-26 18:00:00    4.053000
2021-11-26 19:00:00    4.053000
                         ...   
2021-12-21 15:00:00    0.000000
2021-12-21 16:00:00    0.000000
2021-12-21 17:00:00    0.000000
2021-12-21 18:00:00    0.000000
2021-12-21 19:00:00    0.000000
Freq: 60T, Name: scadavalue, Length: 605, dtype: float64 [[351. 351. 351. 351. 351. 351. 351. 351. 351. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352. 352.
  352. 352. 352. 352. 352. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353. 353.
  353. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354.
  354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 354. 355. 355. 355.
  355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355. 355.
  355. 355. 355.]
 [  5.   5.   5.   5.   5.   5.   5.   5.   5.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.
    6.   6.   6.   6.   6.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.
    7.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   2.   2.   2.
    2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.
    2.   2.   2.]
 [ 15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.   3.   4.
    5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.
   19.  20.  21.  22.  23.   0.   1.   2.   3.   4.   5.   6.   7.   8.
    9.  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.
   23.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
   13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.   0.   1.   2.
    3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.
   17.  18.  19.]]